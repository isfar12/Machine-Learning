{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how XGBoost forms its output, let’s walk through a simple dataset example for a **regression problem**. We’ll use a small dataset to keep it easy to follow, and we’ll go through the first few steps in the training process.\n",
    "\n",
    "### Dataset Example\n",
    "Consider a simple dataset with a single feature \\( x \\) and target \\( y \\):\n",
    "\n",
    "| x | y   |\n",
    "|---|-----|\n",
    "| 1 | 2.1 |\n",
    "| 2 | 2.9 |\n",
    "| 3 | 3.7 |\n",
    "| 4 | 4.2 |\n",
    "| 5 | 5.3 |\n",
    "\n",
    "Our goal is to predict \\( y \\) based on \\( x \\).\n",
    "\n",
    "### Step-by-Step Explanation of XGBoost Output Formation\n",
    "\n",
    "Let’s say we use **4 trees** in the model and a **learning rate \\( \\eta = 0.1 \\)**. In XGBoost, the output at each step is computed by adding the predictions of each tree, scaled by the learning rate. We’ll follow the **gradient boosting process** where each new tree corrects the residuals of the previous predictions.\n",
    "\n",
    "#### 1. Initial Prediction\n",
    "\n",
    "XGBoost typically starts with a simple **initial prediction** for all samples. For regression, this is often the **mean of \\( y \\)**. \n",
    "\n",
    "In our case:\n",
    "\\[\n",
    "\\text{Initial Prediction} = \\frac{2.1 + 2.9 + 3.7 + 4.2 + 5.3}{5} = 3.64\n",
    "\\]\n",
    "\n",
    "So, for each \\( x \\), the initial prediction \\( \\hat{y}^{(0)} \\) is **3.64**.\n",
    "\n",
    "#### 2. Calculate Residuals\n",
    "\n",
    "Now, we calculate the residuals (errors) for each data point, which indicate how far off the initial predictions are from the actual values \\( y \\).\n",
    "\n",
    "| x | y   | Initial Prediction | Residual \\( r = y - \\hat{y}^{(0)} \\) |\n",
    "|---|-----|---------------------|---------------------------------------|\n",
    "| 1 | 2.1 | 3.64               | \\( 2.1 - 3.64 = -1.54 \\)             |\n",
    "| 2 | 2.9 | 3.64               | \\( 2.9 - 3.64 = -0.74 \\)             |\n",
    "| 3 | 3.7 | 3.64               | \\( 3.7 - 3.64 = 0.06 \\)              |\n",
    "| 4 | 4.2 | 3.64               | \\( 4.2 - 3.64 = 0.56 \\)              |\n",
    "| 5 | 5.3 | 3.64               | \\( 5.3 - 3.64 = 1.66 \\)              |\n",
    "\n",
    "These residuals become the target for the **first tree**.\n",
    "\n",
    "#### 3. Train the First Tree on Residuals\n",
    "\n",
    "The first tree is trained to predict the residuals. This tree might learn to predict values close to the residuals based on \\( x \\). Let’s assume the first tree predicts the following residuals (scaled by the learning rate \\( \\eta = 0.1 \\)):\n",
    "\n",
    "| x | Residual \\( r \\) | Tree 1 Prediction (scaled) |\n",
    "|---|-------------------|---------------------------|\n",
    "| 1 | -1.54            | -0.15                     |\n",
    "| 2 | -0.74            | -0.07                     |\n",
    "| 3 | 0.06             | 0.01                      |\n",
    "| 4 | 0.56             | 0.06                      |\n",
    "| 5 | 1.66             | 0.17                      |\n",
    "\n",
    "#### 4. Update Predictions After Tree 1\n",
    "\n",
    "We add the scaled predictions of Tree 1 to our initial predictions:\n",
    "\n",
    "\\[\n",
    "\\hat{y}^{(1)} = \\hat{y}^{(0)} + \\eta \\cdot \\text{Tree 1 Predictions}\n",
    "\\]\n",
    "\n",
    "| x | Initial Prediction \\( \\hat{y}^{(0)} \\) | Tree 1 Prediction (scaled) | Updated Prediction \\( \\hat{y}^{(1)} \\) |\n",
    "|---|----------------------------------------|----------------------------|-----------------------------------------|\n",
    "| 1 | 3.64                                   | -0.15                      | \\( 3.64 - 0.15 = 3.49 \\)               |\n",
    "| 2 | 3.64                                   | -0.07                      | \\( 3.64 - 0.07 = 3.57 \\)               |\n",
    "| 3 | 3.64                                   | 0.01                       | \\( 3.64 + 0.01 = 3.65 \\)               |\n",
    "| 4 | 3.64                                   | 0.06                       | \\( 3.64 + 0.06 = 3.70 \\)               |\n",
    "| 5 | 3.64                                   | 0.17                       | \\( 3.64 + 0.17 = 3.81 \\)               |\n",
    "\n",
    "#### 5. Repeat for Remaining Trees\n",
    "\n",
    "The residuals are recalculated based on the new predictions \\( \\hat{y}^{(1)} \\), and the process repeats for each subsequent tree. Each tree is trained to reduce the residual error from the previous step, and its predictions are scaled by the learning rate before being added to the cumulative predictions.\n",
    "\n",
    "Let’s assume the predictions for Trees 2, 3, and 4 (scaled by \\( \\eta = 0.1 \\)) are as follows:\n",
    "\n",
    "- **Tree 2 Predictions**: \\([-0.10, -0.05, 0.02, 0.04, 0.12]\\)\n",
    "- **Tree 3 Predictions**: \\([-0.08, -0.03, 0.02, 0.03, 0.08]\\)\n",
    "- **Tree 4 Predictions**: \\([-0.06, -0.02, 0.01, 0.02, 0.05]\\)\n",
    "\n",
    "Adding these sequentially to the predictions, we get the final cumulative predictions after four trees:\n",
    "\n",
    "\\[\n",
    "\\text{Final Prediction} = \\hat{y}^{(0)} + \\eta \\cdot \\left( \\text{Tree 1} + \\text{Tree 2} + \\text{Tree 3} + \\text{Tree 4} \\right)\n",
    "\\]\n",
    "\n",
    "| x | Initial Prediction \\( \\hat{y}^{(0)} \\) | Tree 1 | Tree 2 | Tree 3 | Tree 4 | Final Prediction |\n",
    "|---|----------------------------------------|--------|--------|--------|--------|------------------|\n",
    "| 1 | 3.64                                   | -0.15  | -0.10  | -0.08  | -0.06  | 3.25            |\n",
    "| 2 | 3.64                                   | -0.07  | -0.05  | -0.03  | -0.02  | 3.47            |\n",
    "| 3 | 3.64                                   | 0.01   | 0.02   | 0.02   | 0.01   | 3.70            |\n",
    "| 4 | 3.64                                   | 0.06   | 0.04   | 0.03   | 0.02   | 3.79            |\n",
    "| 5 | 3.64                                   | 0.17   | 0.12   | 0.08   | 0.05   | 4.06            |\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The **final prediction for each data point** is the sum of the initial prediction and the contributions from each tree, scaled by the learning rate. The trees correct the residual errors iteratively, so the prediction becomes more accurate with each additional tree.\n",
    "\n",
    "### Key Takeaways\n",
    "- **Initial Prediction**: Start with the mean for regression.\n",
    "- **Iterative Trees**: Each tree learns from the residuals (errors) of the previous predictions.\n",
    "- **Learning Rate Scaling**: Each tree’s output is scaled by the learning rate before adding it to the cumulative prediction.\n",
    "- **Final Output**: The sum of the initial prediction and the scaled outputs of all trees.\n",
    "\n",
    "This is how XGBoost gradually improves its predictions by adding trees that reduce the errors of previous trees, ultimately forming the final output after all trees have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
