{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem, XGBoost typically uses the **log-odds** (logarithm of the odds ratio) for binary classification. Instead of directly predicting probabilities, each tree contributes to the log-odds score, which is later converted into a probability for classification.\n",
    "\n",
    "Let’s go through an example with binary classification to illustrate the output formation in XGBoost.\n",
    "\n",
    "### Example Dataset\n",
    "Consider a dataset with a single feature \\( x \\) and a binary target \\( y \\) (0 or 1):\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 0 |\n",
    "| 2 | 1 |\n",
    "| 3 | 0 |\n",
    "| 4 | 1 |\n",
    "| 5 | 1 |\n",
    "\n",
    "Our goal is to predict whether \\( y \\) is 0 or 1 based on \\( x \\).\n",
    "\n",
    "### Steps to Form the Output for Classification in XGBoost\n",
    "\n",
    "#### 1. Initial Prediction\n",
    "\n",
    "XGBoost starts by initializing the prediction for each data point to a **base value**. For binary classification, this base value is often set to the log-odds of the proportion of positive and negative samples in the dataset.\n",
    "\n",
    "In this case:\n",
    "- Total samples = 5\n",
    "- Positive samples (\\( y = 1 \\)) = 3\n",
    "- Negative samples (\\( y = 0 \\)) = 2\n",
    "\n",
    "The **initial log-odds prediction** is calculated as:\n",
    "\\[\n",
    "\\text{Initial Prediction (Log-Odds)} = \\ln\\left(\\frac{\\text{Positive Count}}{\\text{Negative Count}}\\right) = \\ln\\left(\\frac{3}{2}\\right) \\approx 0.41\n",
    "\\]\n",
    "\n",
    "This initial log-odds prediction, \\( 0.41 \\), will be used for all samples.\n",
    "\n",
    "#### 2. Convert Log-Odds to Probability\n",
    "\n",
    "Using the initial log-odds score, we can convert this to a probability:\n",
    "\\[\n",
    "P = \\frac{1}{1 + e^{-\\text{Log-Odds}}} = \\frac{1}{1 + e^{-0.41}} \\approx 0.60\n",
    "\\]\n",
    "\n",
    "So, the **initial probability prediction** for each data point is **0.60**.\n",
    "\n",
    "#### 3. Calculate Residuals\n",
    "\n",
    "To improve the prediction, we calculate the **gradient** for each point. In classification, this gradient is based on the residual error between the predicted probability and the actual class label. Here, it is calculated as:\n",
    "\n",
    "\\[\n",
    "\\text{Residual} = y - P\n",
    "\\]\n",
    "\n",
    "| x | y | Initial Probability (P) | Residual \\( r = y - P \\) |\n",
    "|---|---|--------------------------|--------------------------|\n",
    "| 1 | 0 | 0.60                     | \\( 0 - 0.60 = -0.60 \\)   |\n",
    "| 2 | 1 | 0.60                     | \\( 1 - 0.60 = 0.40 \\)    |\n",
    "| 3 | 0 | 0.60                     | \\( 0 - 0.60 = -0.60 \\)   |\n",
    "| 4 | 1 | 0.60                     | \\( 1 - 0.60 = 0.40 \\)    |\n",
    "| 5 | 1 | 0.60                     | \\( 1 - 0.60 = 0.40 \\)    |\n",
    "\n",
    "#### 4. Train the First Tree on Residuals\n",
    "\n",
    "The first tree is trained to predict these residuals. Let’s assume it outputs the following values for each data point (scaled by a **learning rate \\( \\eta = 0.1 \\)**):\n",
    "\n",
    "| x | Residual \\( r \\) | Tree 1 Prediction (scaled) |\n",
    "|---|-------------------|---------------------------|\n",
    "| 1 | -0.60            | -0.06                     |\n",
    "| 2 | 0.40             | 0.04                      |\n",
    "| 3 | -0.60            | -0.06                     |\n",
    "| 4 | 0.40             | 0.04                      |\n",
    "| 5 | 0.40             | 0.04                      |\n",
    "\n",
    "#### 5. Update Log-Odds Prediction After Tree 1\n",
    "\n",
    "The updated log-odds prediction is computed by adding the output of Tree 1 to the initial prediction:\n",
    "\n",
    "\\[\n",
    "\\text{Updated Log-Odds} = \\text{Initial Log-Odds} + \\eta \\cdot \\text{Tree 1 Prediction}\n",
    "\\]\n",
    "\n",
    "| x | Initial Log-Odds | Tree 1 Prediction (scaled) | Updated Log-Odds |\n",
    "|---|-------------------|---------------------------|-------------------|\n",
    "| 1 | 0.41             | -0.06                     | \\( 0.41 - 0.06 = 0.35 \\) |\n",
    "| 2 | 0.41             | 0.04                      | \\( 0.41 + 0.04 = 0.45 \\) |\n",
    "| 3 | 0.41             | -0.06                     | \\( 0.41 - 0.06 = 0.35 \\) |\n",
    "| 4 | 0.41             | 0.04                      | \\( 0.41 + 0.04 = 0.45 \\) |\n",
    "| 5 | 0.41             | 0.04                      | \\( 0.41 + 0.04 = 0.45 \\) |\n",
    "\n",
    "#### 6. Convert Updated Log-Odds to Probability\n",
    "\n",
    "Now, we convert the updated log-odds to probabilities:\n",
    "\n",
    "\\[\n",
    "P = \\frac{1}{1 + e^{-\\text{Updated Log-Odds}}}\n",
    "\\]\n",
    "\n",
    "| x | Updated Log-Odds | Updated Probability |\n",
    "|---|-------------------|--------------------|\n",
    "| 1 | 0.35             | \\( \\frac{1}{1 + e^{-0.35}} \\approx 0.59 \\) |\n",
    "| 2 | 0.45             | \\( \\frac{1}{1 + e^{-0.45}} \\approx 0.61 \\) |\n",
    "| 3 | 0.35             | \\( \\frac{1}{1 + e^{-0.35}} \\approx 0.59 \\) |\n",
    "| 4 | 0.45             | \\( \\frac{1}{1 + e^{-0.45}} \\approx 0.61 \\) |\n",
    "| 5 | 0.45             | \\( \\frac{1}{1 + e^{-0.45}} \\approx 0.61 \\) |\n",
    "\n",
    "#### 7. Repeat for Additional Trees\n",
    "\n",
    "Subsequent trees are trained to minimize the residuals between these updated probabilities and the true labels. Each tree’s output (after being scaled by the learning rate) is added to the cumulative log-odds prediction, and the process repeats.\n",
    "\n",
    "After four trees, the **final prediction** for each data point \\( x \\) is obtained by adding up the log-odds contributions from each tree and converting the result to a probability.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Initialize** with log-odds based on the dataset’s class ratio.\n",
    "2. **Train trees** on residuals between actual classes and predicted probabilities.\n",
    "3. **Update log-odds** by adding each tree’s output, scaled by the learning rate.\n",
    "4. **Convert log-odds to probabilities** for final predictions.\n",
    "5. Repeat until desired accuracy is achieved or max trees are reached. \n",
    "\n",
    "This process lets XGBoost sequentially refine its predictions with each new tree, reducing classification errors by adjusting probabilities closer to the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
