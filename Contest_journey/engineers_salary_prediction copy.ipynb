{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0aa0c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e53d5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../Dataset/engineers_salary_prediction_train.csv\")\n",
    "test=pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "322cc801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>salary_category</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>High</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362079</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300989</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406159</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs    job_title job_posted_date salary_category job_state feature_1  \\\n",
       "0    1       Others         2024/07            High        NY         A   \n",
       "1    2  Job_Title_1         2024/07             Low        CA         A   \n",
       "2    3       Others         2024/07             Low        CA         A   \n",
       "3    4       Others         2024/07             Low        CA         A   \n",
       "4    5       Others         2024/07             Low        CA         A   \n",
       "\n",
       "   feature_2  feature_3  feature_4  feature_5  ...  job_desc_291  \\\n",
       "0     0.6429      False      False       True  ...     -0.362079   \n",
       "1     0.4678      False      False      False  ...     -0.300989   \n",
       "2     0.4610      False      False      False  ...      0.000000   \n",
       "3     0.5064      False      False      False  ...      0.000000   \n",
       "4     0.4640      False      False      False  ...     -0.406159   \n",
       "\n",
       "   job_desc_292  job_desc_293  job_desc_294  job_desc_295  job_desc_296  \\\n",
       "0     -0.499308     -0.367894     -0.214881      0.014870     -0.271177   \n",
       "1     -0.415411     -0.341824     -0.319064      0.042322     -0.124755   \n",
       "2      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4     -0.654657     -0.074398     -0.464479      0.081037     -0.136992   \n",
       "\n",
       "   job_desc_297  job_desc_298  job_desc_299  job_desc_300  \n",
       "0     -0.113347     -0.587955     -0.919095     -0.207340  \n",
       "1      0.023489     -0.893224     -0.823024      0.112364  \n",
       "2      0.000000      0.000000      0.000000      0.000000  \n",
       "3      0.000000      0.000000      0.000000      0.000000  \n",
       "4     -0.276270     -0.696853     -0.601466      0.089939  \n",
       "\n",
       "[5 rows x 317 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5967f835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='job_state,feature_1'>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "df.groupby(by=\"job_state\")[\"feature_1\"].value_counts().sort_values().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "328b4dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new\n",
       "Others_A          858\n",
       "Job_Title_10_A     50\n",
       "Job_Title_5_A      46\n",
       "Job_Title_3_A      45\n",
       "Job_Title_7_A      37\n",
       "Job_Title_8_A      35\n",
       "Job_Title_2_A      32\n",
       "Job_Title_1_A      27\n",
       "Others_B           22\n",
       "Job_Title_13_A     11\n",
       "Job_Title_23_A      9\n",
       "Job_Title_11_A      9\n",
       "Job_Title_19_A      7\n",
       "Job_Title_12_A      7\n",
       "Job_Title_16_A      6\n",
       "Others_C            6\n",
       "Job_Title_6_A       5\n",
       "Job_Title_5_B       5\n",
       "Job_Title_4_A       5\n",
       "Job_Title_20_A      5\n",
       "Job_Title_26_A      4\n",
       "Job_Title_25_A      4\n",
       "Job_Title_21_A      4\n",
       "Job_Title_17_A      4\n",
       "Job_Title_18_A      4\n",
       "Job_Title_8_B       4\n",
       "Job_Title_9_A       4\n",
       "Job_Title_15_A      4\n",
       "Job_Title_24_A      3\n",
       "Job_Title_14_A      3\n",
       "Job_Title_22_A      3\n",
       "Job_Title_10_B      2\n",
       "Others_D            2\n",
       "Others_E            1\n",
       "Job_Title_7_B       1\n",
       "Job_Title_8_D       1\n",
       "Job_Title_3_D       1\n",
       "Job_Title_27_A      1\n",
       "Job_Title_10_D      1\n",
       "Job_Title_23_B      1\n",
       "Job_Title_20_B      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1700x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"new\"]=df[\"job_title\"]+\"_\"+df[\"feature_1\"]\n",
    "plt.figure(figsize=(17,18))\n",
    "df[\"new\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ecc3d535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job_title',\n",
       " 'job_posted_date',\n",
       " 'salary_category',\n",
       " 'job_state',\n",
       " 'feature_1',\n",
       " 'new']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c5612404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1281</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054078</td>\n",
       "      <td>-0.573635</td>\n",
       "      <td>-0.306883</td>\n",
       "      <td>-0.325092</td>\n",
       "      <td>0.089463</td>\n",
       "      <td>-0.353476</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.667958</td>\n",
       "      <td>-0.702116</td>\n",
       "      <td>-0.206267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/08</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.868718</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>-0.179036</td>\n",
       "      <td>-0.717763</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>-0.190448</td>\n",
       "      <td>-1.261702</td>\n",
       "      <td>-0.505897</td>\n",
       "      <td>0.082080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283</td>\n",
       "      <td>Others</td>\n",
       "      <td>2023/01</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416109</td>\n",
       "      <td>-0.619822</td>\n",
       "      <td>-0.493653</td>\n",
       "      <td>-0.347556</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>-0.331212</td>\n",
       "      <td>-0.381348</td>\n",
       "      <td>-0.506540</td>\n",
       "      <td>-0.773561</td>\n",
       "      <td>-0.105221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1284</td>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297560</td>\n",
       "      <td>-0.481448</td>\n",
       "      <td>-0.497642</td>\n",
       "      <td>-0.254823</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>-0.362739</td>\n",
       "      <td>-0.102704</td>\n",
       "      <td>-0.491272</td>\n",
       "      <td>-0.808156</td>\n",
       "      <td>-0.048326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1285</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176458</td>\n",
       "      <td>-0.726473</td>\n",
       "      <td>-0.323976</td>\n",
       "      <td>-0.145825</td>\n",
       "      <td>-0.046866</td>\n",
       "      <td>-0.229873</td>\n",
       "      <td>-0.568318</td>\n",
       "      <td>-0.614605</td>\n",
       "      <td>-0.770506</td>\n",
       "      <td>0.142140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    obs    job_title job_posted_date job_state feature_1  feature_2  \\\n",
       "0  1281       Others         2024/06        CA         A     0.6473   \n",
       "1  1282       Others         2024/08        NY         A     0.4238   \n",
       "2  1283       Others         2023/01        CA         A     0.6219   \n",
       "3  1284  Job_Title_5         2024/06        NY         A     0.6704   \n",
       "4  1285       Others         2024/05        CA         A     0.7310   \n",
       "\n",
       "   feature_3  feature_4  feature_5  feature_6  ...  job_desc_291  \\\n",
       "0      False      False       True       True  ...     -0.054078   \n",
       "1       True      False      False      False  ...     -0.868718   \n",
       "2       True      False      False       True  ...     -0.416109   \n",
       "3      False      False      False       True  ...     -0.297560   \n",
       "4      False      False      False       True  ...     -0.176458   \n",
       "\n",
       "   job_desc_292  job_desc_293  job_desc_294  job_desc_295  job_desc_296  \\\n",
       "0     -0.573635     -0.306883     -0.325092      0.089463     -0.353476   \n",
       "1     -0.337967     -0.179036     -0.717763      0.404843      0.032468   \n",
       "2     -0.619822     -0.493653     -0.347556      0.071679     -0.331212   \n",
       "3     -0.481448     -0.497642     -0.254823      0.047404     -0.362739   \n",
       "4     -0.726473     -0.323976     -0.145825     -0.046866     -0.229873   \n",
       "\n",
       "   job_desc_297  job_desc_298  job_desc_299  job_desc_300  \n",
       "0     -0.159314     -0.667958     -0.702116     -0.206267  \n",
       "1     -0.190448     -1.261702     -0.505897      0.082080  \n",
       "2     -0.381348     -0.506540     -0.773561     -0.105221  \n",
       "3     -0.102704     -0.491272     -0.808156     -0.048326  \n",
       "4     -0.568318     -0.614605     -0.770506      0.142140  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1523a683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns=df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "51da238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns=df.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "len(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "54569aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns=df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "len(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "31faf8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('O'), dtype('float64'), dtype('bool')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0427cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2024/07', '2024/01', '2024/05', '2024/06', '2023/02', '2024/08',\n",
       "       '2024/04', '2023/09', '2022/05', '2024/02', '2023/07', '2023/03',\n",
       "       '2023/10', '2021/04', '2022/03', '2024/03', '2023/01', '2022/04',\n",
       "       '2023/11', '2023/06', '2022/02', '2022/12', '2022/10', '2023/12',\n",
       "       '2023/05', nan, '2021/06', '2023/04', '2021/08', '2021/02',\n",
       "       '2022/06', '2023/08', '2022/01', '2020/09', '2021/12', '2021/07',\n",
       "       '2021/03', '2022/11', '2020/12', '2018/11', '2021/01', '2021/11'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_posted_date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dc90923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title\n",
       "Others          889\n",
       "Job_Title_10     53\n",
       "Job_Title_5      51\n",
       "Job_Title_3      46\n",
       "Job_Title_8      40\n",
       "Job_Title_7      38\n",
       "Job_Title_2      32\n",
       "Job_Title_1      27\n",
       "Job_Title_13     11\n",
       "Job_Title_23     10\n",
       "Job_Title_11      9\n",
       "Job_Title_19      7\n",
       "Job_Title_12      7\n",
       "Job_Title_16      6\n",
       "Job_Title_20      6\n",
       "Job_Title_6       5\n",
       "Job_Title_4       5\n",
       "Job_Title_9       4\n",
       "Job_Title_25      4\n",
       "Job_Title_15      4\n",
       "Job_Title_17      4\n",
       "Job_Title_18      4\n",
       "Job_Title_21      4\n",
       "Job_Title_26      4\n",
       "Job_Title_14      3\n",
       "Job_Title_24      3\n",
       "Job_Title_22      3\n",
       "Job_Title_27      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "169b71ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary_category\n",
       "High      501\n",
       "Low       419\n",
       "Medium    360\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"salary_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6ec51f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_catagory(val):\n",
    "    if(val==\"High\"):\n",
    "        return 2\n",
    "    elif(val==\"Medium\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "aa489db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"salary_category\"]=df[\"salary_category\"].apply(job_catagory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c3fb46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>salary_category</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>2</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "      <td>Job_Title_1_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/01</td>\n",
       "      <td>2</td>\n",
       "      <td>WA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6681</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998258</td>\n",
       "      <td>-1.017181</td>\n",
       "      <td>0.372677</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>-0.440318</td>\n",
       "      <td>-0.442595</td>\n",
       "      <td>-0.761192</td>\n",
       "      <td>-0.606944</td>\n",
       "      <td>-0.123160</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.684944</td>\n",
       "      <td>-0.484077</td>\n",
       "      <td>-0.289621</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>-0.445414</td>\n",
       "      <td>-0.321164</td>\n",
       "      <td>-0.700979</td>\n",
       "      <td>-0.849695</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>0</td>\n",
       "      <td>NC</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4661</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7349</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426043</td>\n",
       "      <td>-0.175965</td>\n",
       "      <td>-0.147661</td>\n",
       "      <td>0.197479</td>\n",
       "      <td>-0.207085</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>-0.569806</td>\n",
       "      <td>-0.584224</td>\n",
       "      <td>0.143645</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>2</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>-0.138691</td>\n",
       "      <td>-0.478873</td>\n",
       "      <td>-0.115098</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>-0.157343</td>\n",
       "      <td>-0.859955</td>\n",
       "      <td>-0.689171</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>Others_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs    job_title job_posted_date  salary_category job_state feature_1  \\\n",
       "0    1       Others         2024/07                2        NY         A   \n",
       "1    2  Job_Title_1         2024/07                0        CA         A   \n",
       "2    3       Others         2024/07                0        CA         A   \n",
       "3    4       Others         2024/07                0        CA         A   \n",
       "4    5       Others         2024/07                0        CA         A   \n",
       "5    6       Others         2024/01                2        WA         A   \n",
       "6    7       Others         2024/07                0        CA         A   \n",
       "7    8       Others         2024/07                0        NC         A   \n",
       "8    9       Others         2024/07                2        CA         A   \n",
       "9   10       Others         2024/05                2        NY         A   \n",
       "\n",
       "   feature_2  feature_3  feature_4  feature_5  ...  job_desc_292  \\\n",
       "0     0.6429      False      False       True  ...     -0.499308   \n",
       "1     0.4678      False      False      False  ...     -0.415411   \n",
       "2     0.4610      False      False      False  ...      0.000000   \n",
       "3     0.5064      False      False      False  ...      0.000000   \n",
       "4     0.4640      False      False      False  ...     -0.654657   \n",
       "5     0.6681      False      False      False  ...     -0.998258   \n",
       "6     0.4510      False      False      False  ...     -0.684944   \n",
       "7     0.4661      False      False      False  ...      0.000000   \n",
       "8     0.7349      False      False      False  ...     -0.426043   \n",
       "9     0.7240      False      False      False  ...     -0.323800   \n",
       "\n",
       "   job_desc_293  job_desc_294  job_desc_295  job_desc_296  job_desc_297  \\\n",
       "0     -0.367894     -0.214881      0.014870     -0.271177     -0.113347   \n",
       "1     -0.341824     -0.319064      0.042322     -0.124755      0.023489   \n",
       "2      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4     -0.074398     -0.464479      0.081037     -0.136992     -0.276270   \n",
       "5     -1.017181      0.372677      0.025714     -0.440318     -0.442595   \n",
       "6     -0.484077     -0.289621      0.009149     -0.445414     -0.321164   \n",
       "7      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "8     -0.175965     -0.147661      0.197479     -0.207085      0.190253   \n",
       "9     -0.138691     -0.478873     -0.115098      0.004634     -0.157343   \n",
       "\n",
       "   job_desc_298  job_desc_299  job_desc_300            new  \n",
       "0     -0.587955     -0.919095     -0.207340       Others_A  \n",
       "1     -0.893224     -0.823024      0.112364  Job_Title_1_A  \n",
       "2      0.000000      0.000000      0.000000       Others_A  \n",
       "3      0.000000      0.000000      0.000000       Others_A  \n",
       "4     -0.696853     -0.601466      0.089939       Others_A  \n",
       "5     -0.761192     -0.606944     -0.123160       Others_A  \n",
       "6     -0.700979     -0.849695      0.004260       Others_A  \n",
       "7      0.000000      0.000000      0.000000       Others_A  \n",
       "8     -0.569806     -0.584224      0.143645       Others_A  \n",
       "9     -0.859955     -0.689171      0.005245       Others_A  \n",
       "\n",
       "[10 rows x 318 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ccb40ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "70f0edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=df.drop(columns=[\"salary_category\"])\n",
    "Y_train=df[\"salary_category\"]\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'bool':\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "        test[col] = test[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ecbf0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop(columns=[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ac3c65f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 311 numaric columns and 5 catagorical columns\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,OrdinalEncoder\n",
    "\n",
    "\n",
    "numaric_columns=X_train.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "catagory_columns=X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numaric_columns)} numaric columns and {len(catagory_columns)} catagorical columns\")\n",
    "num_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"mean\")),\n",
    "        (\"standard\",StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"odrinal\",OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transform=ColumnTransformer(\n",
    "    [\n",
    "        (\"numaric\",num_pipeline,numaric_columns),\n",
    "        (\"cat\",cat_pipeline,catagory_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929ecce",
   "metadata": {},
   "source": [
    "## LightGBM Model Train Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2800071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "\n",
    "# lgb_model = lgb.LGBMClassifier(\n",
    "#     objective='multiclass',\n",
    "#     num_class=len(np.unique(Y_train)),\n",
    "#     boosting_type='gbdt',\n",
    "#     learning_rate=0.05,\n",
    "#     n_estimators=2000,\n",
    "#     max_depth=12,\n",
    "#     num_leaves=30,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     min_child_samples=20,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# model=Pipeline(\n",
    "#     [\n",
    "#         (\"transform\",column_transform),\n",
    "#         (\"lgb\",lgb_model)\n",
    "#     ]\n",
    "# )\n",
    "# model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "59c02525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "42c89afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ccb53df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054078</td>\n",
       "      <td>-0.573635</td>\n",
       "      <td>-0.306883</td>\n",
       "      <td>-0.325092</td>\n",
       "      <td>0.089463</td>\n",
       "      <td>-0.353476</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.667958</td>\n",
       "      <td>-0.702116</td>\n",
       "      <td>-0.206267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/08</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.868718</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>-0.179036</td>\n",
       "      <td>-0.717763</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>-0.190448</td>\n",
       "      <td>-1.261702</td>\n",
       "      <td>-0.505897</td>\n",
       "      <td>0.082080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>2023/01</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416109</td>\n",
       "      <td>-0.619822</td>\n",
       "      <td>-0.493653</td>\n",
       "      <td>-0.347556</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>-0.331212</td>\n",
       "      <td>-0.381348</td>\n",
       "      <td>-0.506540</td>\n",
       "      <td>-0.773561</td>\n",
       "      <td>-0.105221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297560</td>\n",
       "      <td>-0.481448</td>\n",
       "      <td>-0.497642</td>\n",
       "      <td>-0.254823</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>-0.362739</td>\n",
       "      <td>-0.102704</td>\n",
       "      <td>-0.491272</td>\n",
       "      <td>-0.808156</td>\n",
       "      <td>-0.048326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176458</td>\n",
       "      <td>-0.726473</td>\n",
       "      <td>-0.323976</td>\n",
       "      <td>-0.145825</td>\n",
       "      <td>-0.046866</td>\n",
       "      <td>-0.229873</td>\n",
       "      <td>-0.568318</td>\n",
       "      <td>-0.614605</td>\n",
       "      <td>-0.770506</td>\n",
       "      <td>0.142140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 315 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_posted_date job_state feature_1  feature_2  feature_3  \\\n",
       "0       Others         2024/06        CA         A     0.6473          0   \n",
       "1       Others         2024/08        NY         A     0.4238          1   \n",
       "2       Others         2023/01        CA         A     0.6219          1   \n",
       "3  Job_Title_5         2024/06        NY         A     0.6704          0   \n",
       "4       Others         2024/05        CA         A     0.7310          0   \n",
       "\n",
       "   feature_4  feature_5  feature_6  feature_7  ...  job_desc_291  \\\n",
       "0          0          1          1          0  ...     -0.054078   \n",
       "1          0          0          0          0  ...     -0.868718   \n",
       "2          0          0          1          1  ...     -0.416109   \n",
       "3          0          0          1          0  ...     -0.297560   \n",
       "4          0          0          1          1  ...     -0.176458   \n",
       "\n",
       "   job_desc_292  job_desc_293  job_desc_294  job_desc_295  job_desc_296  \\\n",
       "0     -0.573635     -0.306883     -0.325092      0.089463     -0.353476   \n",
       "1     -0.337967     -0.179036     -0.717763      0.404843      0.032468   \n",
       "2     -0.619822     -0.493653     -0.347556      0.071679     -0.331212   \n",
       "3     -0.481448     -0.497642     -0.254823      0.047404     -0.362739   \n",
       "4     -0.726473     -0.323976     -0.145825     -0.046866     -0.229873   \n",
       "\n",
       "   job_desc_297  job_desc_298  job_desc_299  job_desc_300  \n",
       "0     -0.159314     -0.667958     -0.702116     -0.206267  \n",
       "1     -0.190448     -1.261702     -0.505897      0.082080  \n",
       "2     -0.381348     -0.506540     -0.773561     -0.105221  \n",
       "3     -0.102704     -0.491272     -0.808156     -0.048326  \n",
       "4     -0.568318     -0.614605     -0.770506      0.142140  \n",
       "\n",
       "[5 rows x 315 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03042f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 3, 1, 2, 1, 3, 2, 2, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 3, 2, 1, 3, 3, 1, 1, 3, 3, 2, 1, 2, 3, 2, 3, 2, 2, 3, 1, 3, 1,\n",
       "       3, 2, 1, 3, 3, 3, 2, 1, 3, 2, 3, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 2, 1, 3, 3, 2, 2, 3, 3, 1, 1, 2, 1, 2, 1, 2, 2, 2, 3, 3,\n",
       "       1, 1, 3, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1, 3,\n",
       "       1, 3, 1, 1, 3, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1,\n",
       "       2, 2, 3, 1, 3, 1, 1, 1, 1, 3, 2, 2, 1, 1, 3, 1, 1, 2, 2, 1, 2, 3,\n",
       "       1, 3, 3, 3, 3, 1, 2, 1, 2, 1, 1, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2,\n",
       "       2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 2, 3, 2, 3, 2, 1, 1, 2, 1, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 2, 3, 2, 3, 1, 3, 1, 2, 2, 1, 3, 1, 2, 1, 3, 2, 2,\n",
       "       2, 3, 3, 2, 1, 3, 3, 1, 2, 2, 1, 1, 1, 2, 3, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 1, 2, 2, 1, 3, 1, 2, 1, 2, 1, 3, 3, 1, 3, 1, 1, 2, 1, 3, 1,\n",
       "       1, 1, 3, 1, 3, 3, 1, 1, 2, 2, 2, 2, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3,\n",
       "       3, 1, 1, 2, 2, 1, 1, 1, 3, 1, 3, 3, 2, 1, 3, 2, 3, 2, 1, 3, 1, 1,\n",
       "       3, 1, 1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1,\n",
       "       1, 2, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2, 3, 2, 1, 3, 1, 2, 1, 2, 3, 1,\n",
       "       2, 1, 3, 2, 2, 2, 3, 1, 1, 3, 2, 1, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1,\n",
       "       2, 1, 3, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 2, 1, 2, 1, 2, 3, 1, 3, 3,\n",
       "       2, 2, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 2, 2, 3, 1, 3, 3, 1, 1, 2, 2,\n",
       "       2, 3, 1, 3, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 3, 1, 3,\n",
       "       2, 1, 3, 1, 3, 3, 1, 2, 1, 1, 1, 1, 3, 2, 1, 3, 1, 3, 1, 1, 2, 1,\n",
       "       3, 1, 1, 1, 1, 2, 2, 3, 2, 1, 1, 1, 3, 2, 1, 3, 2, 1, 1, 1, 3, 3,\n",
       "       3, 3, 2, 2, 2, 2, 1, 3, 1, 1, 3, 2, 3, 2, 3, 1, 3, 1, 3, 2, 3, 3,\n",
       "       1, 3, 1, 3, 2, 1, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 2, 1,\n",
       "       3, 2, 3, 1, 2, 3, 2, 3, 2, 3, 1, 3, 3, 3, 1, 2, 3, 3, 2, 1, 1, 1,\n",
       "       3, 1, 1, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 1, 1, 2, 3, 2, 3, 3, 2, 3,\n",
       "       1, 1, 3, 1, 2, 1, 1, 3, 1, 3, 2, 2, 3, 1, 1, 3, 3, 1, 1, 2, 1, 1,\n",
       "       1, 3, 1, 2, 3, 2, 3, 1, 1, 3, 3, 3, 3, 1, 3, 2, 2, 1, 3, 1, 3, 2,\n",
       "       1, 3, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2,\n",
       "       2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 2, 2,\n",
       "       1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 1, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1,\n",
       "       2, 2, 3, 3, 1, 1, 3, 1, 3, 1, 2, 2, 3, 1, 1, 2, 2, 1, 3, 2, 3, 2,\n",
       "       2, 2, 1, 3, 1, 3, 3, 3, 3, 2, 2, 2, 1, 2, 3, 3, 2, 1, 1, 3, 2, 1,\n",
       "       3, 3, 2, 1, 1, 1, 2, 3, 1, 1, 3, 1, 3, 3, 1, 2, 2, 2, 1, 1, 3, 3,\n",
       "       2, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 3, 2, 1, 3, 2, 3, 2, 1, 3, 2, 3,\n",
       "       3, 2, 3, 1, 1, 3, 1, 3, 1, 1, 2, 1, 2, 2, 3, 3, 2, 3, 1, 3, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 3, 2, 1, 3, 1, 3, 2, 3, 3, 3, 1,\n",
       "       1, 3, 2, 3, 3, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1,\n",
       "       1, 2, 2, 1, 3, 3, 3, 3, 1, 2, 1, 2, 2, 1, 2, 3, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y=model.predict(Test_X)\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5332794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Low', 'High', 'High', 'Medium', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Low', 'High', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'Low', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Mapping numeric classes to category labels\n",
    "label_map = {1: \"Low\", 2: \"Medium\", 3: \"High\"}\n",
    "mapped_preds = [label_map[val] for val in predicted_y]\n",
    "print(mapped_preds)\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": mapped_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"engineerr_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a27e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAH0CAYAAAB/6lR0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaBNJREFUeJzt3Qm8jGX7B/DbdrIcIgdJ9n2JRFEhRYlsUVlSSEdUsobKTiUVUpaXv62ypTcVFUIoZU+WkDXrK0sSkmWe/+d39T7zzpwzzzlzZu7zzPPM/L4+z+ecMzPXzJwxZ+ae+76v68pgGIahiIiIiDTKqPPKiIiIiIADDCIiItKOAwwiIiLSjgMMIiIi0o4DDCIiItKOAwwiIiLSjgMMIiIi0o4DDCIiItKOAwwiIiLSjgMMIiIiiq0Bxvjx41WxYsVU1qxZVY0aNdT69esjfZeIiIjIzQOMefPmqV69eqnBgwerzZs3qypVqqgGDRqo3377LdJ3jYiIyDVWr16tmjRpom666SaVIUMG9emnn6Yas3LlSnXbbbep6667TpUqVUrNmDEjegYYo0ePVomJiapjx46qQoUKatKkSSp79uxq2rRpkb5rRERErnHhwgX5kI5VgWAcOHBAPfTQQ+ree+9VW7ZsUT169FBPP/20WrJkSZpuN4MTu6levnxZBhMff/yxat68uff09u3bq7Nnz6rPPvssovePiIjIjTJkyKAWLFjg996aVL9+/dQXX3yhtm/f7j2tdevW8v67ePFid89gnDp1Sl27dk0VKFDA73T8/J///Cdi94uIiCja/fDDD6p+/fp+p2GLAk5Pi8wqSvz9999y+MLaEQ4iIqJo8nc6vufhg3ygD/jnzp1Tf/31l8qWLZt7BxgJCQkqU6ZM6sSJE36n4+cbb7wxYMzrr7+uhg4d6ndahozxKmOmXOl6X4mIKDpcvXw03W/jyqn9Wq7n9ffeT/aeh6SIIUOGKKdw5BJJXFycqlatmlq+fLn3NI/HIz/feeedAWNeeukl9ccff/gdGTLmtPFeExERpcJzTcsR6D0Pp+mAD/KBPuDnypUr6NkLx85gAFJUsamzevXq6o477lBjx46VnbDIKgkk0NQQNrOE6q9j36psN9UOOZ6IiCgZw6N0SM8tAPgg/+WXX/qd9vXXX1t+wHfdAKNVq1bq5MmTatCgQbIedOutt8ru1aTrQumFgwsiIooG58+fV3v37vVLQ0X66Q033KCKFCkiMx9Hjx5V77//vpzfpUsX9d5776m+ffuqp556Sq1YsUJ99NFHklni+jRVXTLHFYr0XSAiIpewZQ/G8Z1aridLwfJBXxZFs1DTIimsEqCAVocOHdTBgwflcr4xPXv2VD///LO6+eab1cCBA+VyacEBBhERkU0DjMvHdmi5nribKiqnc+QmT92wnwIHERER2cOxezB04n4KIiJyBI+eTZ5uEJEZDOTpIsPD9yhXrpz3/EuXLqnnnntO5c2bV8XHx6uWLVsmS5khIiJyZRaJoeFwgYgtkVSsWFEdP37ce3z33Xfe87CxZOHChWr+/Plq1apV6tixY6pFixa23j8uqRAREblwiSRz5swBq3KiWMjUqVPV7Nmz1X333SenTZ8+XZUvX16tXbtW1axZ05b7x2UVIiLSznNNxYqIzWDs2bNHetOXKFFCPf744+rQoUNy+qZNm9SVK1f8Gq1g+QS5umlttEJEROQoBpdI0lWNGjUk9xaFsyZOnChFP2rXrq3+/PNPKaqFUuG5c+eOaCdVLpEQERG5bImkYcOG3u8rV64sA46iRYtKpbC01DlPrbMcSnyEWi6cSyRERKSdxx2zD1FTBwOzFWXKlJFSptiXcfnyZXX27NmgO6ma3VSvv/56v8Pw/GnDvSciIgqOYXi0HG6Q0Sl10vft26cKFiwoXVSzZMni10l19+7dskcjpUYrurupcomEiIjSZQbDo+FwgYgskfTp00c1adJElkWQgooe9pkyZVJt2rSRmYdOnTpJN1U0YkF72G7dusngIqUMEt3dVLlEQkRE5LIBxpEjR2Qwcfr0aZUvXz5Vq1YtSUHF9zBmzBiVMWNGKbCFfRUNGjRQEyZMsPU+sl07ERFpZ7hj9kEHNjsjIiKyqdnZ37tWabme68rdo5zOEXswiIiIKLrERLMzIiIiRzBiZ4mEMxgWmEVCRETaeWIni4QDDAvc4ElEROSgAcbq1aslBRV9RpAm+umnn/qdjz2lgwYNkpoXqNqJniPoS+LrzJkz0p8EKaoowoW0VdTKICIicjWDvUhCduHCBVWlShU1fvz4gOePGjVKjRs3Tk2aNEmtW7dO5ciRQ9JQL1265L0MBhc7duxQX3/9tVq0aJEMWjp37qzsxCUSIiLSzhM7SyTpmqaKGYwFCxao5s2by8+4Kcxs9O7dW4ptASpuopEZmp+1bt1a7dy5U1WoUEFt2LBBVa9eXS6DpmiNGjWS+hmIDxbTVImIyFFpqluXaLme6yo3UE5n6x4MdE1FR1TfVuyo3IlmZ2YrdnzFsog5uABcHoW3MONBRETkVoZxTcvhBramqZrt1jFjYdWKHV/z58/vd37mzJmlbLid7dqJiIi0M9yxvKFD1NTBSKldu7mfIi2ZISwVTkRE2nliZ4Bh6xKJ2W4drdetWrHj62+//eZ3/tWrVyWzxM527RxcEBERuWSAUbx4cRkk+LZiP3funOytMFux4+vZs2fVpk2bvJdZsWKF8ng8slcjlHbtGCxwwEBERBFnxE6aqvYlEtSr2Lt3r9/Gzi1btsgeiiJFiqgePXqoESNGqNKlS8uAY+DAgZIZYmaalC9fXj344IMqMTFRUlmvXLminn/+eckwSSmDRHe7di6REBGRdh53bNB0ZJrqypUr1b333pvs9Pbt20sqKm5u8ODBavLkyTJTgVbtaMVepkwZ72WxHIJBxcKFC71t21E7Iz4+Pk33hWmqRETkpDTVSxv+reV6st7eUjkd27UTERHZNcBYP1/L9WS941HldOxFYoGVPImISDtP7FTy5ADDAvdfEBERhS5q6mAQERE5nuGO2QdXdlPt0KGDnO57IGvEad1UuURCRETaebhEkm7dVAEDiuPHj3uPOXPm+J3vhG6qXCIhIiJy0BJJw4YN5UgJ6lVYVeVEN1V0T/Xtpvruu+9KN9W33norTd1UiYiIHMXjjtkH127yRK0MNDQrW7as6tq1qzp9+rT3PHZTJSKiaGWwm2r6wfJIixYtpIrnvn371MsvvywzHhhYZMqUid1UiYgoenliZwbD9gEGSn6bbrnlFlW5cmVVsmRJmdWoV69eunRTJSIiohirg1GiRAmVkJDg7V/ilG6qRERE2hmx0+ws4gOMI0eOyB6MggULpls31VAwTZWIiLTzxE6aqq3dVHEMHTpUmpdhNgJ7MPr27atKlSqlGjRo4KhuqkxTJSIictAMxsaNG1XVqlXlgF69esn3gwYNkk2cW7duVU2bNpXuqSigVa1aNfXtt9/6DQ5mzZqlypUrJ3sykJ6KjqvovmonzmAQEZF2RuwskbCbKhERkU3dVP9aOkHL9WR74FnldBHfg+FUnMEgIiIKHZudWeAeDCIi0s5wx/KGDpzBsMAZDCIi0s4TO1kkHGAQERGR8wcYKHh1++23q5w5c0rJ7+bNm6vdu3f7XebSpUvqueeeU3nz5lXx8fGStnrixAm/yxw6dEg99NBDKnv27HI9L774ohTcsguXSIiISDsPZzBCtmrVKhk8rF27Vtqto47FAw88IG3cTT179lQLFy5U8+fPl8sfO3ZM+pOYrl27JoOLy5cvq++//17NnDlTzZgxQ1Jd7cIlEiIi0s5gmqo2J0+elBkIDCTq1KkjFTbz5cunZs+erR555BG5zK5du6TAFhqe1axZU3311VeqcePGMvAoUKCAXAZFt/r16yfXFxcXF9RtM02ViIgclab6+Vtaridb0z5KxfoeDAwoAFU8ASXAMauBFuwmFNUqUqSIDDAAX9EIzRxcACp9njt3Tu3YsSO97zIRERE5OU0V/UN69Oih7r77blWpUiU5DS3XMQORO3duv8tiMGG2Y8dX38GFeb55nh3dVLFEwn0YRESkleGO5Q3Hz2BgL8b27dvV3LlzVXpjN1UiInI8Dzd5hg0NyhYtWqS++eYbdfPNN3tPR5MzbN5Ex1RfyCIx27Hja9KsEvNnq5bturupcvaCiIjIQQMMLEtgcLFgwQJps168eHG/89HcLEuWLGr58uXe05DGirRUtGoHfN22bZv67bffvJdBRkquXLlUhQoVAt4umqXhfN8jnG6qRERE2hmxk0WSOT2WRZAh8tlnn0ktDHPPBJYssmXLJl/RRRVdVrHxEwOBbt26yaACGSSAtFYMJJ544gk1atQouY4BAwbIdSdtyZ5euAeDiIi087hjcODINFWrWYPp06erDh06eAtt9e7dW82ZM0c2ZiJDZMKECX7LH7/++qvq2rWrWrlypcqRI4dq3769GjlypMqcOfgxEdNUiYjIUWmqH4/Qcj3ZHhmgnI7t2omIiOwaYHw0TMv1ZHvMvsKToWIvEgus5ElERNoZhp7DBTjAICIiIncV2iIiIqLY3OQZkW6qdevWlc2gvkeXLl38LsNuqkREFHU8sVNoK3N6dVPFIAMDgpdfflnSTn/++WfJBjElJiaqYcP+t9kFA4mk3VSRVYJuqsePH1dPPvmk1M947bXXdN9lIiIiexjuGBw4coCxePFiv5/RZh0zEGhyhm6qvgMKq6qcS5culQHJsmXLpAfJrbfeqoYPHy7dVIcMGRJ0N9WkGzY5K0FERBSl3VRNs2bNUgkJCdIEDWW+L1686D1PdzdVDCzSOrhgFgkREUXTEsn48eNVsWLFVNasWVWNGjXU+vXrU7z82LFjVdmyZaVIZuHChVXPnj2ljpVju6lC27ZtVdGiRdVNN92ktm7dKjMT2KfxySefhNxNVTfOdhARkXZGZFJM582bJxW0J02aJIMLDB7wwR3vvVhlSAoVufv376+mTZum7rrrLvXLL79IsUzsmRw9enTkBxhmN9XvvvvO7/TOnTt7v8dMRcGCBVW9evXUvn37VMmSJUO6LbZrJyIiCgyDAux97Nixo/yMgcYXX3whAwgMJJLC/kdMDmBCADDz0aZNG7Vu3Trl2G6qgWA0BXv37g25myrbtRMRUawskfz999+ybcD3SPoh24QO5tgHWb9+fe9pGTNmlJ+xJSEQzFogxlxG2b9/v/ryyy9Vo0aNnNtNNZAtW7bIV8xkhNpNle3aiYgoVgYYrwf4UI3TAjl16pRkZwbaemC17QAzF8j0rFWrlmRwYnUBJSaQGerYbqpYBsH5GAXlzZtX9mBg4wgyTCpXrhxyN1WcnvQ8tmsnIqJo9NJLL8meCl86u42j0SjKQqARKVYZsMLQvXt3yegcOHBgZAYYEydOlK8Y6QTqpooUU6SfYoPJhQsXZGdqy5YtZQBhypQpkyyvoJsqZjPMbqq+dTPSG/dgEBGRU+tgXBfgQ7UVZGzifTXQ1gOrbQcYROBD/tNPP+3dL4n3bOyhfOWVV2SJxfYBRmrNWTGgQDGu1CDLBOs9kcLBBRER6WZ47M8iwQf7atWqqeXLl0t1bTPLEz9jS0MgKB2RdBCBQQoE24SdvUgscAaDiIiiRa9evWQloHr16uqOO+7wriKYWSWoll2oUCHvPo4mTZpI5knVqlW9SySY1cDp5kAjNRxgWODggoiItPNEplR4q1at1MmTJ9WgQYNkXyMqZKPytrnxE/2/fGcssG0B+xjx9ejRoypfvnwyuHj11VeDvs0MRrBzHS6UOa5QpO8CERG5xNXLR9P9Ni5O7KblerJ3fVc5HWcwiIiI7OKJ2s/06V8HA1kkSDdFzQocyAL56quvvOejjjnSTZGiGh8fLxkkSXe2RrpVO7AXCRERkYMGGKjaOXLkSKkAtnHjRnXfffepZs2aeZuUoebFwoUL1fz58yWb5NixY6pFixbJWrWj8hhKlc6cOVM6smLdyE7cg0FERNHU7MxutuzBQCfVN998Uz3yyCOyUQSFtvA97Nq1S5UvX17KldasWVNmOxo3biwDD3PzCWqmoyEaNqikpVU792AQEZGj9mC800XL9WTvPknFdLt2zEbMnTtXUmGwVIJZjStXrvjVQy9XrpwqUqSItx667lbtoeISCRERkcM2eaKPCAYU2G+BfRboS4LS3+g5ghmI3LlzW9ZDD7VVu+5uqkRERNoZ3OQZlrJly8pgAm1dUe4bxT1+/vlnlZ7YTZWIiBzPEzt7MNJlgIFZilKlSklpUrzxV6lSRb3zzjtS8xybN8+ePWtZDz2UVu3AbqpEREQxsgfDhJrnWL7AgANtX1H/3LR7925JS8WSSqit2gFNX8zUWPPg8ggRETmuDoZHwxGLezAwk9CwYUPZuPnnn39Kxgjavi5ZskSWLTp16iQ10ZFZgkFAt27dZFCBDJJQW7WnB/YiISIip3ZTjckBBmYe0DTl+PHjMqBA0S0MLu6//345f8yYMVLvHAW2MKuBDBH0m3dSq3bg4IKIiCh07EVCRERkVx2MN/7pXhqu7P2mK6djLxIiIiKbGC7JANGBAwwiIiK7eKJ20SAyWSRuxEqeRERELuqmWrduXUkf9T26dOniuG6q3ORJRETpkkViaDhicYnE7KZaunRpKdWNbqjopvrjjz+qihUrymUSExP9skIwkEjaTRVFtdBNFdkoyEpB/YzXXntN990lIiKyjyd2lki0DzCaNGni9/Orr74qsxpr1671DjAwoLCqyrl06VIpK75s2TLpQXLrrbeq4cOHSzfVIUOGpKmbKhEREcVAN1XTrFmzVEJCgqpUqZIU5rp48aL3PKd0UyUiItLOEzu9SGztpgpt27ZVRYsWVTfddJPaunWrzEygXPgnn3wSVjdVIiIix/NwiURLN1U0HPv444+lEueqVatkkNG5c2fv5TBTUbBgQVWvXj21b98+VbJkyZBvk+3aiYiIYrSbaiA1atSQr3v37g2rmyrbtRMRkeMZsZNFYms31UAw0wGYyQinm6rudu1ERETaedhNNV26qWIZBD83atRI5c2bV/Zg9OzZU9WpU0dqZ4TTTRXnJT2fyyNEREQx0E318OHDkn46duxYySwpXLiwdFXFAMJp3VSJiIh0M1ySAaIDu6mmUCqc1TyJiGKHHd1Uz/droeV64t/4J/PSydjsjIiIyC6eqP1MnwybnVng7AUREVHoOINBRERkFyN29mCk+wwGGp8hm6NHjx7e01DhE1khyCRBpU9s9Exa+8IJHVWJiIi08sROmmq6DjA2bNig/vWvf3lTUE1ITV24cKGaP3++VPg8duyYatGiRbKOqpcvX5aOqujIOmPGDDVo0KD0vLtERETk9AHG+fPn1eOPP66mTJmi8uTJ4z0dBbCmTp2qRo8ere677z6p9jl9+nQZSKDjqm9H1Q8//FC6qaKuBjqqjh8/XgYdREREbmR4DC1HTA8wsASCWYj69ev7nb5p0yZ15coVv9PLlSsnhbnQSRXYUZWIiKKSJ3aWSNJlkydatG/evFmWSJJCZU70KsmdO7ff6RhMmN1S2VGViIjI3bQPMFCts3v37tI/JGvWrMou7KZKRESO52EWSciwBIJy4bfddpvKnDmzHNjIOW7cOPkeMxHYR3H27Fm/OGSRmN1SQ+moym6qRETkeJ7YWSLRPsCoV6+edENFl1TzqF69umz4NL/PkiWLWr58uTdm9+7dkpaK3iOhdlTV3U0VpcKJiIi08sTOAEP7EknOnDlVpUqV/E5DwzLUvDBP79Spk+rVq5e64YYbZNDQrVs3GVTUrFkz5I6qurupspInERGRyyp5jhkzRmXMmFEKbGHfBDJEJkyY4D2fHVWJiCgaGdHbXzQZdlMlIiKyqZvqucQHtFxPrilLldOx2RkRERFpxwGGBW7yJCIi7Tzc5BnzuMmTiIh0M1wyONCBMxgWOINBRETksnbtdevWldN8jy5dujiqXTtnMIiISDsPl0jStV07JCYm+qWdYiCRtF07qnaiy+rx48fVk08+KQW6XnvtNWXXDAYHGUREpJVHxQzb27X7DigwgDAPFNwyOaFdOwcXRERELmrXbpo1a5ZKSEiQ6p4o833x4kXveWzXTkRE0brJ09BwuIHt7dqhbdu2qmjRouqmm25SW7duVf369ZN+JJ988olj2rVziYSIiLTzuGNw4Np27Z07d/Z+j5mKggULSpO0ffv2qZIlS4Z0u2zXTkREjudRMcP2du3YwJlUjRo15OvevXsd066dsxdEREQuateORmZJ4XTATEZ6tGvHcgfrWhARUaQZ3IORfu3asQwye/Zs1ahRIzkNezB69uyp6tSp401n1d2unbMRRETkCB4VM2wvFR4XF6eWLVumxo4dqy5cuKAKFy4sbdsxgDCxXTsREZG7sV27BWaREBHFFjvatZ95+B4t13PDglXK6djszAIHF0REpJ1HxQw2OyMiIiLtOINBRERkE4MzGKEbMmRIsk6p5cqV855/6dIlyQZBBkl8fLxs8Exa8yLSnVSJiIjShUfTEaszGBUrVpRMEe+NZP7fzSAl9YsvvlDz58+XYljPP/+8atGihVqzZo1jOqkSERGRAwcYGFAEqriJ4ldTp06VOhj33XefnDZ9+nRVvnx5tXbtWlWzZk1vJ1UMUNB/BN1U0UkV/UowO4I0VyIiIjcyXDL74NhNnnv27JFGZiVKlJAKnljyMMuIX7lyxa/DKpZPihQpIh1UgZ1UiYgoanliZ4lE+wADfUVmzJihFi9erCZOnKgOHDigateurf7880+pyIkZiNy5c/vFYDBhdkl1QidVYGlxIiJKjxkMQ8MRkwOMhg0bqkcffVTKfmPm4csvv1Rnz55VH330kUpP6KSKWQ7fI5waYqyDQURE0WT8+PGqWLFi0ukckwHr169P8fJ470ZSBvqEoRVHmTJl5D3dMXUwMFuBO4VOqdiXcfnyZbnTvpBFYu7ZCKWTanp0UyUiIoqWGYx58+apXr16qcGDB6vNmzerKlWqyCSAb1NRX3ivvv/++9XBgwfVxx9/rHbv3q2mTJmiChUq5JwBxvnz56XBGUZA1apVk2yQ5cuXe8/HncYeDfQcCbWTamrdVEPBJRIiIoqWAcbo0aNVYmKi6tixo7yXTpo0SUpBTJs2LeDlcfqZM2fUp59+qu6++26Z+bjnnntkYBKxAUafPn3UqlWrZNSDNNOHH35Ympe1adNGZhU6deoko6hvvvlGNn3il8WgAhkkSTup/vTTT2rJkiWpdlIFnIdBiO9hdlMlIiKKJn8H2BaA06xmI/B+65tgkTFjRvnZTLBI6vPPP5f3Zrz3Yh8kuqGjVARKSURsgHHkyBEZTJQtW1Y99thjUlALKaj58uWT88eMGaMaN24sBbbQoh3LHp988kmyTqr4il+uXbt2UgeDnVSJiMj1jAxajkDbAnBaIKdOnZKBQaAECqvkif3798vSCOKw72LgwIHq7bffViNGjAj6V2U3VSIiIpu6qf6nTl0t15Pn6yXJZiwwkx9opv/YsWOydwKrCuZ2BOjbt6+sOKxbty5ZDPZOovI2MkHxgd9cZnnzzTelAGYw2IvEAtu1ExGRU11nMZgIJCEhQQYJgRIorJInsG8SeybNwQWgKCZmPLDkEkzRS3ZTtcDBBRER6WZ4Mmg50gKDASRZ+CZYeDwe+dl3RsMXNnYi+xOXM/3yyy8y8Ai2ojYHGERERFGeRdKrVy9JM505c6bauXOn6tq1q7pw4YIkWgD2OiIb04TzkUXSvXt3GVighxg2eWLTZ7C4RGKBSyRERBQtWrVqpU6ePKkGDRokyxzo84WK2+bGT5SLQGaJqXDhwpLFiQalKJyJPRwYbKAvWMQ2eaIh2dChQ/1OQ0bJrl275Pu6devKphJfzzzzjOTkmvCLYvSEVFa0dG/fvr3sjvXtypremzw5wCAiii12bPI8euc/jT7DVeiHFcrpbG/XDij24Zt2imIfJqe0a+fggoiIdDNc0kfEde3afQcUVuezXTsREUUrI40bNN3M1nbtplmzZknaDCqDYVPJxYsXveexXTsREZH7ZU6vdu3Yd4HlDezHQLv27du3q5w5c6q2bduqokWLygBk69atMjOBfiRmNc/0aNdu9hVJy7IH92AQEZFuRtSWtrRhgIF27SbsPMWAAwMKtGtHH5LOnTt7z8dMBXJq69WrJw3RSpYsGfLtoqJZ0qpm2L+KfiQcKBARkRMYXCJJn3btgWAAAub5bNdORETkfra2aw9ky5Yt8tU83ynt2jnrQURE0VDJM2qWSNCuvUmTJrIsggYrgwcP9rZrx0Bj9uzZqlGjRtJlFXswUMQDXVWxnJK0XfuoUaNk30Ww7dqTnh9Ou3buwSAiIt0M7sEIv1376dOnpUV7rVq1vO3a0ZkN6adjx46VEqWoFIa27RhAJG3XjkJbmM3IkSOHFNqyu107BxdEREShY7t2IiIimyp57r/lAS3XU2LbUuV0bHaWSmorERGRLoaRQcvhBhxgEBERkXbspmqBezCIiEg3I4Z6kaTLDMbRo0dVu3btJFMkW7ZsUlBr48aN3vOx7QMtY5GaivPr168v5cV9oQ89yowjPRW1NFCkCymvduESCRER6eYxMmg5YnKA8fvvv6u7775bup9+9dVX0rjs7bffVnny5PFeBumn48aNkxbt69atk0wR9BtBlokJgwv0HkENDGSVrF692q8KKBERkdsYMbQHQ3sWSf/+/dWaNWvUt98GngHAzaEPSe/evaVmBqAoFvqNoIdJ69at1c6dO6UWxoYNG1T16tXlMosXL5b6GUiDRXwwmEVCREROyiLZXe5/7TTCUXbXVyrmZjA+//xzGRQ8+uijKn/+/Kpq1apqypQp3vMPHDggxbOwLGJCWW+UDEcnVcBXLIuYgwvA5TNmzCgzHkRERG5kxFAlT+0DjP3796uJEyeq0qVLqyVLlkjBrBdeeEHNnDnTryNqoI6p5nn4isGJr8yZM6sbbrgh5I6qREREkWYYeo6YzCLxeDwy8/Daa6/Jz5jBQKt27LdARc70klI3VSIiInL5DAYyQ5I2JStfvrw6dOiQX0fUQB1TzfPw1bfZGVy9elUyS6w6qurupsosEiIi0s3gEknokEGye/duv9N++eUXaX4GxYsXl0HC8uXLveefO3dO9lag9wjg69mzZ9WmTZu8l1mxYoXMjpjt3ZNiN1UiInI6TwylqWpfIkF31LvuukuWSB577DG1fv16NXnyZDkASxY9evRQI0aMkH0aGHAMHDhQMkOaN2/unfF48MEHVWJioiytXLlyRT3//POSYWKVQaK7myoRERE5aIBx++23qwULFsiMAjqgYgCB7qmoa2Hq27evdFNFXQvMVKDjKtJQs2bN6r3MrFmzZFBRr149yR5B11XUziAiInIrwyWzDzqwm2oKezC4TEJEFDvsqIOxtVgTLddT+eBC5XRsdmaBgwsiIqLQsdkZERGRTTwxtETCAQYREZFNDA4wiIiISDcjanc9OqRde4cOHSSF1PdAWqovtmsnIiJyr8zp1a793nvvlXbt+fLlU3v27PFr1w4YUEyfPt37c9IaFhhcHD9+XNq1ow5Gx44dJa119uzZyg7c5ElERLp5uEQSujfeeEMVLlzYb/CAWhhJYUBhVfYb7dpRF8O3Xfu7774r7drfeuutoNu1ExEROYkRQwMM29u1m1auXCnnly1bVjqunj592nse27UTERG5m+3t2s3lkffff1/6kWDGY9WqVaphw4bq2rVrIbdrRydV9DTxPcKpIcY9GEREpJuHvUjSt107eoqYsAG0cuXKqmTJkjKrgdLgoUA31aFDh/qdliFjvMqQKVdI18c9GEREpJuhYoft7doDKVGihEpISFB79+4NuV277m6qRERE5KJ27YEcOXJE9mBgcBJqu3ZsGkVKq+8RTjdVLpEQEZFunhhaIsmYHu3a165dK0skmJFAWilatT/33HNyPmpZvPjii3KZgwcPyj6MZs2aqVKlSqkGDRoka9eOdu9r1qxJtV27blwiISKi9MgiMTQcMTnAMNu1z5kzR1WqVEkNHz7cr117pkyZ1NatW1XTpk1VmTJlpIBWtWrV1LfffutXCwPt2suVKyd7MpCeipbuGKgQERGR87FduwW2ayciii12tGv/9sZHtFxP7f98rJyOvUgscHBBRES6Gcodyxs6cIBBRERkE0/UrhnY1OyMiIiIYpv2AUaxYsWSdUrFYWaRXLp0Sb5Hp9X4+HjVsmVLdeLECb/rQM2Mhx56SGXPnl0qeiLrBHUwiIiI3MyjMmg5YnKJBA3KzJLfgCqe999/v/QmMdNYv/jiCzV//nx1/fXXS/ppixYtJBUVEIvBBQpqff/999JR9cknn1RZsmTxVgclIiJyI8MlgwNXZJH06NFDLVq0SFq2oz8I2rejNsYjj/yzk3bXrl1S9wINzmrWrCkt3hs3bqyOHTumChQoIJdBmfF+/fqpkydPqri4OFuySIiIKLbYkUWyvEArLddT78Q8FdN7MC5fvqw+/PBD9dRTT8kyCSpzXrlyRTqjmlDrokiRIjLAAHxFfxJzcAEowIXByY4dO9Lz7hIREaUrj6ZDxXoWyaeffiolvzt06CA/oxMqZiDQit0XBhNml1R89R1cmOeb5xEREbmVEUNLJOk6wJg6daq0YbejvDfatePwhdWfcPqREBERkcOWSH799Ve1bNky9fTTT3tPw8ZNLJtgVsMXskjMLqn4mjSrxPzZqpOq2a4dm0Z9D8Pzp7cqJ5uXERFRpHliaIkk3QYY06dPlxRTZISY0HME2SBocGZC51WkpaKDKuDrtm3b/Nq1f/3119IdNWkb+GDbtaMqZ1orc3JAQkREunliaICRLkskaKuOAUb79u1V5sz/uwnMKqC5Wa9evdQNN9wgg4Zu3brJoAIZJPDAAw/IQOKJJ55Qo0aNkn0XAwYMkNoZvs3QksJ5Sc8PZ3mEpcKJiIgcNsDA0ghmJZA9ktSYMWNUxowZpcAW9kwgQ2TChAne89FtFWmtXbt2lYFHjhw5ZKAybNiw9LirREREtjFiaJMnu6laYDdVIqLYYkcdjIU3ttFyPU3+M0c5HZudWeDggoiIdPPE0AwGm50RERGRdpzBICIisomhYgcHGERERDbxqNhhe7v2unXrJjuvS5cuftfBdu1ERETuZnu7dkhMTPRLO8VAwsR27UREFK08MdS+QvsMBtqxY3BgHqhpUbJkSXXPPff4DSh8L4OCW6alS5eqn3/+Wbqw3nrrrdLLZPjw4Wr8+PFSZtwurORJRETpsQfD0HC4ga3t2k2zZs1SCQkJqlKlSlLi++LFi97znNKunWmqRERELmnXDm3btlVFixaVDqtbt25V/fr1k34kn3zySVjt2tlNlYiInM6jYoft7do7d+7s/R4zFQULFlT16tVT+/btk6WUUKGb6tChQ/1Oy5AxXmXI9L/ll7RgJU8iItLNE0OfeW1t1x5IjRo15OvevXvDateeUjdVIiKiWDd+/HjJ9MyaNau8965fvz6ouLlz58pqQPPmzZ3brj2QLVu2yFfMZITTrh2dVHEZ34PLI0RE5LRS4R4NR1rNmzdPOpkPHjxYbd68WVWpUkX2N/q+1wZy8OBB1adPH1W7dtpn9DPa2a4dyyDICNm0aZPc6c8//1xSUOvUqaMqV66crF37Tz/9pJYsWRJUu3bduDxCRETRkkUyevRoKRHRsWNHeY+dNGmSZHROmzbNMgZlIx5//HHZflCiRAlnDDCs2rXHxcXJeRhElCtXTvXu3Vvati9cuDBZu3Z8xWxGu3btZBBid7t2pqkSEVF67MHwaDiQ1IDsSt8jaaKDb0YnPtjXr1/fe1rGjBnlZ2RuWsH7LlYiOnXq5JxNnhhABOoCX7hwYbVq1apU45Fl8uWXX6pI4gwGERE51esBEhuw/DFkyJBklz116pTMRgTK0Ny1a1fA6//uu+8kUcPcxhAK9iIhIiJyWZrqSy+9JHsqfOnaRvDnn3/KNoUpU6ZIzapQcYBBRERkE0PT9WAwEeyAAoMEbDsIlKEZKDsT+yWxT7JJkyZ+eysB+ypRuyqYshLpWsmTiIiIIgv7H6tVq6aWL1/uN2DAz9jrmBT2SCKbE8sj5tG0aVN17733yvfY7hAM7QMMrPMMHDhQFS9eXGXLlk1GOcgc8d2Tge8HDRokqam4DDaa7Nmzx+96zpw5I7tXkW6aO3du2WRy/vx5ZRdu8iQiIqdu8kwrLKdgyWPmzJlq586dqmvXrurChQuSVQJIpsCyC6BOBlp5+B54H86ZM6d8jwFLRJZI3njjDTVx4kT5JSpWrKg2btwov8D111+vXnjhBbnMqFGj1Lhx4+QyGIhgQIJ8XDQ5wy8GGFygkypqYFy5ckWuA1VAZ8+erfsuExERRXWp8FatWqmTJ0/Kh3u03UAz0cWLF3s3fiLzE5klOmUwAqV7hKFx48Zyh7H71IRUVMxUoPEZbg6lw5GiiuIdgKqbiJkxY4Zq3bq1jK6Qp4vW79WrV5fL4IFo1KiROnLkiF/p8ZRkjiuk81cjIqIodvXy0XS/jSk3t9NyPYlHPlROp32J5K677pJ1nV9++UV+RrEspLugJwkcOHBARk+++biY3UDZUjMfF18xHWMOLgCXx+hq3bp1uu8yERGRbTMYHg2HG2hfIunfv78U/MAmEexaxZ6MV199VZY8fDuiBsrHNc/DVxT38LujmTOrG264IcWOqkRERE5mxFAHC+0DjI8++kjNmjVL9kpgDwZ2nPbo0UOWNVA6PL3obtfObqpEREQOWiJ58cUXZRYDeynQjh3FOnr27ClVx8DMuU0pHxdfkzZguXr1qmSWWHVUxfVjqcX3MDx/hvx7cHBBRES6eWJoiUT7AOPixYvJdqJiqcQs0oGsEQwSfPNxsaSCvRVmPi6+nj17Vmqnm1asWCHXYbZ3T4rt2omIyOk8MTTA0L5Egspf2HNRpEgRWSL58ccfpYub2fgMSxZYMhkxYoQqXbq0N00VSyhmr/ny5curBx98UDq/oeMb0lSff/55mRWxyiAJVNUsnHbtXCIhIiLdDBU7tA8w3n33XRkwPPvss7LMgQHBM888I7m3pr59+0qBD9S1wExFrVq1JA3VrIEB2MeBQUW9evVkRgSprqidYRcOLoiIiBxUB8NJwqmDwRkMIqLYYkcdjHeK6KmD0f2Q8+tgsNkZERGRTTwqdrDZGREREWnHGQwLXB4hIiLdPCp2RKSbaocOHSTDw/dA1oiTuqkSERHpZmg63CAi3VQBA4rp06d7f06aYhrpbqrc5ElEROSgAcb333+vmjVrph566CH5uVixYmrOnDlq/fr1fpfDgMKqKie6qSJt1bebKtJf0U31rbfeCrqbKhERkZN4YqgXie3dVE0rV66UhmZly5ZVXbt2VadPn/aex26qREQUjTys5Jl+3VTN5ZEWLVrIPo19+/apl19+WQYgGFggxgndVLk8QkRE5LJuqij5bUJDtMqVK8tmUMxqoHJnKNhNlYiInM5QscP2bqqBlChRQiUkJKi9e/c6ppsqERGRbh5laDncwPZuqoEcOXJE9mAULFhQfmY3VSIiikYe7sFIv26qqGUxdOhQaV6G2QjswUDzs1KlSqkGDRo4ppsqEREROajZ2Z9//imFthYsWODtptqmTRvpphoXF6f++usvacuOgQdmKXD+Aw88IMW4ChQo4L0eLIdgULFw4UK/bqrx8fG2NDsjIqLYYkezs2FF/5fwEI5Bv85STsduqkRERDYNMIZoGmAMccEAg83OiIiISDs2OyMiIrKJJ4a2BnKAQUREZBOPS1JMdeASCREREbljgIFMElTvLFq0qLRsR38SNC4zYV8pskpQ9wLno8/Inj17HNWuHZU8iYiIdDJiqF17ugwwnn76aWmz/sEHH6ht27ZJGioGEUeP/rNDd9SoUZJyihoXaF6WI0cOqYFx6dIl73VgcLFjxw65nkWLFqnVq1dLu3a7sEw4ERHp5omhQlva01RR5yJnzpzqs88+87Zsh2rVqklDM9S7QO2L3r17qz59+sh5qLqJGhgzZsyQYlpo116hQgW/du1o34527aj6GWy79nDSVNmLhIgottiRpvpSsbZaruf1g7NVzM1goGcIOqhmzZrV73QshaBt+4EDB6QjKmY0TOgbghLg6KYKbNdORETRiL1IwoDZC/QSwUzFsWPHZLDx4YcfyqDh+PHj3nbrvlU7zZ/N89iunYiIopHBPRjhwd4LrLwUKlRI+oNgvwXKhSdtgqYTWrWfO3fO74jiIqVERORCnhjag5Eu7/glS5ZUq1atkqyPw4cPq/Xr10vDMrRlN9utnzhxwi8GP5vnOaFdO7NIiIiIHFoHA9khSEX9/fff1ZIlS1SzZs1U8eLFZZCwfPly7+Uw24C9FVhaAbZrJyKiaOSJoT0Y6VLJE4MJLE+ULVtW7d27V7344ouqXLlyqmPHjtJCHTUyRowYoUqXLi0DDnRfRWYIuqwC27UTEVE0MlTsSJcBBmYPMKOAlFJszESr9VdffVVlyZJFzu/bt6+6cOGC1LXATEWtWrUkDdU382TWrFkyqKhXr55fu3a7cJMnERFR6NiunYiIyKY6GN2LtdZyPe8cnKucjr1ILHCTJxER6WZo+ucGHGBY4BIJERFR6DjAsMAZDCIi0s3DOhjp2021Q4cOkuHheyBrxEndVDmDQUREunliKE01It1UAQMKlA43jzlz5vhdR6S7qXIGg4iIyEXdVFH/AjMYSE/99NNPA16HE7qpEhFRbLEji6Rrsce0XM/Egx8pp7O9m6pp5cqV0tAMxbi6du2qTp8+7T3PCd1UOYNBRES6ebhEkn7dVM3lkffff1/Khb/xxhvStwSzG7gssJsqERFFI08MbfJMl0qe2Hvx1FNPSTfVTJkyqdtuu026qZq9RVDy23TLLbeoypUrS4M0zGqgcmeo3VRx+MLqD8uFExERxUA31UBwekJCgvQtAXZTJSKiaGSw0Fb6dVMNBBs3sQcDl3VKN1UukRARkW6eGFoiSZdeJIG6qWLT57fffivLGEOHDpXmZZiN2LdvnzQ/Q+0MpLSaHVGxJ+PEiRPebqroxIpNn7Nnzw76fjCLhIiInJRF8lSxR7Rcz7SDH6uYnMHA7MFzzz0nLdqffPJJ6ZaKQQe6qWJPxtatW1XTpk1VmTJlpIAWUlgx+PBtt45uqojHngykp+I6Jk+erOzCJRIiItLNiKElEnZTTWGAwWUSIqLYYccMRvtiLbVcz8yD/1ZOx14kFji4ICIiCh0HGBa4REJERLp5DEPLEbN1MKIBZzCIiEg3Q8UOzmBY4AwGERGRjQMMdDVt0qSJNBxDlcykDcuwZ3TQoEFS0wL9R9BDZM+ePWluxY5Mk9q1a0t6a+HChdWoUaNC/R2JiIgcwcNeJNYuXLigqlSposaPHx/wfAwExo0bJ/Ur0JgMxbYaNGigLl26FHQr9nPnzkmL96JFi0qxrTfffFMNGTLE1jRVLpEQEZFuBtNUgwzOkEEtWLBANW/eXH7GVWFmo3fv3qpPnz7emhgFChRQM2bMkB4kwbRinzhxonrllVeksVlcXJxcpn///jJbsmvXrqDvHwttERGRk9JUWxX95/0yXPN+9V89iPo9GAcOHJBBAZZFTOgJgvLe6KYabCt2XKZOnTrewQVgFmT37t1SdpyIiIhiKIvEbKWOGQtf+Nk8L5hW7PhavHjxZNdhnpcnT540dVM1N2xy2YOIiCLJ45LlDR2iJoskpW6qGFhwcEFERJFmxNAeDK0DDLOVOpqU+cLP5nnBtGLH10DX4Xsb6d1NlWmqREREDhlgYFkDA4Dly5f7ZYRgbwVasAfbih2XQWYJuqiakHGC7qyBlkcAjdKQ9up7YHkkVJzxICIi3Twx1K49zQMM1KvYsmWLHObGTnx/6NAheUPv0aOHGjFihPr888+l/Tq6qSIzxMw0KV++vHrwwQdVYmKiWr9+vVqzZo16/vnnJcMEl4O2bdvKBk/Ux0A667x589Q777yjevXqpfv3JyIiso1hGFqOUKC8RLFixaS+FD7Q4z3YypQpU6QWFT7U40AyRkqX1zLA2Lhxo6pataocgDd9fI/iWtC3b1/VrVs3qWtx++23y4AEaaj4hYJtxY79E0uXLpXBC1q5I+0V1+9bK4OIiIiCgw/qeL8ePHiw2rx5s9SzQnZm0i0LppUrV6o2bdqob775RjI7UfAS9amOHg0+lZft2i2wXTsRUWyxow5GsyKNtVzPZ4cWpenymLHAh/733ntPfsa2BAwaMCGAOlOpuXbtmsxkIB4rE8FgszMiIiKbeDRdT6DSDNiLiCOpy5cvy75HJEOYUHsKyx5mjarUXLx4UfZFoqREzKWp6sbZCyIiclNphtdffz3gZU+dOiUzECnVqEpNv379ZJ+kbyHN1HAGg4iIyCaGphoWmI1ImvgQaPZCh5EjR6q5c+fKvgzf/ZSO7KaKXayI9T3wCzipmyrrYBARkVO7qV4XoDSD1QAjISFBZcqUKcUaVVbeeusteX9G4kXlypWd300Vhg0bpo4fP+49sNHESd1UiYiIoiFNNS4uTjIyfWtUYZMnfjZrVFm9nw8fPlwyQX37h6XbEknDhg3lCAS/9NixY9WAAQNUs2bN5LT3339f1nkw04FaF6acOXNajpyQxopNKdOmTZMHpmLFilJrY/To0balqnIPBhERRYtevXqp9u3by0DhjjvukPdqTBh07NhRzkdmSKFChbz7ON544w1ZjZg9e7asOph7NeLj4+VwZDdVE6Zc8ubNKzU0MEOBcuEmJ3RT5RIJERFFSyXPVq1ayXIHBg233nqrfGjHzIS58RPFMrGaYJo4caJ80H/kkUdky4N54Doc200VXnjhBXXbbbdJusv3338vm1Xwi2GGItRuqrpxBoOIiHQzItioDFWzcQSCDZy+Dh48GPbtRSSLxHfnKzaNYKbimWeekamZUHfBptSunYiIiKK8m2ogWELBEok5Ygqlm2pK7dpDwSUSIiJyahaJG9jeTTUQrAWhqlj+/PlD7qaqu107ERFRNDU7s1ual0jQvGzv3r3en81uqthPUaRIEW831dKlS8uAY+DAgX7dVLGBEwOOe++9VzJJ8HPPnj1Vu3btvIMHdFMdOnSodFNF9bDt27dLN9UxY8ZY3q9AJVLZrp2IiMglAwx0U8XgIOl+CqS/zJgxQ7qpIvUF6aRnz56VTqm+3VQxCEBFMNS1wJ4JDEIwwPDdl2F2U33uueckdxdFQthNlYiI3M7jkuUNHdhN1QK7qRIRxRY7uqnWvTn4Xh4pWXlkmXI6NjuzwMEFERFR6NjsjIiIyCae6F00SIYDDCIiIpsYKnZwgEFERGQTTwwNMbS3a//kk0+kEyr6jOB8pLAmhc6qyBDBZdA0pWXLlskKa6Eu+kMPPaSyZ88u9TFefPFFv34lRERE5Fza27XjfKSmohObFaSlLly4UM2fP1+tWrVKHTt2TLVo0cJ7/rVr12RwgUYr6FUyc+ZMSYFFqioREZFbeWKokmdYaaqYoViwYIG3iJYvlP1GjYsff/xROreZUGEzX7580gIWXdpg165dqnz58lJ0q2bNmuqrr75SjRs3loGH2eRs0qRJUnTr5MmTfl1W0ytNlYiIYosdaao1b6qr5XrWHvNvTuZEtqepbtq0SUqA+7Z0L1eunFQBNVu64+stt9zi15UV7dpRdnzHjh223E/2IiEiInLRJk+0W8cMRO7cuS1buuNroJbv5nl2dFNlHQwiItLN45LlDR2iptCW7m6qREREuhma/rmB7QMMdFvF5k30KbFq6R5Ku3Z2UyUiInIO2wcYaF6WJUsWv5buu3fvlrRUs6U7vm7btk399ttvfu3ac+XKpSpUqBDwetFEDef7HuF0UyUiItLNYLv20Nu1nzlzRgYLyAAxBw/mzAMOLF2gDTu6pyIGA4Fu3brJoAIZJIA6GhhIPPHEE2rUqFGy72LAgAFSOyNpS3YiIiK38LhkeSMiaaorV670a9duMtu14+jYsWOy8wcPHiwt2s1CW71791Zz5syRjZnIEJkwYYLf8sevv/6qunbtKreXI0cOuf6RI0eqzJmDHxOxmyoRETkpTfW2grW0XM/m498pp2O7dgscYBARxRY7BhhVb7xby/X8+J81yunYi4SIiMgmnhhaIuEAwwJnL4iISDcjhgYYUVMHQzdW8iQiInJZN9W6devKeb5Hly5d/C7DbqpERBRtPIah5XCDiHRThcTERHX8+HHvgXRUJ3VT5RIJERHpZsRQJc8078Fo2LChHFZQu8LsppoSzExYVeVcunSp+vnnn9WyZcukBwm6sQ4fPly6qSLVNdhuqkRERBRjezBmzZqlEhISVKVKlaTM98WLF73nsZsqERFFI08MLZFEJIukbdu2qmjRorKPY+vWrTIzgYqf2L8RajdVIiIipzNcsrzh2gFG586dvd9jpqJgwYKqXr16at++fapkyZIhXafudu1ERETk8jTVGjVqyFezx0ko3VR1t2vnJk8iItLNE0NLJI4YYJiprJjJCLWbKtu1ExGR0xnMIkm/bqpYBpk9e7Zq1KiR1MrAHoyePXuqOnXqqMqVK4fcTRWnJz2PyyNEREQx0k318OHDql27dmr79u1SM6Nw4cLq4YcflgEEZiic0k2ViIhiix3Nzkom3Kblevad2qycjt1UiYiIbBpglEioquV69p/6UTmdI/ZgOBHrYBARkW6G4dFyuAEHGBaYRUJERBQ6tmsnIiKyicclGSA6ZI6l5Q7OShARUSQZ0bvtMX3btV+5ckXKfqM6JzI/cJknn3zSm7JqQirr448/LlkjuXPnVp06dZL0V19IX61du7bKmjWrZJr4dltNKwwsOLggIiJyabt2NCzbvHmzGjhwoHxFbxHUwWjatKnf5TC4QNMyFM9atGiRDFp8y4ejqRlqYaBfyaZNm9Sbb74pKa6TJ08O9fckIiJyxBKJR8PhBmGlqWIGY8GCBap58+aWl9mwYYO64447pK4FCnHt3LlTimjh9OrVq8tlFi9eLIW3jhw5IrMeEydOVK+88ooU2DJbs/fv319mS3bt2mVLmiqWVTjrQUQUO+xIUy2Up6KW6zn6uz2dxR2dRSIluzNkkKUQsxU7vjcHF1C/fn2VMWNGtW7dOu9lUNnTHFyY7doxG/L7778rO3BwQURE5NBNnpcuXZI9GW3atPFW6cSsRP78+f3vRObMUmrcbMWOr8WLF7ds154nT55076bKGQwiItLNw02e4cOGz8cee0ze5LHkkd7YTZWIiJzOiKFmZxnTc3CBfRdmF1QTGp75dkmFq1evSmaJ2Yo9lHbturupspInERHpZhiGliMmBxjm4GLPnj1q2bJl0jHVF1qxnz17VrJDTCtWrFAej0fVqFHDexlkluC6TBiolC1bNuDyCKCTKgYyvkc43VQ5g0FERGTjAAP1KtCeHYdvu3a0aMeA4JFHHlEbN25Us2bNUteuXZM9EzguX74sly9fvrx68MEHVWJiolq/fr1as2aNev7551Xr1q0lgwTatm0rGzxRHwPprPPmzVPvvPOO6tWrl7ILZzCIiEg3D9NUQ2vXjloVSTdnmr755htVt25d+R7LIRhULFy4ULJHWrZsqcaNG6fi4+P9Cm0999xzks6akJCgunXrJhtG04LdVImIyElpqgm5ymi5nlPnflFOx3btFphFQkQUWzjA0CsmepGEgoMLIiLSzRO9n+mT4QCDiIjIJkYMDTDSvZInERERxZ6IdFMtVqyYxPoeI0eOTLduqkRERE7giaEskoh0U4Vhw4ap48ePew9kiZjYTZWIiKKREUOFttK8B6Nhw4ZyBILy3CiI5eu9996Tbqqok4FuqqacOXNaVuVEDQ3UzZg2bZrUw6hYsaLU2hg9erRfW/f0xCwSIiIiF3VTNWFJBFU+q1atKjMUKBduYjdVIiKK1iwSj4bDDWzvpgovvPCCuu2226SD6vfffy99RLBMghmKULupEhEROZ3hkv0Tjh5gpNRN1bfkd+XKlWWm4plnnpGOqOgpEgq2ayciIqfzuGT2wZXdVANBkzMskRw8eDDkbqps105ERBTD3VQDwQZO9CTJnz9/yN1U2a6diIiczmAWScrdVPfu3ev92eymiv0UBQsWlG6qSFFdtGiRt5sq4HwshWAD57p166RhGjJJ8HPPnj1Vu3btvIMHdFMdOnSodFPFHo7t27dLN9UxY8ZY3i8srSRdXgmnXTsREZFuRgztwbC9myoGH88++6zatWuX7JnA5Z944gnZl+E7QGA3VSIiirZmZ9dlLazlev6+dFg5HbupEhER2TTAiLvuZi3Xc/nvI2mOQYFMlIXAygIKZr777rtSp8rK/PnzpXAm9keWLl1avfHGG6pRo0ZB3x57kVjgHgwiIoqWPRjz5s2TlYLBgwfLSgIGGKgv9dtvvwW8PEpIoMQEtir8+OOPqnnz5nJgy0KwOINhgWmqRESxxY4ZjCyaZtavpPG+Ilvz9ttvl+ra4PF4pM8Xth/0798/2eVbtWolrUGwn9JUs2ZNdeutt6pJkyYFdZucwSAiIrKJoenAHkb07fI9ktaCMqH1Bvp61a9f33saMjfxMxItAsHpvpcHzHhYXT7wLxvFLl26ZAwePFi+Mjb6YiN524x1fmwkb5ux9sRG+rYjafDgwcnGHTgtkKNHj8r533//vd/pL774onHHHXcEjMmSJYsxe/Zsv9PGjx9v5M+fP+j7GNUDjD/++EMeVHxlbPTFRvK2Gev82EjeNmPtiY30bUfSpUuX5H77HlYDpUgNMNK1FwkRERHpF6j2kxWUesiUKVPACtlW1bGtKmpbXT4Q7sEgIiKKYnFxcapatWpq+fLl3tOwyRM/o3J2IDjd9/JmRW2rywfCGQwiIqIo16tXLymIWb16dal9MXbsWMkS6dixo5z/5JNPqkKFCklfL+jevbu655571Ntvv60eeughNXfuXLVx40Y1efLkoG8zqgcYmD5Czm8oHVoZ6/zYSN42Y50fG8nbZqw9sZG+bTdp1aqVOnnypBo0aJAU2kK66eLFi1WBAgXk/EOHDklmiemuu+5Ss2fPVgMGDFAvv/yyFNr69NNPVaVKlYK+zaiug0FERESRwT0YREREpB0HGERERKQdBxhERESkHQcYREREpB0HGFGE+3XT5sCBA+rq1ashxYYaZxarwS7uWHis0Bth3759lj0S0uN5jdsK5/Yi/dyKNeG8bul4jPm6mX6iaoDxwQcfqLvvvlvddNNN6tdff5XTkOv72WefpduL088//6yeffZZVbVqVVWwYEE58D1Ow3kp+euvv9R3330X8HKXLl1S77//vkoLpFrt3LkzxcugTS9e/JI+ZuiqV6tWLcl1Tgk68SFf2rwc4itUqKDKlSsnqUwp/cGfOnVKjRo1Sj388MNSrAUHvn/zzTclfSolx48fVx9++KH68ssvpXGPL+RyDxs2TKVV2bJl1Z49e1K8DNK4tm3b5i1MM3z4cMkVx2N98803q5EjR1q+QJ05c0Y98sgjqkiRIqpr167q2rVr6umnn5bnCK4DaWD4vdLjuaX78QrmsZoxY4a3ERKev2jznCNHDlWmTBkVHx+vunTpYvm3hdP79Omj6tSpo9544w05bcSIERKXM2dO1bZtW2nmZAUFgBo1aqTy5MmjsmfPLge+x2nLli1L9ffT/bcYzOPFv8XgX7fC+TvU8fyiEBlRYsKECUZCQoIxYsQII1u2bMa+ffvk9OnTpxt169ZNMXbp0qVGw4YNjdy5cxsZM2aUA9/jtK+//toy7ssvvzTi4uKMmjVrSpMZ3Acc+P6uu+4yrrvuOmPx4sUBY3fv3m0ULVrUyJAhg9xenTp1jGPHjnnP/89//iOnB9KzZ8+ABy7/5JNPen8OpHLlyt7facqUKfJYvfDCC8bEiRONHj16GPHx8cbUqVMDxg4fPtzImTOn0bJlS+PGG280Ro4caeTNm1ce89dee83Ily+fMWjQoICx69evN/LkyWMUKlTIaN++vdG3b1858P3NN99s3HDDDcaGDRssY/H/kStXLrm/pUqVMrZv3x7UYwUPP/xwwAMx9evX9/4cSNmyZY3Vq1fL9/gd8fuOHj3a+Oqrr4yxY8caBQoUkMchkKeeesqoVKmS8e677xr33HOP0axZM3n8v/vuO+kJcPvtt8v/l+7nVjiPVziPVfHixY21a9fK93369DGKFStmfPLJJ8bOnTuNTz/91ChTpoz0PggEz9ebbrrJ6N27t1G+fHnj2WefNYoUKWJ8+OGH0g8Bv0O3bt0Cxs6YMcPInDmz0bp1a/l7x2OHA9+3adNGeiq8//77hpVw/hbDebxi7W8xnNetcP4Ow31+UeiiZoCBJ82CBQvke/xhmgOMbdu2yZPRSjgvTniBGDhwoOV1483glltuCXhe8+bNjYceesg4efKksWfPHvkeL9C//vprqn+oeCG89dZbZeDke+B0vGnh+3vvvTdgLF4UDh48KN9XrVrVmDx5st/5s2bNMipUqBAwtmTJksa///1v+X7Lli1GpkyZ5A/UhDcT/KEGUqNGDaNz586Gx+NJdh5Ow3l4Mw0EL9QdO3Y0rl27Zpw7d87o2rWr/J9u3rw51ccK8LjgDb5Dhw5+B2Lw/2D+HAjeyM3/EwwWPvroI7/zFy1aZPk7FyxY0FizZo33PuJ+YDBrwkADL/K6n1vhPF66HisMJvDi72vVqlXyoh5I4cKFvW+2+NvF7WFQYsLjhkFAIKVLlzbee+89wwoaNFn9H+n4Wwz18Yq1v8VwXrfC+TsM9/lFoYuaAUbWrFm9f6y+A4xffvlFzrMSzosTrnfXrl2WsTjP6rbRkW7r1q1+f9hdunSRF2Dc95T+UF9//XV5AVy+fLnf6Rgo7dixw0gJXgw2btzovQ94cfK1d+9eeeELBKebf+SAwZfvpxc8/tmzZw8Yi8cBn2St4DyrxwqftvApM+ljgNPxiSq1AcacOXPkk9m0adPS/HhhkPDDDz/I9/iUZL6QmvD8snq88FiYz0nz8cKA17R//34jR44c2p9b4Txe4TxWeIFesWKFfI+BU9JPwT///LPl75vac+vAgQOWzy28+YT6WIX7txjO4xVrf4vhvG6F83cY7vOLQhc1ezCKFy+utmzZEnDtrnz58pZxKI9av359y/Pr1aunjhw5EvC8YsWKqS+++MIyFucVLVrUcs03c+b/VWrPkCGDmjhxomrSpInUf//ll18sr7d///5q3rx5sq6PdcUrV66oYDVs2FBuB3A7H3/8sd/5H330kSpVqlTAWHTRM9eosb6MPQW+a9Y7duxQ+fPnt4xdv3695f3CeWbJ2kCwDp70McA68wMPPKC+//57lZLWrVurb7/9Vk2dOlW1bNlS/f777ypYWJd+9dVX5Xdt1qyZmjBhgt9a77vvvisldwNBad1FixbJ91999ZXKmjWrWrp0qff8JUuWyPNW93MrnMcrnMfq8ccfV6+88oo6e/aseuKJJ2Qt/vz583LexYsX1ZAhQ2SPQSDYp2Lu39iwYYP8Pfg+X9atWydr7oFUrFhR7q+VadOmyd4EK+H8LYbzeMXa32I4r1vh/B2G+/yiMBhRAmuY+NQ0d+5c+ZSETxZYjzS/t3LbbbdZrgsD1iZxmUAwTYfRd5MmTYx33nlHbhsHvm/atKmsoX/88ccBYzElaLX08txzz3n3g6Tkzz//lLVLTKfjkzFG5al9Ejh69KisjWOduVevXjKyr1WrlpGYmCin4T5/8cUXAWMHDBgga7tPP/20fBLp37+/fMrDmvGkSZNkGtJqDRWzRPikiTXmzz77TNbqceB7nIb7gdmiQGrXri23Ecgbb7wh15vaYwWY1sW6NO4n9i8E83idPXvWqF69usxiPfHEE/LJDp/U77//fnkMrr/+eu++g6QwZY2pa8TiPs6fP1/WgR977DFZksNjbTV7Fs5zS8fjFcpj9ffff8t9w6dZPD54rPCpELOE+DvEcyXpp1/TmDFj5PKYgkf8uHHjZG8B/v7wPMPjPGzYsICx33zzjVw/lozw/MNaPA58j78NzGhiecaKjr/FUB6vWP1bDOV1K5y/w3CfXxS6qBlgmC/oeAJiTQ8HBhz/93//l2JMuC9OWGNv1aqV/HHjBQEHvsdp2MhnBRuVsInUCtY28TsEAwMoTBviDzu1P1T4/fffjX79+sn6Lv7ocJ/xx9q2bVvLzV3mi+irr75qNG7cWO4/ppJx23gxw3Qv1prPnz9vGY83SKz/4o3T/D/C9zht3rx5KQ4e27VrZ3k+/r/wQh2sb7/9Vl6Ugn28Ll++LC+qjRo1MsqVKyf7C7Du/vLLLxuHDx9OMRb7LN566y3vXgzcHl4gsTkP+39SEupzS+fjldbHCrD3ApvoHnzwQeOBBx6QzYPYX5DSc8Pcc/D888/LpjvzbxNvaNWqVTOGDBkizz8rmOLGmwXemPH/gwPf43mO81Ki828xrY9XLP8tpvV1K5y/w3CfXxSaqGx2hulYTM1aTREmdfDgQZmqXLt2rbc+AaYRkbqF1DpMVzsdlnE2bdokSzpIvXIyTI0iTQ4SEhJUlixZbL8PeH6gPgNS+mKhk2I4+FiF9nhhaTYuLk45WaT/Fs3XLSxTI6WZoktUDjAo7VauXKlq1KihsmXLptwCBavw9MVgMFhYw8ULKtoS58uXL02398cff/gNQK+//nplN/y+qAOQKVOmNMeiTgXWsu2839gfgH1O2C9itZ9AJ9wW6jTg/7dEiRIqb968ab4Os1YHB1PBP14YKKAehd2Pmd2vAZRGhosh5QnpXcEckYBd85gutYJd48hnx3onUuR8/fHHH5IOltJUJdYxzd3rmPLEtCFuzyr/PSVYB8X9TcmJEyf8fv7xxx/lPqAuA6b8MeVoBallWOM8dOhQmu/b6dOn5fox/Yvd/VevXjU6derkrVtw5513+tUtCARpbJgONdeIcWDdFdO9vrvLrR5rpEGbcebt4rTUluBSgv9/q/XqK1euGK+88opM85v/n6NGjZI9DZhGx+OOPQ+6/4/XrVsnj69p4cKFch+wdwRTyTNnzrSMxVT9smXL5PszZ84Y9913n3f6Hb8nlkywJBAIliJRN8RcSkor/A1h+cj8PzKPu+++25upkR61cNz4GhDO3zHS983lub/++kv+z7DPCI8XllieeeYZ49KlS1H3GkChcfUAA+tmwR7p8SYQTuySJUvkjaJixYrywoh1UzPFD1JK98KGJewbadGihaRvYTOrWWRn6NChUgTnX//6V8BYqwEY/lDxhpnSgAz3x3xxwhsB3rCwBopNsthshRcYq/0quH7cR7wYNWjQQDYo4k00GOEUrAJs4ENRIhTZwZs2NndhYxfWc3F9KNCGNLdAzDd1XB4vvHjDwIHvX3rpJfl/ePPNN41Qnx9Wa/vYxIf1aWz+w/o8XlTx4op9RniTx/4ibKoLBJvYAh24Lbygmj+n9n/8+eefe4sg4Q0QGwrxf4waC4EgXdNMH8Rl8TzCz3gjwu+K2gp4UwgE9w1/C/iKN0nsW/ntt9+Cehzx+GMAhOeHORjEGxn2gmC/C/7/UtrPEG6hLre9BoTzdxxOMTW3vgZQjA4w7JDSm4BVZTrzwMjY6gUCI25sTgJs0MKbBT7FmcWJUnpxwQswNiwBXsDxguD7SRrf49NmILgsPkn6Dr5QtAm3hY15KQ3I8DiYL0x4IcIfva/u3bvLp1arWOyaRzE0ZEbgfmAXPP7gU/tUHU7BKvPxwqc7E95s8GZoFhrCpkmraot44U9p0xuuF2/8aanyaB54rKz+j0uUKCGzB4DiT7ic7++A+4QX3EDwPEKxKLxxmgfeLPHCjo2B5mmp/R8jowEvwr4Qb1WECZ8MzbofeONJ+iaFmQT8X6Z0u/h7w0Y8VJPEmy/eQPFmH6golAm3hcuYkKmCNzLzzQuZEXi+pkctHDe+BoTzdxxOMTW3vgZQ6KJmgIGR9alTp5KdjinZlKYow3kTwOlIYU1amc48kFZlFYtPGCik4wsvGPhUgjeWlF5ckhaNwR+9b9EYvCFhejcQ/CGiCiCmUH13TQdT7Mb3hcm38I0J9wGfBFKLBUxnYkodL+7mFKdVWeRwClaZj1fSTAL8vnixM5cFrB4v7OxP6cUPj5lVgR/cBqbYk1Z5NA+kdFr9H+N2faeSkxZHwu+MT2SB4P/f/ESHlMBQ/49R/Cnp8gKKVlk9VnjDwTQ04G8u6XIHpuLxvE/tdgHT7NjtX69ePXmM8GZgVdkUzw/f/1+8aeB3NafMMWjBG7eVcAp1ufE1IJy/43CKqbn1NYBCFzUDjKRPXhP+SPFktBLOmwBeUD/44APL68YLqlUsRu6B1oaRuoU/JkzdWcXi05nvmx5efH3/+PDiktILKnLKMR2MlDTzBS7YNx9cHmvDeANJWk0P51lVw/Odlk0Kyw34pGf1AlGlShXvJ0x8UsUb69tvv+09H4+V1ad5wJQ5alCYNm3aJJ+Ozb0GeLysbhtrtnijDjSVi3ichz0KgSD1OaU9Gik9P7A84ltdEuvjR44c8f6MwYbVmzXg/iJtE4NJDCrT8n+M/4+ffvpJ3kxQnTHpm63VcwtLFXis8Xji/wdvGObzC28AeMN95JFH0vz8wBsDloysZoqwF8u3zDYqReJ5aH46xX22GoyFWwvHja8B4fwdY8YF/6/44IbZLcxEmIPYCxcuSI0XpCZH02sAxfAAA8VhcOCPButs5s84sDaIQjl4EbASzpsActXRlCiU5RVMTVqt3eOTGwZFVreLjWu+031J4dNPSn9sJmwOw1ok1mqDKXZjbqgyNzom7Z2Ax9xqKtlqAOgLL3i6C1YBzsPeA7xRYOYGsb57AXD9VvtO8EaLxwgv6JjRwl4IHPgep+EToO8nKV8YoGLZyQreIKxqBqAnQ0p1MlCIy2oK3BfebDFljf0iafk/NjdnYq0/6ZufVX8MQMMo3A6mpPGpH9eF/x98xaf548ePh/z8sFomwXIRbhPPBwz48Mbqu7SDwlN4U0yPWjhufA0I5+84nGJqbn0NoNC5Pk0VqUaA0q9JfxXkdKOGxdtvv60aN24cML5jx47S2nn8+PEBz0cbYbR89m2rbELKIlK0UirZbGXBggVq9erVasyYMQHPnz17tpoyZYr65ptvkp23Zs0ayRm3Ko2LMrpIZXz++eeDSiNEieeNGzeq7du3p1hSedWqVX4/o304WnGb3nnnHWnf/OKLLwZ8nMeNGyetkUOB3xl1SlCbBG3OURYZLZpR8wQlndu3b59iPOqcoMU0/r8aNGigBg4cKKW7fUsto85DIH/++afEBqqTgjbPuXLlChiH28L14vmVVihPjeevVSlxPD9Q3vqxxx5L9bpOnz6tEhMT5bmE3wGtxK38+uuvfj+jpopvqqfZthxtwq3gbwYl0vfv3y/PQzxPUCIctQ7wdxrI0KFD5XkTymNllmL3/f/F7+v7+0NKKauh1sJx42tAOH/Hvi0YFi5cmOz/GH8PVvUs3PwaQKFx/QDDhBdi1JhHsZi0COdNIFrgBQJvonijtHoDICIiiskBhl0wYsYnmty5c3tPu3r1qjQX8v3kg5mAYKriMdaeWB3xVpUQUdgJzZTSCvfn2LFjMRPrxseKiMJguBgaPyHH3vw+pUMXbC4yW8EjCwM51dh9bK5ZmwdOw8Y0q/r2jLUnVkd8JOqkMNbeWKSiImPl0Ucf9RYLM6EAVkqZaIx1fqyOeEo7Vw8wsEHOTE3F91aHzicONnyZAwzsPMdOcGwiw073ixcvyoHvsXESaX7YVBQIY+2J1RHv1jdNxgYXiw8g2KiIDeHIZMCGQaRPmlJKF2Ws82N1xFMMDjAiwXeAgVRCtGa2gvPw5hUIY+2JDTc+tRL0yJawemFirPNjAVkxZtEqQP0ODEjNuhspvfkw1vmxOuIpNJlVlBg2bJjq06dPss2af/31l3rzzTfVoEGDtN8mNkbedNNNludjZ/WFCxcYG8HYcOOxU71169aW2RzYU4CMD8a6MxaQIYasBBO+X7FihWS9YN9Ijx49GOviWB3xFCIjSlgVccESis6Rqe8MRqNGjaSoTNImRYDTUJIbJZsDYaw9seHGo9bEhAkTQqqTwljnxwIKeK1evTrZ6agZgtkv1Nawimes82N1xFOMz2BguSdQiuVPP/2kbrjhhnS5zUmTJkmNDHwCvuWWW1SBAgW8LYS3bdsmGQqoB8DYyMWGG4/c/t27d1teN3L669Spw1iXxkKtWrXUJ598omrXru13Op4Xy5cvV/feey9jXRyrI55iNE01T548MrD4448/ktVxQH2L8+fPS1qpVSGttMIb1dSpU+XNyqwhsWTJkoAFeh544AFvIbBAGGtPrI54il5bt25VmzZtkkJQgaAA3b///W81ePBgxrowVkc8hchwObNTJFIOsVPYt4Mkyu2ilW+wUIMf6YwoPWsut6DuvW8ToXB07do14DQ9Y50TG8nbZqzzYyN524y1J1ZHPP3D9QMM08qVKwM2pErq9ddfl0Y9geLRca9+/fqSwmTus8DlW7Zsqb2GBmOdGRvJ22as82MjeduMtSdWRzz9I2rmhe+55x7pzZCa1157TZ05cybZ6f3791cjRoxQX3/9tYqLi/Oeft9998m0ug7hrEYx1p7YSN42Y50fG8nbZqw9sTri6R9RM8AI94mDzX4PP/xwstPz58+vTp06ZcM9IyIiih4xN8Cwgt4iyJdP6scff1SFChWKyH0iIiJyKw4w/guFevr16ycZBshEQdYB2gOjeFdKramJiIgoOQ4wfPZmlCtXThUuXFhSW5Efjdx5VHwbMGBApO8eERGRq0RNoa1w92Vg5mLcuHFSUhz7MTDIqFq1qipdurS222nXrp3U6mCsc2MjeduMdX5sJG+bsfbE6oinKCm0FW6hLMBySNasWdWOHTtCGlBMnz5dxcfHq0cffdTv9Pnz56uLFy+q9u3bMzbCsW6934y1J9at95uxwcfqiKc0MqLI1atXjfnz5xvDhg2TA98HUxvD7Lb3ww8/hHS7pUuXNlasWBGwtkaZMmUY64BYt95vxtoT69b7zdjgY3XEU9pEzQAD1TZLlChhZM+e3dumOUeOHEaxYsWMbdu2pRr/+eefG7Vq1Qrqskldd911xoEDB5KdjtOyZs3KWAfEuvV+M9aeWLfeb8YGH6sjntImajZ5Pv3006pixYrqyJEjavPmzXIcPnxYVa5cWXXu3DnVeGSKrF+/XlWpUkVly5ZNGqT5HilBrQzUug/UaC1v3ryMdUCsW+83Y+2Jdev9ZmzwsTriKY2MKIHRZ6CeIZiRCGZk6tvDJNCRkr59+xpFixaVqTcs0+BYvny5nNa7d2/GOiDWrfebsXx+MFZPrI54SpuoGWBUrlxZnihJ4bRKlSql623//fffxmOPPSYN17JkySJHpkyZjI4dO8p5jI18rFvvN2P5/GCsnlgd8ZQ2rs4iOXfunPf77777TvXt21cNGTJE1axZU05DD5Fhw4apkSNHSvZISg4dOpTi+UWKFEn1/uzZs0dt2bJFllhuueUWVbRo0aB/F8baE+vW+81YPj8YqydWRzwFx9UDjIwZM0rVTZP5q5in+f587dq1NF1XUqnFJ70samngSZsnT56g4xhrX2wkb5uxzo+N5G0z1p5YHfGUCsPFkFoU7JGaLVu2+B0bNmwwJk+ebJQrV87497//nWJs9+7djf/7v/+T77Gmd/fdd8sUHLJYvvnmG8Y6INat95uxfH4wVk+sjnhKG1cPMOywaNEi45577knxMoUKFZIBCSxYsMAoWLCgsXv3bmPAgAHGXXfdxVgHxLr1fjOWzw/G6onVEU8xPMD4/fffjbfeesvo1KmTHKNHjzbOnj0b1nXu2bNHamukllt9+PBh+T4xMVFGybB//34jZ86cjHVArFvvN2P5/GCsnlgd8RSjdTA2btyoSpYsqcaMGaPOnDkjx+jRo+U01MQIZsOo7/HHH3+oXbt2SaOz1MqHFyhQQP3888+ynrd48WJ1//33y+koPZspUybGOiDWrfebsXx+MFZPrI54itFmZz179lRNmzZVU6ZMUZkz//NrXb16VQpw9ejRQ61evTrF+Ny5cyfb5IkZHnRXnTt3boqxHTt2VI899pj0N8F11K9fX05ft26ddGhlbORj3Xq/GcvnB2P1xOqIpxjKIvGFdKMff/wx2ZMEo9Xq1avLCDUlq1atSpZVki9fPlWqVCnvgCUlH3/8sVQORROdm2++WU6bOXOmDFyaNWvGWAfEuvV+M5bPD8bqidURTzE4wMDU1wcffKAeeOABv9OXLFkiZcBPnDiRYjxmOO66665kgwnMgnz//feqTp06Qd2PS5cuSWfWUDDWnthI3jZjnR8bydtmrD2xOuIpdVGzB6NVq1aqU6dOat68eTI6xYGlDSyRtGnTJtX4e++9V/ZtJIW9GDgvJVjPGz58uCpUqJC0At6/f7+cPnDgQGkNz9jIx7r1fjOWzw/G6onVEU9pZEQJlHl94YUXjLi4OCNjxoyS24wdwz169DAuXbqUajwu/9tvvyU7HSlMqe0uHjp0qHRy/fDDD41s2bIZ+/btk9Pnzp1r1KxZk7EOiHXr/WYsnx+M1ROrI57SJmoGGKYLFy4YW7dulQPfp+bhhx+WA4OSRo0aeX/G0bRpU2n33qBBgxSvo2TJksayZcvk+/j4eO+TdufOnUbu3LkZ64BYt95vxvL5wVg9sTriKW1cnUXSokULNWPGDJUrVy75PiWYDkM79y5duqjrr7/ee7r5PQZbOXPmlM2ipri4OOlrkpiYmOJ1Hz16VDaDJuXxeNSVK1cY64BYt95vxtoT69b7zdjgY3XEUwztwcDgwEwtxfcpHdisOWnSJPXEE0/4Xcf06dPlGDx4sKzBmT/j+Ne//qVeeukllZCQkOL9qFChgvr2228D7lauWrUqYx0Q69b7zVh7Yt16vxkbfKyOeEojI4bs2LEj1aqcofj000+N66+/3hg5cqRc/5tvvmk8/fTTsh9k6dKljHVArFvvN2P5/GCsnlgd8ZQ2MTXAQHMbNDKzMn/+fOPRRx81atSoYVStWtXvSM3q1auN+vXrG/ny5ZPNQ2iis2TJkqDuF2PtiXXr/WYsnx+M1ROrI56CFzV1MMI1btw49corr6gOHTqoyZMnS8W3ffv2qQ0bNqjnnntOvfrqq5G+i0RERK7BAcZ/oQIo9mGgZgY2e/7000+qRIkSatCgQVIf47333ov0XSQiInINDjD+K3v27Grnzp2qaNGiKn/+/Orrr79WVapUUXv27JFMktOnT/tdPk+ePMl6l1hJWsCLsfbERvK2Gev82EjeNmPtidURT6FzdZqqTjfeeKM8uTDAKFKkiFq7dq0MMA4cOCAprEmNHTvW+z0GHyNGjFANGjRQd955p5z2ww8/SJlyVIhjbGRi3Xq/GcvnB2P1xOqIpzCkYb9GVOvUqZMxZMgQ+f69996TzT/YCITiK0899VSKsS1atDDefffdZKfjtGbNmjHWAbFuvd+MtSfWrfebscHH6ointOEA47+uXbtmXLlyxfvznDlzjG7duhnjxo2TMuQpyZEjh7Fnz55kp+M0nMfYyMe69X4z1p5Yt95vxgYfqyOe0sbVhbZ0Qnt2306qrVu3lsySbt26SUXPlOTNm1d99tlnyU7HaTiPsZGPdev9Zqw9sW6934wNPlZHPKUN92D4QIU3VO9Eeioqu6HjHlrAFy9eXNWqVcsybujQodK1deXKlapGjRpy2rp169TixYvVlClTUrxNxtoT69b7zVg+PxirJ1ZHPKVRGmc8otbHH38s+y5Q1Q1dWM0mOFiba9iwYarxa9euNdq2bestzIXvcVowGGtPrFvvN2P5/GCsnlgd8RQ8DjD+69ZbbzVmzpyZrMve5s2bjQIFCmi5jddff934/fffGevg2EjeNmOdHxvJ22asPbE64ukfHGD8F2YvDhw4kGyAga+Y0dAhZ86c3utlrDNjI3nbjHV+bCRvm7H2xOqIp39wk6dPHYy9e/cmO/27776Tip46hFPTjLH2xEbythnr/NhI3jZj7YnVEU//4ADjvxITE1X37t1lww+qvh07dkzNmjVL9enTR3Xt2jXSd4+IiMhVYjqLZOvWrapSpUqSovrSSy8pj8ej6tWrpy5evKjq1KmjrrvuOhlgIFWViIiIghfTA4yqVauq48ePS+8RLIOgc+qLL74oSyXnz59XFSpUUPHx8ZG+m0RERK4T0wOM3LlzS68RDDAOHjwoMxgoqoWBBREREYUupgcYLVu2VPfcc48qWLCg7LuoXr26ypQpU8DL7t+/P+zbq127tsqWLRtjHRwbydtmrPNjI3nbjLUnVkc8/SPm27WjghuWRF544QU1bNgwlTNnzoCXwwbQlFy7dk0tWLBAWr5D+fLlVfPmzf3KjzM2srFuvd+M5fODsXpidcRTGvw3XTXmdejQwTh37lxIsdu3bzdKlChhZM+e3VsdDo1zihUrZmzbto2xDoh16/1mLJ8fjNUTqyOe0oYDDA1q1qxpNGnSxDhz5oz3NHzftGlT484772SsA2Lder8Zy+cHY/XE6ointOEAQ4OsWbPKyDgpjIhxHmMjH+vW+81Ye2Lder8ZG3ysjnhKGxba0qBMmTLqxIkTyU7/7bffVKlSpRjrgFi33m/G2hPr1vvN2OBjdcRTGqVxQEL/9ccff3iPL774wqhYsaIxf/584/Dhw3Lg+1tuuUXOY2xkYt16vxnL5wdjnfF/TOGJ+SySUKH6J1JbTebDaJ7m+zN2LTPW/li33m/G8vnBWD2xOuIpdMzLCdE333zDWIfHRvK2Gev82EjeNmPtidURT6HjDAYRERFpxxkMTc6ePaumTp3qLd5SsWJF9dRTT6nrr7+esQ6Jdev9ZiyfH4zVE6sjnoLHGQwNNm7cqBo0aCClZe+44w45DY3T/vrrL7V06VJ12223MTbCsW6934zl84OxemJ1xFMahblJlAzDqFWrllQCvXLlivc0fN++fXujdu3ajHVArFvvN2P5/GCsnlgd8ZQ2HGBogAItO3fuTHb6jh07jGzZsjHWAbFuvd+MtSfWrfebscHH6ointGGhLQ1y5cqlDh06lOz0w4cPWzZPY6y9sW6934y1J9at95uxwcfqiKc0SuOAhALo1q2bcfPNNxtz5841Dh06JMecOXPktO7duzPWAbFuvd+M5fODsXpidcRT2nCAocHff/9tvPDCC0ZcXJyRMWNGI0OGDMZ1111n9OjRw7h06RJjHRDr1vvNWD4/GKsnVkc8pQ2zSDS6ePGi2rdvn3xfsmRJlT17dsY6LNat95uxfH4wVk+sjngKDutghKhFixZqxowZsqaH71MSHx8vudZdunSRXGvG2hMLbrzfjLUnFtx4vxkbfCyEG0+h4wAjRHjymbXsU3si/v3332rSpElqzZo16vPPP2esTbFmjNvuN2PtiTVj3Ha/GRt8rBkTTjyFIY1LKhQipEFlz56dsQ6OjeRtM9b5sZG8bcbaE6sjnv6HAwybXL161diyZQtjHRwbydtmrPNjI3nbjLUnVkc8/Q83eRIREZF2LLRFRERE2nGAQURERNpxgEFERETacYBBRERE2nGAQURERNpxgEFERETacYBBRERE2nGAQUREREq3/wfghtK5k+eTEgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "sns.heatmap(X_train.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661420de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('float64'), dtype('int32')], dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(\n",
    "    [\n",
    "        (\"transformer\",column_transform)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1ddba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46477322, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.14717881, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.20977889, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.12256281, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.0659181 , -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.0670786 , -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_trained_processed = pipeline.transform(X_train)\n",
    "X_trained_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 421)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trained_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbdca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254151e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ab36c",
   "metadata": {},
   "source": [
    "### he he he some help from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68203ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Subtract 1 from Y_train to make the labels zero-indexed\n",
    "Y_train_zero_indexed = Y_train - 1\n",
    "\n",
    "# Now apply to_categorical with the correct number of classes\n",
    "Y_train_one_hot = to_categorical(Y_train_zero_indexed, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5648",
   "metadata": {},
   "source": [
    "# Work with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9712e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,020</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_90 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │        \u001b[38;5;34m75,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │           \u001b[38;5;34m720\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m18,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_66 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m2,020\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_67 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,343</span> (380.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97,343\u001b[0m (380.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,743</span> (377.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m96,743\u001b[0m (377.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> (2.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m600\u001b[0m (2.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "\n",
    "model=Sequential(\n",
    "    [\n",
    "        Dense(180,activation=\"relu\",input_shape=[X_trained_processed.shape[1]]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(100,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "        \n",
    "        Dense(20,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(3,activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='accuracy', factor=0.3, patience=10, min_lr=1e-6\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy', patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='ai/ann_best_model.keras',      \n",
    "    monitor='accuracy',                 \n",
    "    mode='max',                        \n",
    "    save_best_only=True,                 \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfa89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(.01),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['accuracy', 'AUC', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca42be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9970 - Precision: 0.9694 - accuracy: 0.9684 - loss: 0.1027\n",
      "Epoch 1: accuracy improved from -inf to 0.95996, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - AUC: 0.9969 - Precision: 0.9692 - accuracy: 0.9681 - loss: 0.1037 - val_AUC: 0.8435 - val_Precision: 0.6988 - val_accuracy: 0.6914 - val_loss: 1.3987 - learning_rate: 0.0100\n",
      "Epoch 2/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9940 - Precision: 0.9476 - accuracy: 0.9447 - loss: 0.1462\n",
      "Epoch 2: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9938 - Precision: 0.9478 - accuracy: 0.9443 - loss: 0.1483 - val_AUC: 0.8564 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.1937 - learning_rate: 0.0100\n",
      "Epoch 3/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9948 - Precision: 0.9534 - accuracy: 0.9490 - loss: 0.1312\n",
      "Epoch 3: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9945 - Precision: 0.9539 - accuracy: 0.9487 - loss: 0.1320 - val_AUC: 0.8554 - val_Precision: 0.7211 - val_accuracy: 0.7188 - val_loss: 1.2396 - learning_rate: 0.0100\n",
      "Epoch 4/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9940 - Precision: 0.9612 - accuracy: 0.9500 - loss: 0.1425\n",
      "Epoch 4: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9934 - Precision: 0.9607 - accuracy: 0.9501 - loss: 0.1463 - val_AUC: 0.8437 - val_Precision: 0.7120 - val_accuracy: 0.7148 - val_loss: 1.3139 - learning_rate: 0.0100\n",
      "Epoch 5/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9929 - Precision: 0.9561 - accuracy: 0.9424 - loss: 0.1490\n",
      "Epoch 5: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9930 - Precision: 0.9561 - accuracy: 0.9426 - loss: 0.1485 - val_AUC: 0.8564 - val_Precision: 0.6996 - val_accuracy: 0.6953 - val_loss: 1.2571 - learning_rate: 0.0100\n",
      "Epoch 6/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9945 - Precision: 0.9411 - accuracy: 0.9381 - loss: 0.1415\n",
      "Epoch 6: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9940 - Precision: 0.9418 - accuracy: 0.9394 - loss: 0.1452 - val_AUC: 0.8345 - val_Precision: 0.6908 - val_accuracy: 0.6914 - val_loss: 1.3936 - learning_rate: 0.0100\n",
      "Epoch 7/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9947 - Precision: 0.9624 - accuracy: 0.9522 - loss: 0.1424\n",
      "Epoch 7: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9947 - Precision: 0.9623 - accuracy: 0.9521 - loss: 0.1422 - val_AUC: 0.8295 - val_Precision: 0.6996 - val_accuracy: 0.7031 - val_loss: 1.4193 - learning_rate: 0.0100\n",
      "Epoch 8/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9926 - Precision: 0.9567 - accuracy: 0.9523 - loss: 0.1439\n",
      "Epoch 8: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9927 - Precision: 0.9564 - accuracy: 0.9522 - loss: 0.1442 - val_AUC: 0.8517 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.3213 - learning_rate: 0.0100\n",
      "Epoch 9/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9924 - Precision: 0.9483 - accuracy: 0.9424 - loss: 0.1514\n",
      "Epoch 9: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9925 - Precision: 0.9477 - accuracy: 0.9424 - loss: 0.1529 - val_AUC: 0.8512 - val_Precision: 0.7222 - val_accuracy: 0.7188 - val_loss: 1.2713 - learning_rate: 0.0100\n",
      "Epoch 10/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9967 - Precision: 0.9691 - accuracy: 0.9659 - loss: 0.1023\n",
      "Epoch 10: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9961 - Precision: 0.9672 - accuracy: 0.9637 - loss: 0.1095 - val_AUC: 0.8488 - val_Precision: 0.7441 - val_accuracy: 0.7383 - val_loss: 1.2898 - learning_rate: 0.0100\n",
      "Epoch 11/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9941 - Precision: 0.9561 - accuracy: 0.9535 - loss: 0.1315\n",
      "Epoch 11: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9943 - Precision: 0.9566 - accuracy: 0.9541 - loss: 0.1292 - val_AUC: 0.8452 - val_Precision: 0.7244 - val_accuracy: 0.7188 - val_loss: 1.3606 - learning_rate: 0.0100\n",
      "Epoch 12/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9919 - Precision: 0.9585 - accuracy: 0.9489 - loss: 0.1567\n",
      "Epoch 12: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9927 - Precision: 0.9591 - accuracy: 0.9505 - loss: 0.1488 - val_AUC: 0.8508 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.3921 - learning_rate: 0.0030\n",
      "Epoch 13/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9960 - Precision: 0.9709 - accuracy: 0.9645 - loss: 0.0897\n",
      "Epoch 13: accuracy improved from 0.95996 to 0.96191, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9960 - Precision: 0.9692 - accuracy: 0.9637 - loss: 0.0945 - val_AUC: 0.8462 - val_Precision: 0.7331 - val_accuracy: 0.7305 - val_loss: 1.3244 - learning_rate: 0.0030\n",
      "Epoch 14/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9960 - Precision: 0.9678 - accuracy: 0.9610 - loss: 0.1022\n",
      "Epoch 14: accuracy improved from 0.96191 to 0.96484, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9962 - Precision: 0.9686 - accuracy: 0.9617 - loss: 0.1011 - val_AUC: 0.8498 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.3150 - learning_rate: 0.0030\n",
      "Epoch 15/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9959 - Precision: 0.9511 - accuracy: 0.9501 - loss: 0.1264\n",
      "Epoch 15: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9959 - Precision: 0.9513 - accuracy: 0.9503 - loss: 0.1261 - val_AUC: 0.8493 - val_Precision: 0.7194 - val_accuracy: 0.7148 - val_loss: 1.3294 - learning_rate: 0.0030\n",
      "Epoch 16/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9973 - Precision: 0.9606 - accuracy: 0.9584 - loss: 0.1012\n",
      "Epoch 16: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9970 - Precision: 0.9604 - accuracy: 0.9585 - loss: 0.1041 - val_AUC: 0.8486 - val_Precision: 0.7331 - val_accuracy: 0.7305 - val_loss: 1.3227 - learning_rate: 0.0030\n",
      "Epoch 17/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9944 - Precision: 0.9556 - accuracy: 0.9519 - loss: 0.1289\n",
      "Epoch 17: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9945 - Precision: 0.9559 - accuracy: 0.9523 - loss: 0.1280 - val_AUC: 0.8499 - val_Precision: 0.7165 - val_accuracy: 0.7148 - val_loss: 1.4177 - learning_rate: 0.0030\n",
      "Epoch 18/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9972 - Precision: 0.9707 - accuracy: 0.9664 - loss: 0.0892\n",
      "Epoch 18: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9972 - Precision: 0.9712 - accuracy: 0.9669 - loss: 0.0888 - val_AUC: 0.8485 - val_Precision: 0.7189 - val_accuracy: 0.7109 - val_loss: 1.3645 - learning_rate: 0.0030\n",
      "Epoch 19/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9763 - accuracy: 0.9745 - loss: 0.0702\n",
      "Epoch 19: accuracy improved from 0.96484 to 0.97559, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9980 - Precision: 0.9765 - accuracy: 0.9744 - loss: 0.0705 - val_AUC: 0.8408 - val_Precision: 0.7143 - val_accuracy: 0.7109 - val_loss: 1.3718 - learning_rate: 0.0030\n",
      "Epoch 20/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9722 - accuracy: 0.9663 - loss: 0.0880\n",
      "Epoch 20: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9981 - Precision: 0.9734 - accuracy: 0.9678 - loss: 0.0862 - val_AUC: 0.8436 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 1.3972 - learning_rate: 0.0030\n",
      "Epoch 21/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9886 - accuracy: 0.9846 - loss: 0.0480\n",
      "Epoch 21: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9873 - accuracy: 0.9833 - loss: 0.0512 - val_AUC: 0.8444 - val_Precision: 0.7137 - val_accuracy: 0.7109 - val_loss: 1.5647 - learning_rate: 0.0030\n",
      "Epoch 22/200\n",
      "\u001b[1m55/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9965 - Precision: 0.9688 - accuracy: 0.9677 - loss: 0.0976\n",
      "Epoch 22: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9963 - Precision: 0.9684 - accuracy: 0.9672 - loss: 0.0991 - val_AUC: 0.8403 - val_Precision: 0.7480 - val_accuracy: 0.7383 - val_loss: 1.5427 - learning_rate: 0.0030\n",
      "Epoch 23/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9982 - Precision: 0.9783 - accuracy: 0.9777 - loss: 0.0850\n",
      "Epoch 23: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9982 - Precision: 0.9781 - accuracy: 0.9774 - loss: 0.0852 - val_AUC: 0.8410 - val_Precision: 0.7323 - val_accuracy: 0.7266 - val_loss: 1.5238 - learning_rate: 0.0030\n",
      "Epoch 24/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9824 - accuracy: 0.9782 - loss: 0.0721\n",
      "Epoch 24: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9817 - accuracy: 0.9775 - loss: 0.0733 - val_AUC: 0.8445 - val_Precision: 0.7087 - val_accuracy: 0.7070 - val_loss: 1.4916 - learning_rate: 0.0030\n",
      "Epoch 25/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9973 - Precision: 0.9620 - accuracy: 0.9588 - loss: 0.1012\n",
      "Epoch 25: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9974 - Precision: 0.9625 - accuracy: 0.9595 - loss: 0.1009 - val_AUC: 0.8454 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 1.4946 - learning_rate: 0.0030\n",
      "Epoch 26/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9983 - Precision: 0.9723 - accuracy: 0.9716 - loss: 0.0752\n",
      "Epoch 26: accuracy improved from 0.97559 to 0.97852, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9983 - Precision: 0.9730 - accuracy: 0.9722 - loss: 0.0752 - val_AUC: 0.8462 - val_Precision: 0.7273 - val_accuracy: 0.7188 - val_loss: 1.5027 - learning_rate: 0.0030\n",
      "Epoch 27/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9959 - Precision: 0.9685 - accuracy: 0.9656 - loss: 0.1117\n",
      "Epoch 27: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9960 - Precision: 0.9688 - accuracy: 0.9657 - loss: 0.1108 - val_AUC: 0.8471 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4547 - learning_rate: 0.0030\n",
      "Epoch 28/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9727 - accuracy: 0.9698 - loss: 0.0872\n",
      "Epoch 28: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9979 - Precision: 0.9727 - accuracy: 0.9698 - loss: 0.0871 - val_AUC: 0.8483 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4406 - learning_rate: 0.0030\n",
      "Epoch 29/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9986 - Precision: 0.9714 - accuracy: 0.9715 - loss: 0.0687\n",
      "Epoch 29: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9720 - accuracy: 0.9720 - loss: 0.0683 - val_AUC: 0.8458 - val_Precision: 0.7373 - val_accuracy: 0.7383 - val_loss: 1.4644 - learning_rate: 0.0030\n",
      "Epoch 30/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9979 - Precision: 0.9798 - accuracy: 0.9782 - loss: 0.0773\n",
      "Epoch 30: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9977 - Precision: 0.9790 - accuracy: 0.9771 - loss: 0.0792 - val_AUC: 0.8479 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 1.4558 - learning_rate: 0.0030\n",
      "Epoch 31/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9974 - Precision: 0.9690 - accuracy: 0.9633 - loss: 0.0975\n",
      "Epoch 31: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9975 - Precision: 0.9700 - accuracy: 0.9646 - loss: 0.0955 - val_AUC: 0.8438 - val_Precision: 0.7165 - val_accuracy: 0.7109 - val_loss: 1.4669 - learning_rate: 0.0030\n",
      "Epoch 32/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9778 - accuracy: 0.9726 - loss: 0.0856\n",
      "Epoch 32: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9978 - Precision: 0.9770 - accuracy: 0.9719 - loss: 0.0859 - val_AUC: 0.8495 - val_Precision: 0.7216 - val_accuracy: 0.7227 - val_loss: 1.4690 - learning_rate: 0.0030\n",
      "Epoch 33/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9957 - Precision: 0.9796 - accuracy: 0.9796 - loss: 0.0945\n",
      "Epoch 33: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9957 - Precision: 0.9794 - accuracy: 0.9795 - loss: 0.0946 - val_AUC: 0.8402 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.4435 - learning_rate: 0.0030\n",
      "Epoch 34/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9986 - Precision: 0.9778 - accuracy: 0.9719 - loss: 0.0739\n",
      "Epoch 34: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9789 - accuracy: 0.9726 - loss: 0.0745 - val_AUC: 0.8350 - val_Precision: 0.7302 - val_accuracy: 0.7305 - val_loss: 1.4336 - learning_rate: 0.0030\n",
      "Epoch 35/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9749 - accuracy: 0.9723 - loss: 0.0734\n",
      "Epoch 35: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9744 - accuracy: 0.9717 - loss: 0.0745 - val_AUC: 0.8388 - val_Precision: 0.7302 - val_accuracy: 0.7188 - val_loss: 1.5087 - learning_rate: 0.0030\n",
      "Epoch 36/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9895 - Precision: 0.9866 - accuracy: 0.9822 - loss: 0.1084\n",
      "Epoch 36: accuracy improved from 0.97852 to 0.98145, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9905 - Precision: 0.9868 - accuracy: 0.9820 - loss: 0.1035 - val_AUC: 0.8375 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.5180 - learning_rate: 0.0030\n",
      "Epoch 37/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9993 - Precision: 0.9831 - accuracy: 0.9805 - loss: 0.0542\n",
      "Epoch 37: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9993 - Precision: 0.9824 - accuracy: 0.9798 - loss: 0.0563 - val_AUC: 0.8400 - val_Precision: 0.7160 - val_accuracy: 0.6992 - val_loss: 1.5219 - learning_rate: 0.0030\n",
      "Epoch 38/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9981 - Precision: 0.9757 - accuracy: 0.9666 - loss: 0.0832\n",
      "Epoch 38: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9981 - Precision: 0.9765 - accuracy: 0.9680 - loss: 0.0818 - val_AUC: 0.8392 - val_Precision: 0.7176 - val_accuracy: 0.7148 - val_loss: 1.4776 - learning_rate: 0.0030\n",
      "Epoch 39/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9691 - loss: 0.0659\n",
      "Epoch 39: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9691 - loss: 0.0663 - val_AUC: 0.8444 - val_Precision: 0.7255 - val_accuracy: 0.7227 - val_loss: 1.4939 - learning_rate: 0.0030\n",
      "Epoch 40/200\n",
      "\u001b[1m55/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9970 - Precision: 0.9745 - accuracy: 0.9705 - loss: 0.0958\n",
      "Epoch 40: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9971 - Precision: 0.9752 - accuracy: 0.9715 - loss: 0.0939 - val_AUC: 0.8460 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.4743 - learning_rate: 0.0030\n",
      "Epoch 41/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9924 - Precision: 0.9454 - accuracy: 0.9453 - loss: 0.1386\n",
      "Epoch 41: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9934 - Precision: 0.9497 - accuracy: 0.9494 - loss: 0.1283 - val_AUC: 0.8427 - val_Precision: 0.7262 - val_accuracy: 0.7227 - val_loss: 1.4669 - learning_rate: 0.0030\n",
      "Epoch 42/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9963 - Precision: 0.9736 - accuracy: 0.9719 - loss: 0.0952\n",
      "Epoch 42: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9965 - Precision: 0.9740 - accuracy: 0.9721 - loss: 0.0927 - val_AUC: 0.8473 - val_Precision: 0.7154 - val_accuracy: 0.7109 - val_loss: 1.5719 - learning_rate: 0.0030\n",
      "Epoch 43/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9989 - Precision: 0.9797 - accuracy: 0.9764 - loss: 0.0618\n",
      "Epoch 43: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9989 - Precision: 0.9786 - accuracy: 0.9753 - loss: 0.0638 - val_AUC: 0.8494 - val_Precision: 0.7171 - val_accuracy: 0.7109 - val_loss: 1.4894 - learning_rate: 0.0030\n",
      "Epoch 44/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9676 - accuracy: 0.9669 - loss: 0.0837\n",
      "Epoch 44: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9980 - Precision: 0.9696 - accuracy: 0.9686 - loss: 0.0796 - val_AUC: 0.8517 - val_Precision: 0.7312 - val_accuracy: 0.7266 - val_loss: 1.4830 - learning_rate: 0.0030\n",
      "Epoch 45/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9656 - accuracy: 0.9633 - loss: 0.0938\n",
      "Epoch 45: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9981 - Precision: 0.9672 - accuracy: 0.9645 - loss: 0.0903 - val_AUC: 0.8498 - val_Precision: 0.7371 - val_accuracy: 0.7227 - val_loss: 1.4388 - learning_rate: 0.0030\n",
      "Epoch 46/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9713 - accuracy: 0.9693 - loss: 0.0848\n",
      "Epoch 46: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9976 - Precision: 0.9727 - accuracy: 0.9702 - loss: 0.0867 - val_AUC: 0.8513 - val_Precision: 0.7312 - val_accuracy: 0.7305 - val_loss: 1.4386 - learning_rate: 0.0030\n",
      "Epoch 47/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9985 - Precision: 0.9796 - accuracy: 0.9755 - loss: 0.0706\n",
      "Epoch 47: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9796 - accuracy: 0.9759 - loss: 0.0695 - val_AUC: 0.8489 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 1.4314 - learning_rate: 9.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9984 - Precision: 0.9867 - accuracy: 0.9853 - loss: 0.0539\n",
      "Epoch 48: accuracy improved from 0.98145 to 0.98438, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9867 - accuracy: 0.9851 - loss: 0.0549 - val_AUC: 0.8505 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.4342 - learning_rate: 9.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9815 - accuracy: 0.9777 - loss: 0.0848\n",
      "Epoch 49: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9981 - Precision: 0.9812 - accuracy: 0.9776 - loss: 0.0828 - val_AUC: 0.8473 - val_Precision: 0.7410 - val_accuracy: 0.7383 - val_loss: 1.4123 - learning_rate: 9.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9780 - accuracy: 0.9706 - loss: 0.0776\n",
      "Epoch 50: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9978 - Precision: 0.9776 - accuracy: 0.9705 - loss: 0.0775 - val_AUC: 0.8494 - val_Precision: 0.7402 - val_accuracy: 0.7383 - val_loss: 1.4216 - learning_rate: 9.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9969 - Precision: 0.9648 - accuracy: 0.9636 - loss: 0.0973\n",
      "Epoch 51: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9971 - Precision: 0.9661 - accuracy: 0.9648 - loss: 0.0946 - val_AUC: 0.8514 - val_Precision: 0.7431 - val_accuracy: 0.7383 - val_loss: 1.4191 - learning_rate: 9.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9987 - Precision: 0.9743 - accuracy: 0.9718 - loss: 0.0657\n",
      "Epoch 52: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9744 - accuracy: 0.9715 - loss: 0.0671 - val_AUC: 0.8513 - val_Precision: 0.7421 - val_accuracy: 0.7383 - val_loss: 1.4148 - learning_rate: 9.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9923 - accuracy: 0.9915 - loss: 0.0320\n",
      "Epoch 53: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9895 - accuracy: 0.9885 - loss: 0.0405 - val_AUC: 0.8485 - val_Precision: 0.7323 - val_accuracy: 0.7344 - val_loss: 1.3992 - learning_rate: 9.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m59/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9835 - accuracy: 0.9814 - loss: 0.0418\n",
      "Epoch 54: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9833 - accuracy: 0.9810 - loss: 0.0426 - val_AUC: 0.8460 - val_Precision: 0.7381 - val_accuracy: 0.7344 - val_loss: 1.4061 - learning_rate: 9.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9983 - Precision: 0.9757 - accuracy: 0.9686 - loss: 0.0810\n",
      "Epoch 55: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9757 - accuracy: 0.9689 - loss: 0.0807 - val_AUC: 0.8475 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.4241 - learning_rate: 9.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9736 - accuracy: 0.9703 - loss: 0.0774\n",
      "Epoch 56: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9978 - Precision: 0.9731 - accuracy: 0.9701 - loss: 0.0787 - val_AUC: 0.8468 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 1.4177 - learning_rate: 9.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m62/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9874 - accuracy: 0.9835 - loss: 0.0549\n",
      "Epoch 57: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9871 - accuracy: 0.9832 - loss: 0.0555 - val_AUC: 0.8468 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4182 - learning_rate: 9.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9958 - Precision: 0.9746 - accuracy: 0.9727 - loss: 0.1071\n",
      "Epoch 58: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9959 - Precision: 0.9746 - accuracy: 0.9727 - loss: 0.1064 - val_AUC: 0.8489 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.4065 - learning_rate: 9.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9720 - loss: 0.0641\n",
      "Epoch 59: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9745 - accuracy: 0.9720 - loss: 0.0675 - val_AUC: 0.8476 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4066 - learning_rate: 2.7000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9829 - accuracy: 0.9816 - loss: 0.0707\n",
      "Epoch 60: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9825 - accuracy: 0.9813 - loss: 0.0706 - val_AUC: 0.8478 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4065 - learning_rate: 2.7000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9844 - accuracy: 0.9799 - loss: 0.0433\n",
      "Epoch 61: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9842 - accuracy: 0.9797 - loss: 0.0441 - val_AUC: 0.8482 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4169 - learning_rate: 2.7000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9864 - accuracy: 0.9803 - loss: 0.0568\n",
      "Epoch 62: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9855 - accuracy: 0.9793 - loss: 0.0569 - val_AUC: 0.8455 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4086 - learning_rate: 2.7000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9758 - accuracy: 0.9741 - loss: 0.0670\n",
      "Epoch 63: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9760 - accuracy: 0.9742 - loss: 0.0664 - val_AUC: 0.8474 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4164 - learning_rate: 2.7000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9780 - accuracy: 0.9768 - loss: 0.0742\n",
      "Epoch 64: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9977 - Precision: 0.9779 - accuracy: 0.9767 - loss: 0.0752 - val_AUC: 0.8462 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4182 - learning_rate: 2.7000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9985 - Precision: 0.9742 - accuracy: 0.9719 - loss: 0.0780\n",
      "Epoch 65: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9744 - accuracy: 0.9720 - loss: 0.0777 - val_AUC: 0.8457 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4152 - learning_rate: 2.7000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9997 - Precision: 0.9926 - accuracy: 0.9878 - loss: 0.0369\n",
      "Epoch 66: accuracy improved from 0.98438 to 0.98535, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9996 - Precision: 0.9917 - accuracy: 0.9873 - loss: 0.0383 - val_AUC: 0.8436 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4137 - learning_rate: 2.7000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9806 - accuracy: 0.9776 - loss: 0.0564\n",
      "Epoch 67: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9824 - accuracy: 0.9794 - loss: 0.0526 - val_AUC: 0.8439 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4055 - learning_rate: 2.7000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9775 - accuracy: 0.9766 - loss: 0.0407\n",
      "Epoch 68: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9994 - Precision: 0.9787 - accuracy: 0.9775 - loss: 0.0424 - val_AUC: 0.8440 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4194 - learning_rate: 2.7000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9861 - accuracy: 0.9819 - loss: 0.0477\n",
      "Epoch 69: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9856 - accuracy: 0.9820 - loss: 0.0490 - val_AUC: 0.8438 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4394 - learning_rate: 2.7000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9927 - Precision: 0.9722 - accuracy: 0.9703 - loss: 0.1223\n",
      "Epoch 70: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9934 - Precision: 0.9729 - accuracy: 0.9707 - loss: 0.1161 - val_AUC: 0.8412 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4261 - learning_rate: 2.7000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9805 - accuracy: 0.9789 - loss: 0.0549\n",
      "Epoch 71: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9803 - accuracy: 0.9786 - loss: 0.0544 - val_AUC: 0.8428 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4400 - learning_rate: 2.7000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9901 - accuracy: 0.9891 - loss: 0.0374\n",
      "Epoch 72: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9893 - accuracy: 0.9882 - loss: 0.0393 - val_AUC: 0.8440 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4494 - learning_rate: 2.7000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m46/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9913 - Precision: 0.9559 - accuracy: 0.9543 - loss: 0.1164\n",
      "Epoch 73: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9931 - Precision: 0.9622 - accuracy: 0.9608 - loss: 0.1027 - val_AUC: 0.8444 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4408 - learning_rate: 2.7000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m62/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9887 - accuracy: 0.9884 - loss: 0.0404\n",
      "Epoch 74: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9885 - accuracy: 0.9881 - loss: 0.0409 - val_AUC: 0.8444 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4499 - learning_rate: 2.7000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9853 - accuracy: 0.9853 - loss: 0.0499\n",
      "Epoch 75: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9990 - Precision: 0.9842 - accuracy: 0.9841 - loss: 0.0524 - val_AUC: 0.8442 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4357 - learning_rate: 2.7000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9866 - accuracy: 0.9842 - loss: 0.0483\n",
      "Epoch 76: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9857 - accuracy: 0.9830 - loss: 0.0505 - val_AUC: 0.8442 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4299 - learning_rate: 2.7000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9984 - Precision: 0.9832 - accuracy: 0.9785 - loss: 0.0746\n",
      "Epoch 77: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9983 - Precision: 0.9827 - accuracy: 0.9781 - loss: 0.0765 - val_AUC: 0.8443 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4397 - learning_rate: 8.1000e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9903 - accuracy: 0.9843 - loss: 0.0507\n",
      "Epoch 78: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9901 - accuracy: 0.9842 - loss: 0.0510 - val_AUC: 0.8443 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4398 - learning_rate: 8.1000e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9851 - accuracy: 0.9787 - loss: 0.0441\n",
      "Epoch 79: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9849 - accuracy: 0.9787 - loss: 0.0446 - val_AUC: 0.8445 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4293 - learning_rate: 8.1000e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9988 - Precision: 0.9728 - accuracy: 0.9730 - loss: 0.0650\n",
      "Epoch 80: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9739 - accuracy: 0.9740 - loss: 0.0634 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4364 - learning_rate: 8.1000e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9879 - accuracy: 0.9875 - loss: 0.0393\n",
      "Epoch 81: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9877 - accuracy: 0.9873 - loss: 0.0398 - val_AUC: 0.8452 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4351 - learning_rate: 8.1000e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9977 - Precision: 0.9856 - accuracy: 0.9844 - loss: 0.0594 \n",
      "Epoch 82: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9972 - Precision: 0.9838 - accuracy: 0.9825 - loss: 0.0657 - val_AUC: 0.8450 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4406 - learning_rate: 8.1000e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m47/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9793 - accuracy: 0.9791 - loss: 0.0558\n",
      "Epoch 83: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9784 - accuracy: 0.9779 - loss: 0.0574 - val_AUC: 0.8451 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4400 - learning_rate: 8.1000e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9896 - accuracy: 0.9897 - loss: 0.0403\n",
      "Epoch 84: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9887 - accuracy: 0.9888 - loss: 0.0430 - val_AUC: 0.8449 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4378 - learning_rate: 8.1000e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9997 - Precision: 0.9909 - accuracy: 0.9883 - loss: 0.0347\n",
      "Epoch 85: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9995 - Precision: 0.9884 - accuracy: 0.9859 - loss: 0.0405 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4307 - learning_rate: 8.1000e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9971 - Precision: 0.9752 - accuracy: 0.9747 - loss: 0.0808\n",
      "Epoch 86: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9972 - Precision: 0.9756 - accuracy: 0.9750 - loss: 0.0786 - val_AUC: 0.8452 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4417 - learning_rate: 8.1000e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9797 - accuracy: 0.9781 - loss: 0.0484\n",
      "Epoch 87: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9794 - accuracy: 0.9776 - loss: 0.0510 - val_AUC: 0.8451 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4415 - learning_rate: 2.4300e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9835 - accuracy: 0.9785 - loss: 0.0533\n",
      "Epoch 88: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9835 - accuracy: 0.9785 - loss: 0.0535 - val_AUC: 0.8448 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4361 - learning_rate: 2.4300e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9856 - accuracy: 0.9831 - loss: 0.0461\n",
      "Epoch 89: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9857 - accuracy: 0.9831 - loss: 0.0462 - val_AUC: 0.8448 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4369 - learning_rate: 2.4300e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9955 - Precision: 0.9772 - accuracy: 0.9716 - loss: 0.1073\n",
      "Epoch 90: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9958 - Precision: 0.9775 - accuracy: 0.9726 - loss: 0.1008 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4474 - learning_rate: 2.4300e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9813 - accuracy: 0.9813 - loss: 0.0509\n",
      "Epoch 91: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9813 - accuracy: 0.9813 - loss: 0.0508 - val_AUC: 0.8445 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4453 - learning_rate: 2.4300e-05\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_trained_processed,Y_train_one_hot,epochs=200,validation_split=0.2,batch_size=16,callbacks=[early_stopping,lr_scheduler,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7357e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X_encoded=pipeline.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0e4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(854, 421)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\".//ai/ann_best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(Test_X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcf1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9929667e-01, 7.0321001e-04, 1.6503252e-07],\n",
       "       [2.8722810e-02, 9.6408790e-01, 7.1892454e-03],\n",
       "       [9.9999905e-01, 9.2857380e-07, 6.4296866e-09],\n",
       "       ...,\n",
       "       [9.4172263e-01, 3.0688921e-02, 2.7588399e-02],\n",
       "       [8.6403525e-01, 8.4469788e-02, 5.1494971e-02],\n",
       "       [9.9113524e-01, 8.8646896e-03, 2.3500960e-13]], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the mapping based on index\n",
    "index_to_label = {0: 'High', 1: 'Medium', 2: 'Low'}\n",
    "\n",
    "# Get the index of the max probability for each row\n",
    "predicted_indices = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# Map the indices to labels\n",
    "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cafc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Low', 'High', 'High', 'Medium', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Low', 'High', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'Low', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "\n",
    "print(mapped_preds)\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"new_engineerr_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb03c59",
   "metadata": {},
   "source": [
    "# Let's try PCA with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "708c8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.columns.tolist()\n",
    "job_features=[f for f in features if f.startswith(\"job_desc\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "41408656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.193511</td>\n",
       "      <td>2.275482</td>\n",
       "      <td>-0.440363</td>\n",
       "      <td>-0.327473</td>\n",
       "      <td>0.058464</td>\n",
       "      <td>-0.154043</td>\n",
       "      <td>-0.393158</td>\n",
       "      <td>-0.367905</td>\n",
       "      <td>-0.703665</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362079</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100152</td>\n",
       "      <td>2.291134</td>\n",
       "      <td>-0.356041</td>\n",
       "      <td>-0.494735</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>-0.356995</td>\n",
       "      <td>-0.633020</td>\n",
       "      <td>-0.444805</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>0.187210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300989</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406864</td>\n",
       "      <td>1.986625</td>\n",
       "      <td>-0.726046</td>\n",
       "      <td>-0.316294</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>-0.451118</td>\n",
       "      <td>-0.659871</td>\n",
       "      <td>-0.451544</td>\n",
       "      <td>-0.505597</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406159</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>0.247692</td>\n",
       "      <td>2.241664</td>\n",
       "      <td>-0.773778</td>\n",
       "      <td>-0.349046</td>\n",
       "      <td>-0.112922</td>\n",
       "      <td>-0.443660</td>\n",
       "      <td>-0.623765</td>\n",
       "      <td>-0.574480</td>\n",
       "      <td>-0.646541</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089303</td>\n",
       "      <td>-0.471856</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.245694</td>\n",
       "      <td>0.251105</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>-0.184764</td>\n",
       "      <td>-0.482634</td>\n",
       "      <td>-0.819574</td>\n",
       "      <td>-0.241306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>-0.000119</td>\n",
       "      <td>2.386898</td>\n",
       "      <td>-0.568260</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>-0.176302</td>\n",
       "      <td>0.119118</td>\n",
       "      <td>-0.414387</td>\n",
       "      <td>-0.924835</td>\n",
       "      <td>-0.025357</td>\n",
       "      <td>0.178054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079315</td>\n",
       "      <td>-0.796785</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>-0.462344</td>\n",
       "      <td>-0.053803</td>\n",
       "      <td>-0.297083</td>\n",
       "      <td>-0.277624</td>\n",
       "      <td>-0.924568</td>\n",
       "      <td>-0.897608</td>\n",
       "      <td>-0.236864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.020596</td>\n",
       "      <td>1.710889</td>\n",
       "      <td>-0.127225</td>\n",
       "      <td>0.094155</td>\n",
       "      <td>-0.230931</td>\n",
       "      <td>-0.094211</td>\n",
       "      <td>-0.464730</td>\n",
       "      <td>-0.415120</td>\n",
       "      <td>-0.250254</td>\n",
       "      <td>0.244051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030367</td>\n",
       "      <td>-0.329539</td>\n",
       "      <td>-0.429462</td>\n",
       "      <td>-0.141221</td>\n",
       "      <td>0.119336</td>\n",
       "      <td>-0.483549</td>\n",
       "      <td>-0.146601</td>\n",
       "      <td>-0.715821</td>\n",
       "      <td>-0.739832</td>\n",
       "      <td>-0.128655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0.385978</td>\n",
       "      <td>2.305014</td>\n",
       "      <td>-0.250134</td>\n",
       "      <td>0.010997</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>-0.393612</td>\n",
       "      <td>-0.502189</td>\n",
       "      <td>-0.598519</td>\n",
       "      <td>-0.664700</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213866</td>\n",
       "      <td>-0.242665</td>\n",
       "      <td>-0.311564</td>\n",
       "      <td>-0.032751</td>\n",
       "      <td>-0.301447</td>\n",
       "      <td>-0.480434</td>\n",
       "      <td>-0.112401</td>\n",
       "      <td>-0.828844</td>\n",
       "      <td>-1.066424</td>\n",
       "      <td>-0.228583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0.276502</td>\n",
       "      <td>2.362514</td>\n",
       "      <td>-0.311881</td>\n",
       "      <td>-0.314943</td>\n",
       "      <td>-0.120564</td>\n",
       "      <td>-0.346545</td>\n",
       "      <td>-0.538098</td>\n",
       "      <td>-0.561801</td>\n",
       "      <td>-0.507609</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181357</td>\n",
       "      <td>-0.721596</td>\n",
       "      <td>-0.324486</td>\n",
       "      <td>-0.084486</td>\n",
       "      <td>-0.100463</td>\n",
       "      <td>-0.329224</td>\n",
       "      <td>-0.369456</td>\n",
       "      <td>-0.872554</td>\n",
       "      <td>-0.829080</td>\n",
       "      <td>-0.081490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_desc_001  job_desc_002  job_desc_003  job_desc_004  job_desc_005  \\\n",
       "0         0.193511      2.275482     -0.440363     -0.327473      0.058464   \n",
       "1         0.100152      2.291134     -0.356041     -0.494735      0.038632   \n",
       "2         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4         0.406864      1.986625     -0.726046     -0.316294      0.062115   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1275      0.247692      2.241664     -0.773778     -0.349046     -0.112922   \n",
       "1276     -0.000119      2.386898     -0.568260     -0.072558     -0.176302   \n",
       "1277      0.020596      1.710889     -0.127225      0.094155     -0.230931   \n",
       "1278      0.385978      2.305014     -0.250134      0.010997      0.008592   \n",
       "1279      0.276502      2.362514     -0.311881     -0.314943     -0.120564   \n",
       "\n",
       "      job_desc_006  job_desc_007  job_desc_008  job_desc_009  job_desc_010  \\\n",
       "0        -0.154043     -0.393158     -0.367905     -0.703665      0.562969   \n",
       "1        -0.356995     -0.633020     -0.444805     -0.252597      0.187210   \n",
       "2         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4        -0.451118     -0.659871     -0.451544     -0.505597      0.119204   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1275     -0.443660     -0.623765     -0.574480     -0.646541      0.056761   \n",
       "1276      0.119118     -0.414387     -0.924835     -0.025357      0.178054   \n",
       "1277     -0.094211     -0.464730     -0.415120     -0.250254      0.244051   \n",
       "1278     -0.393612     -0.502189     -0.598519     -0.664700     -0.041492   \n",
       "1279     -0.346545     -0.538098     -0.561801     -0.507609      0.109028   \n",
       "\n",
       "      ...  job_desc_291  job_desc_292  job_desc_293  job_desc_294  \\\n",
       "0     ...     -0.362079     -0.499308     -0.367894     -0.214881   \n",
       "1     ...     -0.300989     -0.415411     -0.341824     -0.319064   \n",
       "2     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "3     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "4     ...     -0.406159     -0.654657     -0.074398     -0.464479   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1275  ...     -0.089303     -0.471856      0.002497      0.245694   \n",
       "1276  ...     -0.079315     -0.796785      0.154025     -0.462344   \n",
       "1277  ...     -0.030367     -0.329539     -0.429462     -0.141221   \n",
       "1278  ...     -0.213866     -0.242665     -0.311564     -0.032751   \n",
       "1279  ...     -0.181357     -0.721596     -0.324486     -0.084486   \n",
       "\n",
       "      job_desc_295  job_desc_296  job_desc_297  job_desc_298  job_desc_299  \\\n",
       "0         0.014870     -0.271177     -0.113347     -0.587955     -0.919095   \n",
       "1         0.042322     -0.124755      0.023489     -0.893224     -0.823024   \n",
       "2         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4         0.081037     -0.136992     -0.276270     -0.696853     -0.601466   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1275      0.251105      0.119141     -0.184764     -0.482634     -0.819574   \n",
       "1276     -0.053803     -0.297083     -0.277624     -0.924568     -0.897608   \n",
       "1277      0.119336     -0.483549     -0.146601     -0.715821     -0.739832   \n",
       "1278     -0.301447     -0.480434     -0.112401     -0.828844     -1.066424   \n",
       "1279     -0.100463     -0.329224     -0.369456     -0.872554     -0.829080   \n",
       "\n",
       "      job_desc_300  \n",
       "0        -0.207340  \n",
       "1         0.112364  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.089939  \n",
       "...            ...  \n",
       "1275     -0.241306  \n",
       "1276     -0.236864  \n",
       "1277     -0.128655  \n",
       "1278     -0.228583  \n",
       "1279     -0.081490  \n",
       "\n",
       "[1280 rows x 300 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[job_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "da266200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.380112</td>\n",
       "      <td>0.469987</td>\n",
       "      <td>-0.075655</td>\n",
       "      <td>-1.038070</td>\n",
       "      <td>0.417364</td>\n",
       "      <td>-0.591893</td>\n",
       "      <td>-0.349075</td>\n",
       "      <td>0.128731</td>\n",
       "      <td>0.344220</td>\n",
       "      <td>0.286221</td>\n",
       "      <td>-0.299898</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>-0.377720</td>\n",
       "      <td>-0.368970</td>\n",
       "      <td>-0.465331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.210201</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>-0.126489</td>\n",
       "      <td>-1.323580</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>-0.470055</td>\n",
       "      <td>-0.958401</td>\n",
       "      <td>-0.599469</td>\n",
       "      <td>0.581653</td>\n",
       "      <td>-0.648634</td>\n",
       "      <td>0.075395</td>\n",
       "      <td>-0.215588</td>\n",
       "      <td>0.553147</td>\n",
       "      <td>-0.104168</td>\n",
       "      <td>-0.105895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>-0.032149</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>-0.032149</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.261816</td>\n",
       "      <td>-1.331539</td>\n",
       "      <td>-0.602937</td>\n",
       "      <td>-0.419041</td>\n",
       "      <td>0.312946</td>\n",
       "      <td>-0.456935</td>\n",
       "      <td>-0.397791</td>\n",
       "      <td>0.386783</td>\n",
       "      <td>0.059608</td>\n",
       "      <td>0.020664</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.515361</td>\n",
       "      <td>0.564903</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.057638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PCA_1     PCA_2     PCA_3     PCA_4     PCA_5     PCA_6     PCA_7  \\\n",
       "0  1.380112  0.469987 -0.075655 -1.038070  0.417364 -0.591893 -0.349075   \n",
       "1  1.210201  0.024486 -0.126489 -1.323580  0.505531 -0.470055 -0.958401   \n",
       "2 -7.344770 -0.170561  0.067727  0.056237 -0.017499 -0.233536  0.121877   \n",
       "3 -7.344770 -0.170561  0.067727  0.056237 -0.017499 -0.233536  0.121877   \n",
       "4  1.261816 -1.331539 -0.602937 -0.419041  0.312946 -0.456935 -0.397791   \n",
       "\n",
       "      PCA_8     PCA_9    PCA_10    PCA_11    PCA_12    PCA_13    PCA_14  \\\n",
       "0  0.128731  0.344220  0.286221 -0.299898  0.089952 -0.377720 -0.368970   \n",
       "1 -0.599469  0.581653 -0.648634  0.075395 -0.215588  0.553147 -0.104168   \n",
       "2 -0.030255  0.088041  0.049815 -0.032149  0.032170 -0.002428 -0.031415   \n",
       "3 -0.030255  0.088041  0.049815 -0.032149  0.032170 -0.002428 -0.031415   \n",
       "4  0.386783  0.059608  0.020664  0.414326  0.515361  0.564903  0.044567   \n",
       "\n",
       "     PCA_15  \n",
       "0 -0.465331  \n",
       "1 -0.105895  \n",
       "2  0.003118  \n",
       "3  0.003118  \n",
       "4  0.057638  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(15)\n",
    "new_job_features=pca.fit_transform(X_train[job_features])\n",
    "\n",
    "pca_df=pd.DataFrame(new_job_features,columns=[\"PCA_1\",\"PCA_2\",\"PCA_3\",\"PCA_4\",\"PCA_5\",\"PCA_6\",\"PCA_7\",\"PCA_8\",\"PCA_9\",\"PCA_10\",\"PCA_11\",\"PCA_12\",\"PCA_13\",\"PCA_14\",\"PCA_15\"])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "588dbbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_8936\\1933739389.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X_train_pca[\"job_posted_date\"]=pd.to_datetime(X_train_pca[\"job_posted_date\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128731</td>\n",
       "      <td>0.344220</td>\n",
       "      <td>0.286221</td>\n",
       "      <td>-0.299898</td>\n",
       "      <td>0.089952</td>\n",
       "      <td>-0.377720</td>\n",
       "      <td>-0.368970</td>\n",
       "      <td>-0.465331</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599469</td>\n",
       "      <td>0.581653</td>\n",
       "      <td>-0.648634</td>\n",
       "      <td>0.075395</td>\n",
       "      <td>-0.215588</td>\n",
       "      <td>0.553147</td>\n",
       "      <td>-0.104168</td>\n",
       "      <td>-0.105895</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>-0.032149</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>-0.032149</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386783</td>\n",
       "      <td>0.059608</td>\n",
       "      <td>0.020664</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.515361</td>\n",
       "      <td>0.564903</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.057638</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_state feature_1  feature_2  feature_3  feature_4  \\\n",
       "0       Others        NY         A     0.6429          0          0   \n",
       "1  Job_Title_1        CA         A     0.4678          0          0   \n",
       "2       Others        CA         A     0.4610          0          0   \n",
       "3       Others        CA         A     0.5064          0          0   \n",
       "4       Others        CA         A     0.4640          0          0   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8  ...     PCA_8     PCA_9  \\\n",
       "0          1          1          1          0  ...  0.128731  0.344220   \n",
       "1          0          1          1          1  ... -0.599469  0.581653   \n",
       "2          0          1          1          1  ... -0.030255  0.088041   \n",
       "3          0          1          1          1  ... -0.030255  0.088041   \n",
       "4          0          1          1          0  ...  0.386783  0.059608   \n",
       "\n",
       "     PCA_10    PCA_11    PCA_12    PCA_13    PCA_14    PCA_15    year  month  \n",
       "0  0.286221 -0.299898  0.089952 -0.377720 -0.368970 -0.465331  2024.0    7.0  \n",
       "1 -0.648634  0.075395 -0.215588  0.553147 -0.104168 -0.105895  2024.0    7.0  \n",
       "2  0.049815 -0.032149  0.032170 -0.002428 -0.031415  0.003118  2024.0    7.0  \n",
       "3  0.049815 -0.032149  0.032170 -0.002428 -0.031415  0.003118  2024.0    7.0  \n",
       "4  0.020664  0.414326  0.515361  0.564903  0.044567  0.057638  2024.0    7.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca=X_train.drop(columns=job_features)\n",
    "X_train_pca=pd.concat([X_train_pca,pca_df],axis=1)\n",
    "X_train_pca[\"job_posted_date\"]=pd.to_datetime(X_train_pca[\"job_posted_date\"])\n",
    "X_train_pca[\"year\"]=X_train_pca[\"job_posted_date\"].dt.year\n",
    "X_train_pca[\"month\"]=X_train_pca[\"job_posted_date\"].dt.month\n",
    "X_train_pca.drop(columns=[\"job_posted_date\"],inplace=True)\n",
    "X_train_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "efd60fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 numaric columns and 4 catagorical columns\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "numaric_columns=X_train_pca.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "catagory_columns=X_train_pca.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numaric_columns)} numaric columns and {len(catagory_columns)} catagorical columns\")\n",
    "num_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"mean\")),\n",
    "        (\"standard\",StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"odrinal\",OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transform=ColumnTransformer(\n",
    "    [\n",
    "        (\"numaric\",num_pipeline,numaric_columns),\n",
    "        (\"cat\",cat_pipeline,catagory_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3ea2ddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 40959 stored elements and shape (1280, 138)>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline=Pipeline(\n",
    "    [\n",
    "        (\"transformer\",column_transform)\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train_pca)\n",
    "\n",
    "X_trained_processed = pipeline.transform(X_train_pca)\n",
    "X_trained_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "962c9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Or however many CPU cores you have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cbf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote=SMOTE(random_state=42)\n",
    "X_trained_processed,Y_train=smote.fit_resample(X_trained_processed,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "55cbbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train_zero_indexed = Y_train\n",
    "\n",
    "# Now apply to_categorical with the correct number of classes\n",
    "Y_train_one_hot = to_categorical(Y_train_zero_indexed, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cc21522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │        \u001b[38;5;34m18,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │           \u001b[38;5;34m544\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m13,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,631</span> (135.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,631\u001b[0m (135.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,139</span> (133.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,139\u001b[0m (133.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">492</span> (1.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m492\u001b[0m (1.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "\n",
    "model=Sequential(\n",
    "    [\n",
    "        Dense(136,activation=\"relu\",input_shape=[X_trained_processed.shape[1]]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(100,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(10,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(3,activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "322e6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.3, patience=10, min_lr=1e-6\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=35, restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='pca/ann_best_model.keras',      \n",
    "    monitor='val_accuracy',                 \n",
    "    mode='max',                        \n",
    "    save_best_only=True,                 \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "41acb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(.01),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['accuracy', 'AUC', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "766e029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history=model.fit(X_trained_processed,Y_train_one_hot,epochs=200,validation_split=0.2,batch_size=20,callbacks=[early_stopping,lr_scheduler,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67a495",
   "metadata": {},
   "source": [
    "# Trying Advanced Modified model strucure from ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fecb64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">108,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">63,180</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">243</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │       \u001b[38;5;34m108,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │         \u001b[38;5;34m1,400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │        \u001b[38;5;34m63,180\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │           \u001b[38;5;34m720\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │        \u001b[38;5;34m14,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │           \u001b[38;5;34m320\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m243\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">188,843</span> (737.67 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m188,843\u001b[0m (737.67 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">187,623</span> (732.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m187,623\u001b[0m (732.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,220</span> (4.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,220\u001b[0m (4.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.6708 - Precision: 0.5178 - accuracy: 0.5073 - loss: 1.1835\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64453, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - AUC: 0.6820 - Precision: 0.5290 - accuracy: 0.5172 - loss: 1.1596 - val_AUC: 0.8199 - val_Precision: 0.6556 - val_accuracy: 0.6445 - val_loss: 0.9213 - learning_rate: 0.0100\n",
      "Epoch 2/250\n",
      "\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8657 - Precision: 0.7360 - accuracy: 0.7014 - loss: 0.7000\n",
      "Epoch 2: val_accuracy improved from 0.64453 to 0.68750, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.8663 - Precision: 0.7390 - accuracy: 0.7056 - loss: 0.7011 - val_AUC: 0.8625 - val_Precision: 0.7113 - val_accuracy: 0.6875 - val_loss: 0.7875 - learning_rate: 0.0100\n",
      "Epoch 3/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9039 - Precision: 0.8084 - accuracy: 0.7618 - loss: 0.5999\n",
      "Epoch 3: val_accuracy improved from 0.68750 to 0.69141, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - AUC: 0.9025 - Precision: 0.8041 - accuracy: 0.7591 - loss: 0.6034 - val_AUC: 0.8495 - val_Precision: 0.7210 - val_accuracy: 0.6914 - val_loss: 0.8246 - learning_rate: 0.0100\n",
      "Epoch 4/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9325 - Precision: 0.8239 - accuracy: 0.7857 - loss: 0.5056\n",
      "Epoch 4: val_accuracy improved from 0.69141 to 0.71484, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9320 - Precision: 0.8246 - accuracy: 0.7867 - loss: 0.5071 - val_AUC: 0.8641 - val_Precision: 0.7469 - val_accuracy: 0.7148 - val_loss: 0.8670 - learning_rate: 0.0100\n",
      "Epoch 5/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9408 - Precision: 0.8504 - accuracy: 0.8123 - loss: 0.4742\n",
      "Epoch 5: val_accuracy did not improve from 0.71484\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9392 - Precision: 0.8457 - accuracy: 0.8089 - loss: 0.4793 - val_AUC: 0.8351 - val_Precision: 0.6749 - val_accuracy: 0.6641 - val_loss: 1.0689 - learning_rate: 0.0100\n",
      "Epoch 6/250\n",
      "\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9318 - Precision: 0.8252 - accuracy: 0.7991 - loss: 0.5027\n",
      "Epoch 6: val_accuracy improved from 0.71484 to 0.71875, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - AUC: 0.9324 - Precision: 0.8264 - accuracy: 0.8004 - loss: 0.5002 - val_AUC: 0.8617 - val_Precision: 0.7189 - val_accuracy: 0.7188 - val_loss: 0.9479 - learning_rate: 0.0100\n",
      "Epoch 7/250\n",
      "\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9488 - Precision: 0.8415 - accuracy: 0.8248 - loss: 0.4375 \n",
      "Epoch 7: val_accuracy did not improve from 0.71875\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9478 - Precision: 0.8436 - accuracy: 0.8276 - loss: 0.4407 - val_AUC: 0.8559 - val_Precision: 0.7208 - val_accuracy: 0.7031 - val_loss: 0.9205 - learning_rate: 0.0100\n",
      "Epoch 8/250\n",
      "\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9656 - Precision: 0.8815 - accuracy: 0.8485 - loss: 0.3609\n",
      "Epoch 8: val_accuracy improved from 0.71875 to 0.72656, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9644 - Precision: 0.8788 - accuracy: 0.8470 - loss: 0.3659 - val_AUC: 0.8644 - val_Precision: 0.7368 - val_accuracy: 0.7266 - val_loss: 1.0122 - learning_rate: 0.0100\n",
      "Epoch 9/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9708 - Precision: 0.9128 - accuracy: 0.8882 - loss: 0.3276\n",
      "Epoch 9: val_accuracy did not improve from 0.72656\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9696 - Precision: 0.9086 - accuracy: 0.8849 - loss: 0.3338 - val_AUC: 0.8543 - val_Precision: 0.7167 - val_accuracy: 0.6992 - val_loss: 1.1993 - learning_rate: 0.0100\n",
      "Epoch 10/250\n",
      "\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9654 - Precision: 0.8796 - accuracy: 0.8704 - loss: 0.3566\n",
      "Epoch 10: val_accuracy did not improve from 0.72656\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9652 - Precision: 0.8781 - accuracy: 0.8687 - loss: 0.3583 - val_AUC: 0.8471 - val_Precision: 0.6976 - val_accuracy: 0.6875 - val_loss: 1.0990 - learning_rate: 0.0100\n",
      "Epoch 11/250\n",
      "\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9708 - Precision: 0.8876 - accuracy: 0.8794 - loss: 0.3223\n",
      "Epoch 11: val_accuracy did not improve from 0.72656\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9700 - Precision: 0.8855 - accuracy: 0.8773 - loss: 0.3271 - val_AUC: 0.8623 - val_Precision: 0.7336 - val_accuracy: 0.7031 - val_loss: 1.0825 - learning_rate: 0.0100\n",
      "Epoch 12/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9673 - Precision: 0.8695 - accuracy: 0.8509 - loss: 0.3466\n",
      "Epoch 12: val_accuracy did not improve from 0.72656\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9675 - Precision: 0.8716 - accuracy: 0.8532 - loss: 0.3455 - val_AUC: 0.8532 - val_Precision: 0.7347 - val_accuracy: 0.7109 - val_loss: 1.0852 - learning_rate: 0.0100\n",
      "Epoch 13/250\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9784 - Precision: 0.9060 - accuracy: 0.8920 - loss: 0.2830\n",
      "Epoch 13: val_accuracy improved from 0.72656 to 0.73828, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - AUC: 0.9782 - Precision: 0.9059 - accuracy: 0.8919 - loss: 0.2840 - val_AUC: 0.8563 - val_Precision: 0.7470 - val_accuracy: 0.7383 - val_loss: 1.0499 - learning_rate: 0.0100\n",
      "Epoch 14/250\n",
      "\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9828 - Precision: 0.9323 - accuracy: 0.9238 - loss: 0.2427\n",
      "Epoch 14: val_accuracy did not improve from 0.73828\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9798 - Precision: 0.9196 - accuracy: 0.9116 - loss: 0.2636 - val_AUC: 0.8556 - val_Precision: 0.7458 - val_accuracy: 0.7227 - val_loss: 1.0389 - learning_rate: 0.0100\n",
      "Epoch 15/250\n",
      "\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9763 - Precision: 0.9180 - accuracy: 0.9097 - loss: 0.2955\n",
      "Epoch 15: val_accuracy did not improve from 0.73828\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - AUC: 0.9790 - Precision: 0.9212 - accuracy: 0.9126 - loss: 0.2771 - val_AUC: 0.8472 - val_Precision: 0.6908 - val_accuracy: 0.6758 - val_loss: 1.1927 - learning_rate: 0.0100\n",
      "Epoch 16/250\n",
      "\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9807 - Precision: 0.9098 - accuracy: 0.9054 - loss: 0.2596\n",
      "Epoch 16: val_accuracy did not improve from 0.73828\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9793 - Precision: 0.9089 - accuracy: 0.9035 - loss: 0.2689 - val_AUC: 0.8564 - val_Precision: 0.7240 - val_accuracy: 0.7148 - val_loss: 1.2889 - learning_rate: 0.0100\n",
      "Epoch 17/250\n",
      "\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9864 - Precision: 0.9364 - accuracy: 0.9300 - loss: 0.2179\n",
      "Epoch 17: val_accuracy improved from 0.73828 to 0.75781, saving model to gpt2/best_model.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - AUC: 0.9831 - Precision: 0.9256 - accuracy: 0.9190 - loss: 0.2416 - val_AUC: 0.8679 - val_Precision: 0.7631 - val_accuracy: 0.7578 - val_loss: 1.0649 - learning_rate: 0.0100\n",
      "Epoch 18/250\n",
      "\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9820 - Precision: 0.9110 - accuracy: 0.9033 - loss: 0.2480\n",
      "Epoch 18: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9796 - Precision: 0.9059 - accuracy: 0.8976 - loss: 0.2657 - val_AUC: 0.8542 - val_Precision: 0.7052 - val_accuracy: 0.6953 - val_loss: 1.0623 - learning_rate: 0.0100\n",
      "Epoch 19/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9937 - Precision: 0.9543 - accuracy: 0.9439 - loss: 0.1681\n",
      "Epoch 19: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9932 - Precision: 0.9527 - accuracy: 0.9427 - loss: 0.1708 - val_AUC: 0.8512 - val_Precision: 0.7000 - val_accuracy: 0.6875 - val_loss: 1.2113 - learning_rate: 0.0100\n",
      "Epoch 20/250\n",
      "\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9913 - Precision: 0.9495 - accuracy: 0.9437 - loss: 0.1746\n",
      "Epoch 20: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9887 - Precision: 0.9405 - accuracy: 0.9346 - loss: 0.1955 - val_AUC: 0.8468 - val_Precision: 0.6980 - val_accuracy: 0.6992 - val_loss: 1.2160 - learning_rate: 0.0100\n",
      "Epoch 21/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9855 - Precision: 0.9229 - accuracy: 0.9146 - loss: 0.2255\n",
      "Epoch 21: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9853 - Precision: 0.9234 - accuracy: 0.9153 - loss: 0.2255 - val_AUC: 0.8577 - val_Precision: 0.7183 - val_accuracy: 0.7148 - val_loss: 1.1086 - learning_rate: 0.0100\n",
      "Epoch 22/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9892 - Precision: 0.9367 - accuracy: 0.9273 - loss: 0.1860\n",
      "Epoch 22: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9887 - Precision: 0.9338 - accuracy: 0.9243 - loss: 0.1916 - val_AUC: 0.8467 - val_Precision: 0.7200 - val_accuracy: 0.7188 - val_loss: 1.1909 - learning_rate: 0.0100\n",
      "Epoch 23/250\n",
      "\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9908 - Precision: 0.9327 - accuracy: 0.9197 - loss: 0.1853\n",
      "Epoch 23: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9899 - Precision: 0.9314 - accuracy: 0.9193 - loss: 0.1912 - val_AUC: 0.8428 - val_Precision: 0.7205 - val_accuracy: 0.7148 - val_loss: 1.3336 - learning_rate: 0.0100\n",
      "Epoch 24/250\n",
      "\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9859 - Precision: 0.9314 - accuracy: 0.9251 - loss: 0.2142\n",
      "Epoch 24: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9863 - Precision: 0.9315 - accuracy: 0.9250 - loss: 0.2136 - val_AUC: 0.8574 - val_Precision: 0.7328 - val_accuracy: 0.7227 - val_loss: 1.2976 - learning_rate: 0.0100\n",
      "Epoch 25/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9928 - Precision: 0.9416 - accuracy: 0.9380 - loss: 0.1636\n",
      "Epoch 25: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9924 - Precision: 0.9414 - accuracy: 0.9376 - loss: 0.1665 - val_AUC: 0.8531 - val_Precision: 0.7362 - val_accuracy: 0.7305 - val_loss: 1.3060 - learning_rate: 0.0100\n",
      "Epoch 26/250\n",
      "\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9954 - Precision: 0.9625 - accuracy: 0.9607 - loss: 0.1294\n",
      "Epoch 26: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9937 - Precision: 0.9513 - accuracy: 0.9488 - loss: 0.1492 - val_AUC: 0.8730 - val_Precision: 0.7621 - val_accuracy: 0.7461 - val_loss: 1.2646 - learning_rate: 0.0100\n",
      "Epoch 27/250\n",
      "\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9900 - Precision: 0.9247 - accuracy: 0.9227 - loss: 0.1887\n",
      "Epoch 27: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9899 - Precision: 0.9249 - accuracy: 0.9227 - loss: 0.1893 - val_AUC: 0.8613 - val_Precision: 0.7269 - val_accuracy: 0.7188 - val_loss: 1.2374 - learning_rate: 0.0100\n",
      "Epoch 28/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9922 - Precision: 0.9585 - accuracy: 0.9524 - loss: 0.1579\n",
      "Epoch 28: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9920 - Precision: 0.9571 - accuracy: 0.9509 - loss: 0.1597 - val_AUC: 0.8596 - val_Precision: 0.7352 - val_accuracy: 0.7266 - val_loss: 1.2505 - learning_rate: 0.0100\n",
      "Epoch 29/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9925 - Precision: 0.9447 - accuracy: 0.9369 - loss: 0.1593\n",
      "Epoch 29: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9923 - Precision: 0.9434 - accuracy: 0.9360 - loss: 0.1616 - val_AUC: 0.8484 - val_Precision: 0.7131 - val_accuracy: 0.7070 - val_loss: 1.4251 - learning_rate: 0.0100\n",
      "Epoch 30/250\n",
      "\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9938 - Precision: 0.9508 - accuracy: 0.9418 - loss: 0.1496\n",
      "Epoch 30: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9933 - Precision: 0.9488 - accuracy: 0.9406 - loss: 0.1543 - val_AUC: 0.8601 - val_Precision: 0.7283 - val_accuracy: 0.7266 - val_loss: 1.3358 - learning_rate: 0.0100\n",
      "Epoch 31/250\n",
      "\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9943 - Precision: 0.9506 - accuracy: 0.9465 - loss: 0.1329\n",
      "Epoch 31: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9939 - Precision: 0.9505 - accuracy: 0.9461 - loss: 0.1381 - val_AUC: 0.8529 - val_Precision: 0.7126 - val_accuracy: 0.7148 - val_loss: 1.4485 - learning_rate: 0.0100\n",
      "Epoch 32/250\n",
      "\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9965 - Precision: 0.9589 - accuracy: 0.9566 - loss: 0.1138\n",
      "Epoch 32: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9964 - Precision: 0.9585 - accuracy: 0.9564 - loss: 0.1149 - val_AUC: 0.8450 - val_Precision: 0.7075 - val_accuracy: 0.7070 - val_loss: 1.4890 - learning_rate: 0.0100\n",
      "Epoch 33/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9930 - Precision: 0.9476 - accuracy: 0.9448 - loss: 0.1472\n",
      "Epoch 33: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9934 - Precision: 0.9496 - accuracy: 0.9470 - loss: 0.1420 - val_AUC: 0.8633 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 1.4886 - learning_rate: 0.0100\n",
      "Epoch 34/250\n",
      "\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9971 - Precision: 0.9622 - accuracy: 0.9619 - loss: 0.0990\n",
      "Epoch 34: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9969 - Precision: 0.9603 - accuracy: 0.9598 - loss: 0.1027 - val_AUC: 0.8453 - val_Precision: 0.6980 - val_accuracy: 0.6992 - val_loss: 1.4998 - learning_rate: 0.0100\n",
      "Epoch 35/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9925 - Precision: 0.9605 - accuracy: 0.9607 - loss: 0.1477\n",
      "Epoch 35: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9923 - Precision: 0.9578 - accuracy: 0.9579 - loss: 0.1521 - val_AUC: 0.8689 - val_Precision: 0.7490 - val_accuracy: 0.7500 - val_loss: 1.3069 - learning_rate: 0.0100\n",
      "Epoch 36/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9958 - Precision: 0.9675 - accuracy: 0.9645 - loss: 0.1103\n",
      "Epoch 36: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - AUC: 0.9957 - Precision: 0.9664 - accuracy: 0.9637 - loss: 0.1128 - val_AUC: 0.8632 - val_Precision: 0.7381 - val_accuracy: 0.7266 - val_loss: 1.3888 - learning_rate: 0.0100\n",
      "Epoch 37/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9909 - Precision: 0.9354 - accuracy: 0.9332 - loss: 0.1782\n",
      "Epoch 37: val_accuracy did not improve from 0.75781\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9913 - Precision: 0.9387 - accuracy: 0.9368 - loss: 0.1726 - val_AUC: 0.8481 - val_Precision: 0.7143 - val_accuracy: 0.7070 - val_loss: 1.5882 - learning_rate: 0.0100\n",
      "Epoch 38/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9930 - Precision: 0.9503 - accuracy: 0.9461 - loss: 0.1390\n",
      "Epoch 38: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9931 - Precision: 0.9498 - accuracy: 0.9457 - loss: 0.1394 - val_AUC: 0.8486 - val_Precision: 0.7183 - val_accuracy: 0.7109 - val_loss: 1.4409 - learning_rate: 0.0030\n",
      "Epoch 39/250\n",
      "\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9963 - Precision: 0.9703 - accuracy: 0.9691 - loss: 0.0938\n",
      "Epoch 39: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9962 - Precision: 0.9700 - accuracy: 0.9687 - loss: 0.0947 - val_AUC: 0.8580 - val_Precision: 0.7283 - val_accuracy: 0.7305 - val_loss: 1.3371 - learning_rate: 0.0030\n",
      "Epoch 40/250\n",
      "\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9769 - accuracy: 0.9770 - loss: 0.0732\n",
      "Epoch 40: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9986 - Precision: 0.9765 - accuracy: 0.9764 - loss: 0.0743 - val_AUC: 0.8588 - val_Precision: 0.7460 - val_accuracy: 0.7383 - val_loss: 1.3215 - learning_rate: 0.0030\n",
      "Epoch 41/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9971 - Precision: 0.9780 - accuracy: 0.9780 - loss: 0.0911\n",
      "Epoch 41: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9973 - Precision: 0.9763 - accuracy: 0.9761 - loss: 0.0897 - val_AUC: 0.8578 - val_Precision: 0.7312 - val_accuracy: 0.7266 - val_loss: 1.3323 - learning_rate: 0.0030\n",
      "Epoch 42/250\n",
      "\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9977 - Precision: 0.9675 - accuracy: 0.9643 - loss: 0.0879\n",
      "Epoch 42: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9978 - Precision: 0.9681 - accuracy: 0.9653 - loss: 0.0861 - val_AUC: 0.8514 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 1.3712 - learning_rate: 0.0030\n",
      "Epoch 43/250\n",
      "\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9989 - Precision: 0.9815 - accuracy: 0.9809 - loss: 0.0633\n",
      "Epoch 43: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9986 - Precision: 0.9801 - accuracy: 0.9789 - loss: 0.0685 - val_AUC: 0.8482 - val_Precision: 0.7283 - val_accuracy: 0.7305 - val_loss: 1.4442 - learning_rate: 0.0030\n",
      "Epoch 44/250\n",
      "\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9981 - Precision: 0.9735 - accuracy: 0.9707 - loss: 0.0756\n",
      "Epoch 44: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9980 - Precision: 0.9729 - accuracy: 0.9703 - loss: 0.0769 - val_AUC: 0.8476 - val_Precision: 0.7302 - val_accuracy: 0.7188 - val_loss: 1.4596 - learning_rate: 0.0030\n",
      "Epoch 45/250\n",
      "\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9960 - Precision: 0.9593 - accuracy: 0.9584 - loss: 0.1137\n",
      "Epoch 45: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9965 - Precision: 0.9635 - accuracy: 0.9620 - loss: 0.1053 - val_AUC: 0.8510 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4379 - learning_rate: 0.0030\n",
      "Epoch 46/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9858 - accuracy: 0.9831 - loss: 0.0609\n",
      "Epoch 46: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9991 - Precision: 0.9843 - accuracy: 0.9815 - loss: 0.0616 - val_AUC: 0.8519 - val_Precision: 0.7273 - val_accuracy: 0.7227 - val_loss: 1.4477 - learning_rate: 0.0030\n",
      "Epoch 47/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9783 - accuracy: 0.9744 - loss: 0.0727\n",
      "Epoch 47: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9986 - Precision: 0.9770 - accuracy: 0.9735 - loss: 0.0735 - val_AUC: 0.8537 - val_Precision: 0.7273 - val_accuracy: 0.7266 - val_loss: 1.4466 - learning_rate: 0.0030\n",
      "Epoch 48/250\n",
      "\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9985 - Precision: 0.9828 - accuracy: 0.9803 - loss: 0.0530\n",
      "Epoch 48: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - AUC: 0.9985 - Precision: 0.9825 - accuracy: 0.9801 - loss: 0.0539 - val_AUC: 0.8541 - val_Precision: 0.7283 - val_accuracy: 0.7266 - val_loss: 1.4628 - learning_rate: 0.0030\n",
      "Epoch 49/250\n",
      "\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9843 - accuracy: 0.9809 - loss: 0.0502\n",
      "Epoch 49: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9993 - Precision: 0.9832 - accuracy: 0.9801 - loss: 0.0520 - val_AUC: 0.8505 - val_Precision: 0.7402 - val_accuracy: 0.7383 - val_loss: 1.5166 - learning_rate: 0.0030\n",
      "Epoch 50/250\n",
      "\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9824 - accuracy: 0.9824 - loss: 0.0418 \n",
      "Epoch 50: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9995 - Precision: 0.9826 - accuracy: 0.9823 - loss: 0.0456 - val_AUC: 0.8515 - val_Precision: 0.7412 - val_accuracy: 0.7383 - val_loss: 1.5258 - learning_rate: 0.0030\n",
      "Epoch 51/250\n",
      "\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9969 - Precision: 0.9719 - accuracy: 0.9719 - loss: 0.0907\n",
      "Epoch 51: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9970 - Precision: 0.9729 - accuracy: 0.9729 - loss: 0.0881 - val_AUC: 0.8533 - val_Precision: 0.7362 - val_accuracy: 0.7305 - val_loss: 1.5226 - learning_rate: 0.0030\n",
      "Epoch 52/250\n",
      "\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9982 - Precision: 0.9810 - accuracy: 0.9810 - loss: 0.0675\n",
      "Epoch 52: val_accuracy did not improve from 0.75781\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9982 - Precision: 0.9812 - accuracy: 0.9812 - loss: 0.0660 - val_AUC: 0.8491 - val_Precision: 0.7283 - val_accuracy: 0.7227 - val_loss: 1.5413 - learning_rate: 0.0030\n",
      "Epoch 52: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
    "from tensorflow.keras.initializers import HeNormal, Constant\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    # Input layer + first hidden block\n",
    "    Dense(350,\n",
    "          kernel_initializer=HeNormal(),\n",
    "          bias_initializer=Constant(0.1),\n",
    "          input_shape=(X_trained_processed.shape[1],)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    # Or use LeakyReLU(alpha=0.01):\n",
    "    # LeakyReLU(alpha=0.01),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Second hidden block\n",
    "    Dense(180, kernel_initializer=HeNormal(), bias_initializer=Constant(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Third hidden block\n",
    "    Dense(80, kernel_initializer=HeNormal(), bias_initializer=Constant(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer for 3-class classification\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam + LR schedule\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Save only the best model based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        filepath='gpt2/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=2\n",
    "    ),\n",
    "    \n",
    "    # Stop training early if no improvement (prevents overfitting)\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        patience=35,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate if stuck\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        factor=0.3,\n",
    "        patience=20,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Example fit call:\n",
    "history = model.fit(\n",
    "    X_trained_processed, Y_train_one_hot,\n",
    "    epochs=250,\n",
    "    batch_size=40,\n",
    "    validation_split=.2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3abdb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model(\"gpt2/best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "af698d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_8936\\3946156637.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  Test_X_pca[\"job_posted_date\"]=pd.to_datetime(Test_X_pca[\"job_posted_date\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105220</td>\n",
       "      <td>-0.855140</td>\n",
       "      <td>0.077269</td>\n",
       "      <td>0.340490</td>\n",
       "      <td>0.791361</td>\n",
       "      <td>-0.376866</td>\n",
       "      <td>-0.190330</td>\n",
       "      <td>-0.381584</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.674082</td>\n",
       "      <td>-0.543439</td>\n",
       "      <td>-0.239267</td>\n",
       "      <td>0.530349</td>\n",
       "      <td>0.242087</td>\n",
       "      <td>-0.836608</td>\n",
       "      <td>-0.338640</td>\n",
       "      <td>1.034629</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496271</td>\n",
       "      <td>-0.034273</td>\n",
       "      <td>-0.258909</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>-0.201633</td>\n",
       "      <td>0.420961</td>\n",
       "      <td>-0.408866</td>\n",
       "      <td>0.081551</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.854203</td>\n",
       "      <td>1.057943</td>\n",
       "      <td>0.206921</td>\n",
       "      <td>-0.485724</td>\n",
       "      <td>-0.026322</td>\n",
       "      <td>0.204576</td>\n",
       "      <td>-0.147593</td>\n",
       "      <td>0.052628</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709678</td>\n",
       "      <td>-0.420860</td>\n",
       "      <td>-0.132472</td>\n",
       "      <td>0.558383</td>\n",
       "      <td>-0.909282</td>\n",
       "      <td>-0.282524</td>\n",
       "      <td>-0.171995</td>\n",
       "      <td>-0.615265</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_state feature_1  feature_2  feature_3  feature_4  \\\n",
       "0       Others        CA         A     0.6473          0          0   \n",
       "1       Others        NY         A     0.4238          1          0   \n",
       "2       Others        CA         A     0.6219          1          0   \n",
       "3  Job_Title_5        NY         A     0.6704          0          0   \n",
       "4       Others        CA         A     0.7310          0          0   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8  ...     PCA_8     PCA_9  \\\n",
       "0          1          1          0          0  ... -0.105220 -0.855140   \n",
       "1          0          0          0          0  ... -1.674082 -0.543439   \n",
       "2          0          1          1          0  ... -0.496271 -0.034273   \n",
       "3          0          1          0          0  ... -0.854203  1.057943   \n",
       "4          0          1          1          0  ...  0.709678 -0.420860   \n",
       "\n",
       "     PCA_10    PCA_11    PCA_12    PCA_13    PCA_14    PCA_15  year  month  \n",
       "0  0.077269  0.340490  0.791361 -0.376866 -0.190330 -0.381584  2024      6  \n",
       "1 -0.239267  0.530349  0.242087 -0.836608 -0.338640  1.034629  2024      8  \n",
       "2 -0.258909  0.033545 -0.201633  0.420961 -0.408866  0.081551  2023      1  \n",
       "3  0.206921 -0.485724 -0.026322  0.204576 -0.147593  0.052628  2024      6  \n",
       "4 -0.132472  0.558383 -0.909282 -0.282524 -0.171995 -0.615265  2024      5  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pca=pca.transform(Test_X[job_features])\n",
    "\n",
    "test_pca_df=pd.DataFrame(test_pca,columns=[\"PCA_1\",\"PCA_2\",\"PCA_3\",\"PCA_4\",\"PCA_5\",\"PCA_6\",\"PCA_7\",\"PCA_8\",\"PCA_9\",\"PCA_10\",\"PCA_11\",\"PCA_12\",\"PCA_13\",\"PCA_14\",\"PCA_15\"])\n",
    "\n",
    "new_Test_X=Test_X.drop(columns=job_features)\n",
    "\n",
    "Test_X_pca=pd.concat([new_Test_X,test_pca_df],axis=1)\n",
    "Test_X_pca[\"job_posted_date\"]=pd.to_datetime(Test_X_pca[\"job_posted_date\"])\n",
    "Test_X_pca[\"year\"]=Test_X_pca[\"job_posted_date\"].dt.year\n",
    "Test_X_pca[\"month\"]=Test_X_pca[\"job_posted_date\"].dt.month\n",
    "Test_X_pca.drop(columns=[\"job_posted_date\"],inplace=True)\n",
    "Test_X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f69d20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X_encoded=pipeline.transform(Test_X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ae57e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(Test_X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the mapping based on index\n",
    "index_to_label = {2: 'High', 1: 'Medium', 0: 'Low'}\n",
    "\n",
    "# Get the index of the max probability for each row\n",
    "predicted_indices = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# Map the indices to labels\n",
    "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea9e1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"pca_new_engineer_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a6daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
