{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0aa0c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e53d5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../Dataset/engineers_salary_prediction_train.csv\")\n",
    "test=pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "322cc801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>salary_category</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>job_desc_011</th>\n",
       "      <th>job_desc_012</th>\n",
       "      <th>job_desc_013</th>\n",
       "      <th>job_desc_014</th>\n",
       "      <th>job_desc_015</th>\n",
       "      <th>job_desc_016</th>\n",
       "      <th>job_desc_017</th>\n",
       "      <th>job_desc_018</th>\n",
       "      <th>job_desc_019</th>\n",
       "      <th>job_desc_020</th>\n",
       "      <th>job_desc_021</th>\n",
       "      <th>job_desc_022</th>\n",
       "      <th>job_desc_023</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_261</th>\n",
       "      <th>job_desc_262</th>\n",
       "      <th>job_desc_263</th>\n",
       "      <th>job_desc_264</th>\n",
       "      <th>job_desc_265</th>\n",
       "      <th>job_desc_266</th>\n",
       "      <th>job_desc_267</th>\n",
       "      <th>job_desc_268</th>\n",
       "      <th>job_desc_269</th>\n",
       "      <th>job_desc_270</th>\n",
       "      <th>job_desc_271</th>\n",
       "      <th>job_desc_272</th>\n",
       "      <th>job_desc_273</th>\n",
       "      <th>job_desc_274</th>\n",
       "      <th>job_desc_275</th>\n",
       "      <th>job_desc_276</th>\n",
       "      <th>job_desc_277</th>\n",
       "      <th>job_desc_278</th>\n",
       "      <th>job_desc_279</th>\n",
       "      <th>job_desc_280</th>\n",
       "      <th>job_desc_281</th>\n",
       "      <th>job_desc_282</th>\n",
       "      <th>job_desc_283</th>\n",
       "      <th>job_desc_284</th>\n",
       "      <th>job_desc_285</th>\n",
       "      <th>job_desc_286</th>\n",
       "      <th>job_desc_287</th>\n",
       "      <th>job_desc_288</th>\n",
       "      <th>job_desc_289</th>\n",
       "      <th>job_desc_290</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>High</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.193511</td>\n",
       "      <td>2.275482</td>\n",
       "      <td>-0.440363</td>\n",
       "      <td>-0.327473</td>\n",
       "      <td>0.058464</td>\n",
       "      <td>-0.154043</td>\n",
       "      <td>-0.393158</td>\n",
       "      <td>-0.367905</td>\n",
       "      <td>-0.703665</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>0.058359</td>\n",
       "      <td>-0.632267</td>\n",
       "      <td>0.326872</td>\n",
       "      <td>-0.276131</td>\n",
       "      <td>0.098252</td>\n",
       "      <td>0.019475</td>\n",
       "      <td>0.105348</td>\n",
       "      <td>-0.411405</td>\n",
       "      <td>0.635027</td>\n",
       "      <td>-0.192049</td>\n",
       "      <td>3.849681</td>\n",
       "      <td>0.529550</td>\n",
       "      <td>0.269379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111471</td>\n",
       "      <td>0.266141</td>\n",
       "      <td>-0.142156</td>\n",
       "      <td>-0.684483</td>\n",
       "      <td>-0.174529</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>-0.024953</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>-0.342473</td>\n",
       "      <td>0.185418</td>\n",
       "      <td>-0.233041</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>0.602155</td>\n",
       "      <td>-0.511326</td>\n",
       "      <td>0.602380</td>\n",
       "      <td>-0.254410</td>\n",
       "      <td>-0.114025</td>\n",
       "      <td>0.086060</td>\n",
       "      <td>-0.012974</td>\n",
       "      <td>0.523685</td>\n",
       "      <td>-0.864781</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>0.283154</td>\n",
       "      <td>-0.848735</td>\n",
       "      <td>-0.108635</td>\n",
       "      <td>0.049179</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>-0.535840</td>\n",
       "      <td>0.113221</td>\n",
       "      <td>-0.362079</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100152</td>\n",
       "      <td>2.291134</td>\n",
       "      <td>-0.356041</td>\n",
       "      <td>-0.494735</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>-0.356995</td>\n",
       "      <td>-0.633020</td>\n",
       "      <td>-0.444805</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>0.187210</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>-0.734535</td>\n",
       "      <td>0.264041</td>\n",
       "      <td>-0.209023</td>\n",
       "      <td>0.144627</td>\n",
       "      <td>-0.182167</td>\n",
       "      <td>0.144194</td>\n",
       "      <td>-0.436690</td>\n",
       "      <td>0.405126</td>\n",
       "      <td>-0.240945</td>\n",
       "      <td>3.781854</td>\n",
       "      <td>0.436835</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033901</td>\n",
       "      <td>0.475052</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>-0.412693</td>\n",
       "      <td>-0.424291</td>\n",
       "      <td>0.518947</td>\n",
       "      <td>-0.151527</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>-0.395344</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>-0.198469</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.390649</td>\n",
       "      <td>-0.484126</td>\n",
       "      <td>0.533130</td>\n",
       "      <td>-0.196634</td>\n",
       "      <td>-0.330162</td>\n",
       "      <td>0.200502</td>\n",
       "      <td>0.047225</td>\n",
       "      <td>0.263908</td>\n",
       "      <td>-0.817924</td>\n",
       "      <td>-0.069964</td>\n",
       "      <td>0.357457</td>\n",
       "      <td>0.323456</td>\n",
       "      <td>-0.977607</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-0.053379</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.511633</td>\n",
       "      <td>-0.105435</td>\n",
       "      <td>-0.300989</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>Low</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>48.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.406864</td>\n",
       "      <td>1.986625</td>\n",
       "      <td>-0.726046</td>\n",
       "      <td>-0.316294</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>-0.451118</td>\n",
       "      <td>-0.659871</td>\n",
       "      <td>-0.451544</td>\n",
       "      <td>-0.505597</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>-0.542267</td>\n",
       "      <td>-0.623306</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>-0.103152</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>0.236105</td>\n",
       "      <td>0.204340</td>\n",
       "      <td>-0.280865</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>-0.435184</td>\n",
       "      <td>3.603102</td>\n",
       "      <td>0.318330</td>\n",
       "      <td>0.206301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474084</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>-0.111558</td>\n",
       "      <td>-0.916092</td>\n",
       "      <td>-0.327722</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>0.074941</td>\n",
       "      <td>0.175365</td>\n",
       "      <td>-0.272249</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>-0.153464</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.295818</td>\n",
       "      <td>-0.174998</td>\n",
       "      <td>0.781811</td>\n",
       "      <td>-0.155236</td>\n",
       "      <td>-0.612957</td>\n",
       "      <td>0.254792</td>\n",
       "      <td>-0.063058</td>\n",
       "      <td>0.259242</td>\n",
       "      <td>-0.669112</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>0.200105</td>\n",
       "      <td>0.227985</td>\n",
       "      <td>-0.829408</td>\n",
       "      <td>-0.361299</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>-0.118158</td>\n",
       "      <td>-0.654845</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>-0.406159</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs    job_title job_posted_date  ... job_desc_298 job_desc_299 job_desc_300\n",
       "0    1       Others         2024/07  ...    -0.587955    -0.919095    -0.207340\n",
       "1    2  Job_Title_1         2024/07  ...    -0.893224    -0.823024     0.112364\n",
       "2    3       Others         2024/07  ...     0.000000     0.000000     0.000000\n",
       "3    4       Others         2024/07  ...     0.000000     0.000000     0.000000\n",
       "4    5       Others         2024/07  ...    -0.696853    -0.601466     0.089939\n",
       "\n",
       "[5 rows x 317 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c5612404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>job_desc_011</th>\n",
       "      <th>job_desc_012</th>\n",
       "      <th>job_desc_013</th>\n",
       "      <th>job_desc_014</th>\n",
       "      <th>job_desc_015</th>\n",
       "      <th>job_desc_016</th>\n",
       "      <th>job_desc_017</th>\n",
       "      <th>job_desc_018</th>\n",
       "      <th>job_desc_019</th>\n",
       "      <th>job_desc_020</th>\n",
       "      <th>job_desc_021</th>\n",
       "      <th>job_desc_022</th>\n",
       "      <th>job_desc_023</th>\n",
       "      <th>job_desc_024</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_261</th>\n",
       "      <th>job_desc_262</th>\n",
       "      <th>job_desc_263</th>\n",
       "      <th>job_desc_264</th>\n",
       "      <th>job_desc_265</th>\n",
       "      <th>job_desc_266</th>\n",
       "      <th>job_desc_267</th>\n",
       "      <th>job_desc_268</th>\n",
       "      <th>job_desc_269</th>\n",
       "      <th>job_desc_270</th>\n",
       "      <th>job_desc_271</th>\n",
       "      <th>job_desc_272</th>\n",
       "      <th>job_desc_273</th>\n",
       "      <th>job_desc_274</th>\n",
       "      <th>job_desc_275</th>\n",
       "      <th>job_desc_276</th>\n",
       "      <th>job_desc_277</th>\n",
       "      <th>job_desc_278</th>\n",
       "      <th>job_desc_279</th>\n",
       "      <th>job_desc_280</th>\n",
       "      <th>job_desc_281</th>\n",
       "      <th>job_desc_282</th>\n",
       "      <th>job_desc_283</th>\n",
       "      <th>job_desc_284</th>\n",
       "      <th>job_desc_285</th>\n",
       "      <th>job_desc_286</th>\n",
       "      <th>job_desc_287</th>\n",
       "      <th>job_desc_288</th>\n",
       "      <th>job_desc_289</th>\n",
       "      <th>job_desc_290</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1281</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.265933</td>\n",
       "      <td>2.070912</td>\n",
       "      <td>-0.646306</td>\n",
       "      <td>-0.318699</td>\n",
       "      <td>-0.181419</td>\n",
       "      <td>-0.406402</td>\n",
       "      <td>-0.555239</td>\n",
       "      <td>-0.625735</td>\n",
       "      <td>-0.488355</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>-0.602128</td>\n",
       "      <td>-0.581684</td>\n",
       "      <td>0.518630</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.271015</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.236189</td>\n",
       "      <td>-0.152509</td>\n",
       "      <td>0.564038</td>\n",
       "      <td>-0.175079</td>\n",
       "      <td>3.536066</td>\n",
       "      <td>0.403862</td>\n",
       "      <td>0.416160</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583465</td>\n",
       "      <td>0.513510</td>\n",
       "      <td>-0.056576</td>\n",
       "      <td>-0.912723</td>\n",
       "      <td>0.052374</td>\n",
       "      <td>0.419195</td>\n",
       "      <td>-0.223077</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>-0.279963</td>\n",
       "      <td>-0.155815</td>\n",
       "      <td>-0.598799</td>\n",
       "      <td>-0.124844</td>\n",
       "      <td>0.193408</td>\n",
       "      <td>-0.324402</td>\n",
       "      <td>0.556537</td>\n",
       "      <td>-0.214403</td>\n",
       "      <td>-0.361113</td>\n",
       "      <td>0.395566</td>\n",
       "      <td>-0.100708</td>\n",
       "      <td>0.081053</td>\n",
       "      <td>-0.291276</td>\n",
       "      <td>-0.084332</td>\n",
       "      <td>0.614833</td>\n",
       "      <td>0.341948</td>\n",
       "      <td>-0.535607</td>\n",
       "      <td>-0.385840</td>\n",
       "      <td>-0.036796</td>\n",
       "      <td>-0.100846</td>\n",
       "      <td>-0.423849</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>-0.054078</td>\n",
       "      <td>-0.573635</td>\n",
       "      <td>-0.306883</td>\n",
       "      <td>-0.325092</td>\n",
       "      <td>0.089463</td>\n",
       "      <td>-0.353476</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.667958</td>\n",
       "      <td>-0.702116</td>\n",
       "      <td>-0.206267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/08</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.134647</td>\n",
       "      <td>2.323485</td>\n",
       "      <td>-0.545002</td>\n",
       "      <td>-0.398136</td>\n",
       "      <td>-0.219534</td>\n",
       "      <td>-0.593899</td>\n",
       "      <td>-0.934010</td>\n",
       "      <td>-0.347509</td>\n",
       "      <td>-0.150662</td>\n",
       "      <td>-0.025937</td>\n",
       "      <td>-0.788193</td>\n",
       "      <td>-0.758666</td>\n",
       "      <td>0.395563</td>\n",
       "      <td>-0.069356</td>\n",
       "      <td>-0.188280</td>\n",
       "      <td>-0.038201</td>\n",
       "      <td>0.553241</td>\n",
       "      <td>-0.329062</td>\n",
       "      <td>0.405169</td>\n",
       "      <td>0.280156</td>\n",
       "      <td>3.380567</td>\n",
       "      <td>0.446040</td>\n",
       "      <td>0.106416</td>\n",
       "      <td>-0.187433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381289</td>\n",
       "      <td>0.264247</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>-1.098606</td>\n",
       "      <td>-0.149598</td>\n",
       "      <td>0.528904</td>\n",
       "      <td>0.063392</td>\n",
       "      <td>-0.291882</td>\n",
       "      <td>-0.414426</td>\n",
       "      <td>-0.078166</td>\n",
       "      <td>-0.276882</td>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.264712</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.525958</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.313780</td>\n",
       "      <td>0.454644</td>\n",
       "      <td>-0.147099</td>\n",
       "      <td>0.385120</td>\n",
       "      <td>-0.784835</td>\n",
       "      <td>-0.293819</td>\n",
       "      <td>0.245624</td>\n",
       "      <td>0.277302</td>\n",
       "      <td>-0.678327</td>\n",
       "      <td>-0.459686</td>\n",
       "      <td>-0.096691</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>-0.497901</td>\n",
       "      <td>-0.123180</td>\n",
       "      <td>-0.868718</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>-0.179036</td>\n",
       "      <td>-0.717763</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>-0.190448</td>\n",
       "      <td>-1.261702</td>\n",
       "      <td>-0.505897</td>\n",
       "      <td>0.082080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283</td>\n",
       "      <td>Others</td>\n",
       "      <td>2023/01</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>120.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.146202</td>\n",
       "      <td>2.149297</td>\n",
       "      <td>-0.473768</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.059767</td>\n",
       "      <td>-0.347324</td>\n",
       "      <td>-0.462137</td>\n",
       "      <td>-0.635334</td>\n",
       "      <td>-0.460823</td>\n",
       "      <td>0.114211</td>\n",
       "      <td>-0.196667</td>\n",
       "      <td>-0.666750</td>\n",
       "      <td>0.199481</td>\n",
       "      <td>-0.359138</td>\n",
       "      <td>-0.244184</td>\n",
       "      <td>-0.033460</td>\n",
       "      <td>0.378710</td>\n",
       "      <td>-0.369817</td>\n",
       "      <td>0.560494</td>\n",
       "      <td>0.048275</td>\n",
       "      <td>3.746783</td>\n",
       "      <td>0.286187</td>\n",
       "      <td>0.286217</td>\n",
       "      <td>0.398518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244125</td>\n",
       "      <td>0.176481</td>\n",
       "      <td>0.042843</td>\n",
       "      <td>-0.743970</td>\n",
       "      <td>-0.036478</td>\n",
       "      <td>0.152495</td>\n",
       "      <td>-0.037869</td>\n",
       "      <td>-0.367833</td>\n",
       "      <td>-0.422593</td>\n",
       "      <td>0.244620</td>\n",
       "      <td>-0.238463</td>\n",
       "      <td>-0.038709</td>\n",
       "      <td>0.382557</td>\n",
       "      <td>-0.725669</td>\n",
       "      <td>0.360670</td>\n",
       "      <td>-0.064571</td>\n",
       "      <td>-0.200544</td>\n",
       "      <td>0.297640</td>\n",
       "      <td>-0.207100</td>\n",
       "      <td>0.574868</td>\n",
       "      <td>-0.784014</td>\n",
       "      <td>-0.083340</td>\n",
       "      <td>0.543385</td>\n",
       "      <td>0.521625</td>\n",
       "      <td>-0.876975</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.028694</td>\n",
       "      <td>0.067733</td>\n",
       "      <td>-0.627439</td>\n",
       "      <td>-0.385466</td>\n",
       "      <td>-0.416109</td>\n",
       "      <td>-0.619822</td>\n",
       "      <td>-0.493653</td>\n",
       "      <td>-0.347556</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>-0.331212</td>\n",
       "      <td>-0.381348</td>\n",
       "      <td>-0.506540</td>\n",
       "      <td>-0.773561</td>\n",
       "      <td>-0.105221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1284</td>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.278451</td>\n",
       "      <td>1.929512</td>\n",
       "      <td>-0.400708</td>\n",
       "      <td>-0.395092</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>-0.280901</td>\n",
       "      <td>-0.719048</td>\n",
       "      <td>-0.251112</td>\n",
       "      <td>-0.182981</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>-0.350532</td>\n",
       "      <td>-0.528154</td>\n",
       "      <td>0.531242</td>\n",
       "      <td>-0.328141</td>\n",
       "      <td>0.200239</td>\n",
       "      <td>0.136406</td>\n",
       "      <td>0.199327</td>\n",
       "      <td>-0.246777</td>\n",
       "      <td>0.501189</td>\n",
       "      <td>-0.470432</td>\n",
       "      <td>3.739622</td>\n",
       "      <td>-0.049199</td>\n",
       "      <td>0.276417</td>\n",
       "      <td>-0.023738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349094</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>-0.080796</td>\n",
       "      <td>-0.612868</td>\n",
       "      <td>-0.063049</td>\n",
       "      <td>0.588547</td>\n",
       "      <td>-0.198869</td>\n",
       "      <td>0.219284</td>\n",
       "      <td>-0.383540</td>\n",
       "      <td>-0.093432</td>\n",
       "      <td>-0.401378</td>\n",
       "      <td>-0.145946</td>\n",
       "      <td>0.446052</td>\n",
       "      <td>-0.765058</td>\n",
       "      <td>0.194471</td>\n",
       "      <td>-0.045353</td>\n",
       "      <td>-0.112149</td>\n",
       "      <td>0.289670</td>\n",
       "      <td>0.147988</td>\n",
       "      <td>0.422955</td>\n",
       "      <td>-0.687581</td>\n",
       "      <td>-0.015552</td>\n",
       "      <td>0.421841</td>\n",
       "      <td>0.290365</td>\n",
       "      <td>-0.965742</td>\n",
       "      <td>0.220519</td>\n",
       "      <td>-0.214108</td>\n",
       "      <td>0.211355</td>\n",
       "      <td>-0.609563</td>\n",
       "      <td>-0.238449</td>\n",
       "      <td>-0.297560</td>\n",
       "      <td>-0.481448</td>\n",
       "      <td>-0.497642</td>\n",
       "      <td>-0.254823</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>-0.362739</td>\n",
       "      <td>-0.102704</td>\n",
       "      <td>-0.491272</td>\n",
       "      <td>-0.808156</td>\n",
       "      <td>-0.048326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1285</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>144.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.154716</td>\n",
       "      <td>2.221011</td>\n",
       "      <td>-0.493091</td>\n",
       "      <td>-0.319782</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>-0.265350</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.306911</td>\n",
       "      <td>-0.237550</td>\n",
       "      <td>0.454508</td>\n",
       "      <td>-0.776502</td>\n",
       "      <td>-0.492894</td>\n",
       "      <td>0.406902</td>\n",
       "      <td>-0.095398</td>\n",
       "      <td>-0.056578</td>\n",
       "      <td>-0.266051</td>\n",
       "      <td>0.043975</td>\n",
       "      <td>-0.335315</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>-0.352373</td>\n",
       "      <td>3.555421</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.208665</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051410</td>\n",
       "      <td>0.517515</td>\n",
       "      <td>-0.173539</td>\n",
       "      <td>-0.700081</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>-0.037963</td>\n",
       "      <td>0.029592</td>\n",
       "      <td>-0.343201</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>-0.278971</td>\n",
       "      <td>0.151569</td>\n",
       "      <td>0.326661</td>\n",
       "      <td>-0.468884</td>\n",
       "      <td>0.576501</td>\n",
       "      <td>-0.213073</td>\n",
       "      <td>-0.532033</td>\n",
       "      <td>0.322577</td>\n",
       "      <td>-0.142936</td>\n",
       "      <td>0.231748</td>\n",
       "      <td>-0.622124</td>\n",
       "      <td>-0.180121</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.171529</td>\n",
       "      <td>-0.699755</td>\n",
       "      <td>-0.575953</td>\n",
       "      <td>0.068527</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-0.331506</td>\n",
       "      <td>-0.216373</td>\n",
       "      <td>-0.176458</td>\n",
       "      <td>-0.726473</td>\n",
       "      <td>-0.323976</td>\n",
       "      <td>-0.145825</td>\n",
       "      <td>-0.046866</td>\n",
       "      <td>-0.229873</td>\n",
       "      <td>-0.568318</td>\n",
       "      <td>-0.614605</td>\n",
       "      <td>-0.770506</td>\n",
       "      <td>0.142140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    obs    job_title job_posted_date  ... job_desc_298 job_desc_299  job_desc_300\n",
       "0  1281       Others         2024/06  ...    -0.667958    -0.702116     -0.206267\n",
       "1  1282       Others         2024/08  ...    -1.261702    -0.505897      0.082080\n",
       "2  1283       Others         2023/01  ...    -0.506540    -0.773561     -0.105221\n",
       "3  1284  Job_Title_5         2024/06  ...    -0.491272    -0.808156     -0.048326\n",
       "4  1285       Others         2024/05  ...    -0.614605    -0.770506      0.142140\n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1523a683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns=df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "51da238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns=df.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "len(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "54569aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns=df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "len(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "31faf8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('O'), dtype('float64'), dtype('bool')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0427cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2024/07', '2024/01', '2024/05', '2024/06', '2023/02', '2024/08',\n",
       "       '2024/04', '2023/09', '2022/05', '2024/02', '2023/07', '2023/03',\n",
       "       '2023/10', '2021/04', '2022/03', '2024/03', '2023/01', '2022/04',\n",
       "       '2023/11', '2023/06', '2022/02', '2022/12', '2022/10', '2023/12',\n",
       "       '2023/05', nan, '2021/06', '2023/04', '2021/08', '2021/02',\n",
       "       '2022/06', '2023/08', '2022/01', '2020/09', '2021/12', '2021/07',\n",
       "       '2021/03', '2022/11', '2020/12', '2018/11', '2021/01', '2021/11'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_posted_date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dc90923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title\n",
       "Others          889\n",
       "Job_Title_10     53\n",
       "Job_Title_5      51\n",
       "Job_Title_3      46\n",
       "Job_Title_8      40\n",
       "Job_Title_7      38\n",
       "Job_Title_2      32\n",
       "Job_Title_1      27\n",
       "Job_Title_13     11\n",
       "Job_Title_23     10\n",
       "Job_Title_11      9\n",
       "Job_Title_19      7\n",
       "Job_Title_12      7\n",
       "Job_Title_16      6\n",
       "Job_Title_20      6\n",
       "Job_Title_6       5\n",
       "Job_Title_4       5\n",
       "Job_Title_9       4\n",
       "Job_Title_25      4\n",
       "Job_Title_15      4\n",
       "Job_Title_17      4\n",
       "Job_Title_18      4\n",
       "Job_Title_21      4\n",
       "Job_Title_26      4\n",
       "Job_Title_14      3\n",
       "Job_Title_24      3\n",
       "Job_Title_22      3\n",
       "Job_Title_27      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "169b71ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary_category\n",
       "High      501\n",
       "Low       419\n",
       "Medium    360\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"salary_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6ec51f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_catagory(val):\n",
    "    if(val==\"High\"):\n",
    "        return 1\n",
    "    elif(val==\"Medium\"):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aa489db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"salary_category\"]=df[\"salary_category\"].apply(job_catagory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c3fb46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>salary_category</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>job_desc_011</th>\n",
       "      <th>job_desc_012</th>\n",
       "      <th>job_desc_013</th>\n",
       "      <th>job_desc_014</th>\n",
       "      <th>job_desc_015</th>\n",
       "      <th>job_desc_016</th>\n",
       "      <th>job_desc_017</th>\n",
       "      <th>job_desc_018</th>\n",
       "      <th>job_desc_019</th>\n",
       "      <th>job_desc_020</th>\n",
       "      <th>job_desc_021</th>\n",
       "      <th>job_desc_022</th>\n",
       "      <th>job_desc_023</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_261</th>\n",
       "      <th>job_desc_262</th>\n",
       "      <th>job_desc_263</th>\n",
       "      <th>job_desc_264</th>\n",
       "      <th>job_desc_265</th>\n",
       "      <th>job_desc_266</th>\n",
       "      <th>job_desc_267</th>\n",
       "      <th>job_desc_268</th>\n",
       "      <th>job_desc_269</th>\n",
       "      <th>job_desc_270</th>\n",
       "      <th>job_desc_271</th>\n",
       "      <th>job_desc_272</th>\n",
       "      <th>job_desc_273</th>\n",
       "      <th>job_desc_274</th>\n",
       "      <th>job_desc_275</th>\n",
       "      <th>job_desc_276</th>\n",
       "      <th>job_desc_277</th>\n",
       "      <th>job_desc_278</th>\n",
       "      <th>job_desc_279</th>\n",
       "      <th>job_desc_280</th>\n",
       "      <th>job_desc_281</th>\n",
       "      <th>job_desc_282</th>\n",
       "      <th>job_desc_283</th>\n",
       "      <th>job_desc_284</th>\n",
       "      <th>job_desc_285</th>\n",
       "      <th>job_desc_286</th>\n",
       "      <th>job_desc_287</th>\n",
       "      <th>job_desc_288</th>\n",
       "      <th>job_desc_289</th>\n",
       "      <th>job_desc_290</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>1</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.193511</td>\n",
       "      <td>2.275482</td>\n",
       "      <td>-0.440363</td>\n",
       "      <td>-0.327473</td>\n",
       "      <td>0.058464</td>\n",
       "      <td>-0.154043</td>\n",
       "      <td>-0.393158</td>\n",
       "      <td>-0.367905</td>\n",
       "      <td>-0.703665</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>0.058359</td>\n",
       "      <td>-0.632267</td>\n",
       "      <td>0.326872</td>\n",
       "      <td>-0.276131</td>\n",
       "      <td>0.098252</td>\n",
       "      <td>0.019475</td>\n",
       "      <td>0.105348</td>\n",
       "      <td>-0.411405</td>\n",
       "      <td>0.635027</td>\n",
       "      <td>-0.192049</td>\n",
       "      <td>3.849681</td>\n",
       "      <td>0.529550</td>\n",
       "      <td>0.269379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111471</td>\n",
       "      <td>0.266141</td>\n",
       "      <td>-0.142156</td>\n",
       "      <td>-0.684483</td>\n",
       "      <td>-0.174529</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>-0.024953</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>-0.342473</td>\n",
       "      <td>0.185418</td>\n",
       "      <td>-0.233041</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>0.602155</td>\n",
       "      <td>-0.511326</td>\n",
       "      <td>0.602380</td>\n",
       "      <td>-0.254410</td>\n",
       "      <td>-0.114025</td>\n",
       "      <td>0.086060</td>\n",
       "      <td>-0.012974</td>\n",
       "      <td>0.523685</td>\n",
       "      <td>-0.864781</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>0.283154</td>\n",
       "      <td>-0.848735</td>\n",
       "      <td>-0.108635</td>\n",
       "      <td>0.049179</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>-0.535840</td>\n",
       "      <td>0.113221</td>\n",
       "      <td>-0.362079</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100152</td>\n",
       "      <td>2.291134</td>\n",
       "      <td>-0.356041</td>\n",
       "      <td>-0.494735</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>-0.356995</td>\n",
       "      <td>-0.633020</td>\n",
       "      <td>-0.444805</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>0.187210</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>-0.734535</td>\n",
       "      <td>0.264041</td>\n",
       "      <td>-0.209023</td>\n",
       "      <td>0.144627</td>\n",
       "      <td>-0.182167</td>\n",
       "      <td>0.144194</td>\n",
       "      <td>-0.436690</td>\n",
       "      <td>0.405126</td>\n",
       "      <td>-0.240945</td>\n",
       "      <td>3.781854</td>\n",
       "      <td>0.436835</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033901</td>\n",
       "      <td>0.475052</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>-0.412693</td>\n",
       "      <td>-0.424291</td>\n",
       "      <td>0.518947</td>\n",
       "      <td>-0.151527</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>-0.395344</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>-0.198469</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.390649</td>\n",
       "      <td>-0.484126</td>\n",
       "      <td>0.533130</td>\n",
       "      <td>-0.196634</td>\n",
       "      <td>-0.330162</td>\n",
       "      <td>0.200502</td>\n",
       "      <td>0.047225</td>\n",
       "      <td>0.263908</td>\n",
       "      <td>-0.817924</td>\n",
       "      <td>-0.069964</td>\n",
       "      <td>0.357457</td>\n",
       "      <td>0.323456</td>\n",
       "      <td>-0.977607</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-0.053379</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.511633</td>\n",
       "      <td>-0.105435</td>\n",
       "      <td>-0.300989</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>48.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.406864</td>\n",
       "      <td>1.986625</td>\n",
       "      <td>-0.726046</td>\n",
       "      <td>-0.316294</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>-0.451118</td>\n",
       "      <td>-0.659871</td>\n",
       "      <td>-0.451544</td>\n",
       "      <td>-0.505597</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>-0.542267</td>\n",
       "      <td>-0.623306</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>-0.103152</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>0.236105</td>\n",
       "      <td>0.204340</td>\n",
       "      <td>-0.280865</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>-0.435184</td>\n",
       "      <td>3.603102</td>\n",
       "      <td>0.318330</td>\n",
       "      <td>0.206301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474084</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>-0.111558</td>\n",
       "      <td>-0.916092</td>\n",
       "      <td>-0.327722</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>0.074941</td>\n",
       "      <td>0.175365</td>\n",
       "      <td>-0.272249</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>-0.153464</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.295818</td>\n",
       "      <td>-0.174998</td>\n",
       "      <td>0.781811</td>\n",
       "      <td>-0.155236</td>\n",
       "      <td>-0.612957</td>\n",
       "      <td>0.254792</td>\n",
       "      <td>-0.063058</td>\n",
       "      <td>0.259242</td>\n",
       "      <td>-0.669112</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>0.200105</td>\n",
       "      <td>0.227985</td>\n",
       "      <td>-0.829408</td>\n",
       "      <td>-0.361299</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>-0.118158</td>\n",
       "      <td>-0.654845</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>-0.406159</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/01</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6681</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>1.783212</td>\n",
       "      <td>-1.191290</td>\n",
       "      <td>-0.955164</td>\n",
       "      <td>-0.270447</td>\n",
       "      <td>-0.029024</td>\n",
       "      <td>-1.426136</td>\n",
       "      <td>-0.441080</td>\n",
       "      <td>-1.171122</td>\n",
       "      <td>0.393520</td>\n",
       "      <td>-0.514735</td>\n",
       "      <td>-0.355996</td>\n",
       "      <td>0.502425</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.166617</td>\n",
       "      <td>0.771915</td>\n",
       "      <td>0.211779</td>\n",
       "      <td>-0.441059</td>\n",
       "      <td>-0.062577</td>\n",
       "      <td>0.275864</td>\n",
       "      <td>2.480022</td>\n",
       "      <td>0.391997</td>\n",
       "      <td>0.342877</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.733511</td>\n",
       "      <td>-0.380078</td>\n",
       "      <td>-0.085317</td>\n",
       "      <td>-0.340938</td>\n",
       "      <td>-0.381462</td>\n",
       "      <td>0.252974</td>\n",
       "      <td>0.755414</td>\n",
       "      <td>1.140219</td>\n",
       "      <td>-0.630318</td>\n",
       "      <td>-0.345270</td>\n",
       "      <td>-1.029515</td>\n",
       "      <td>-0.709832</td>\n",
       "      <td>0.756876</td>\n",
       "      <td>-0.634714</td>\n",
       "      <td>-0.692713</td>\n",
       "      <td>0.175582</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>0.161775</td>\n",
       "      <td>-0.187084</td>\n",
       "      <td>0.959412</td>\n",
       "      <td>0.259934</td>\n",
       "      <td>-0.203246</td>\n",
       "      <td>0.119549</td>\n",
       "      <td>0.255281</td>\n",
       "      <td>-0.014779</td>\n",
       "      <td>0.443926</td>\n",
       "      <td>0.467664</td>\n",
       "      <td>0.333582</td>\n",
       "      <td>-0.299466</td>\n",
       "      <td>-0.199638</td>\n",
       "      <td>-0.249678</td>\n",
       "      <td>-0.998258</td>\n",
       "      <td>-1.017181</td>\n",
       "      <td>0.372677</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>-0.440318</td>\n",
       "      <td>-0.442595</td>\n",
       "      <td>-0.761192</td>\n",
       "      <td>-0.606944</td>\n",
       "      <td>-0.123160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.011472</td>\n",
       "      <td>2.197361</td>\n",
       "      <td>-0.971281</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>-0.013852</td>\n",
       "      <td>-0.312648</td>\n",
       "      <td>-0.739497</td>\n",
       "      <td>-0.290805</td>\n",
       "      <td>-0.498925</td>\n",
       "      <td>0.508402</td>\n",
       "      <td>-0.355440</td>\n",
       "      <td>-0.566840</td>\n",
       "      <td>0.418575</td>\n",
       "      <td>-0.162873</td>\n",
       "      <td>0.139406</td>\n",
       "      <td>0.333299</td>\n",
       "      <td>0.243008</td>\n",
       "      <td>-0.196966</td>\n",
       "      <td>0.518859</td>\n",
       "      <td>-0.009066</td>\n",
       "      <td>3.564287</td>\n",
       "      <td>0.288561</td>\n",
       "      <td>0.486514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221015</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.089997</td>\n",
       "      <td>-0.684353</td>\n",
       "      <td>-0.060202</td>\n",
       "      <td>0.435575</td>\n",
       "      <td>-0.181986</td>\n",
       "      <td>0.082772</td>\n",
       "      <td>-0.277559</td>\n",
       "      <td>0.049794</td>\n",
       "      <td>-0.118941</td>\n",
       "      <td>-0.097852</td>\n",
       "      <td>0.428179</td>\n",
       "      <td>-0.546144</td>\n",
       "      <td>0.577969</td>\n",
       "      <td>-0.134802</td>\n",
       "      <td>-0.489609</td>\n",
       "      <td>0.324994</td>\n",
       "      <td>0.038679</td>\n",
       "      <td>0.698440</td>\n",
       "      <td>-0.822247</td>\n",
       "      <td>0.024653</td>\n",
       "      <td>0.508145</td>\n",
       "      <td>0.398042</td>\n",
       "      <td>-0.563848</td>\n",
       "      <td>-0.413169</td>\n",
       "      <td>-0.168477</td>\n",
       "      <td>0.261310</td>\n",
       "      <td>-0.627042</td>\n",
       "      <td>-0.295964</td>\n",
       "      <td>-0.158930</td>\n",
       "      <td>-0.684944</td>\n",
       "      <td>-0.484077</td>\n",
       "      <td>-0.289621</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>-0.445414</td>\n",
       "      <td>-0.321164</td>\n",
       "      <td>-0.700979</td>\n",
       "      <td>-0.849695</td>\n",
       "      <td>0.004260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>3</td>\n",
       "      <td>NC</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4661</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7349</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>2.042311</td>\n",
       "      <td>-0.470199</td>\n",
       "      <td>-0.142202</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>-0.103648</td>\n",
       "      <td>-0.696957</td>\n",
       "      <td>-0.638606</td>\n",
       "      <td>-0.414142</td>\n",
       "      <td>0.403257</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>-0.561948</td>\n",
       "      <td>0.282535</td>\n",
       "      <td>-0.431970</td>\n",
       "      <td>0.148050</td>\n",
       "      <td>-0.095231</td>\n",
       "      <td>0.066244</td>\n",
       "      <td>-0.222850</td>\n",
       "      <td>0.267926</td>\n",
       "      <td>-0.232987</td>\n",
       "      <td>4.076224</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.176249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159608</td>\n",
       "      <td>0.516007</td>\n",
       "      <td>-0.018415</td>\n",
       "      <td>-0.578263</td>\n",
       "      <td>-0.367416</td>\n",
       "      <td>0.269545</td>\n",
       "      <td>-0.068014</td>\n",
       "      <td>0.066438</td>\n",
       "      <td>-0.220712</td>\n",
       "      <td>-0.073060</td>\n",
       "      <td>-0.186099</td>\n",
       "      <td>0.229099</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>-0.073286</td>\n",
       "      <td>0.634077</td>\n",
       "      <td>-0.029616</td>\n",
       "      <td>-0.243677</td>\n",
       "      <td>0.182301</td>\n",
       "      <td>0.058211</td>\n",
       "      <td>0.113522</td>\n",
       "      <td>-0.698995</td>\n",
       "      <td>0.160198</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>0.287473</td>\n",
       "      <td>-0.768050</td>\n",
       "      <td>-0.238450</td>\n",
       "      <td>-0.360325</td>\n",
       "      <td>0.141020</td>\n",
       "      <td>-0.652147</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>-0.113091</td>\n",
       "      <td>-0.426043</td>\n",
       "      <td>-0.175965</td>\n",
       "      <td>-0.147661</td>\n",
       "      <td>0.197479</td>\n",
       "      <td>-0.207085</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>-0.569806</td>\n",
       "      <td>-0.584224</td>\n",
       "      <td>0.143645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>1</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>1.685989</td>\n",
       "      <td>-0.464767</td>\n",
       "      <td>-0.331728</td>\n",
       "      <td>-0.174358</td>\n",
       "      <td>-0.470768</td>\n",
       "      <td>-1.034051</td>\n",
       "      <td>-0.499993</td>\n",
       "      <td>-0.318963</td>\n",
       "      <td>-0.062629</td>\n",
       "      <td>-0.145777</td>\n",
       "      <td>-0.451793</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>-0.325305</td>\n",
       "      <td>-0.199183</td>\n",
       "      <td>-0.069669</td>\n",
       "      <td>-0.099558</td>\n",
       "      <td>-0.298029</td>\n",
       "      <td>0.565138</td>\n",
       "      <td>0.023049</td>\n",
       "      <td>3.876088</td>\n",
       "      <td>0.289247</td>\n",
       "      <td>0.439327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391872</td>\n",
       "      <td>0.293312</td>\n",
       "      <td>-0.141985</td>\n",
       "      <td>-0.590205</td>\n",
       "      <td>-0.373565</td>\n",
       "      <td>0.391756</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>-0.026017</td>\n",
       "      <td>-0.226575</td>\n",
       "      <td>-0.492045</td>\n",
       "      <td>-0.173868</td>\n",
       "      <td>-0.386220</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>-0.304582</td>\n",
       "      <td>0.502670</td>\n",
       "      <td>-0.040141</td>\n",
       "      <td>-0.399059</td>\n",
       "      <td>0.404345</td>\n",
       "      <td>0.122063</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>-0.831696</td>\n",
       "      <td>-0.019179</td>\n",
       "      <td>0.189408</td>\n",
       "      <td>0.108477</td>\n",
       "      <td>-0.670186</td>\n",
       "      <td>-0.249211</td>\n",
       "      <td>-0.246905</td>\n",
       "      <td>0.078171</td>\n",
       "      <td>-0.428274</td>\n",
       "      <td>-0.359708</td>\n",
       "      <td>-0.649244</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>-0.138691</td>\n",
       "      <td>-0.478873</td>\n",
       "      <td>-0.115098</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>-0.157343</td>\n",
       "      <td>-0.859955</td>\n",
       "      <td>-0.689171</td>\n",
       "      <td>0.005245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs    job_title job_posted_date  ...  job_desc_298 job_desc_299 job_desc_300\n",
       "0    1       Others         2024/07  ...     -0.587955    -0.919095    -0.207340\n",
       "1    2  Job_Title_1         2024/07  ...     -0.893224    -0.823024     0.112364\n",
       "2    3       Others         2024/07  ...      0.000000     0.000000     0.000000\n",
       "3    4       Others         2024/07  ...      0.000000     0.000000     0.000000\n",
       "4    5       Others         2024/07  ...     -0.696853    -0.601466     0.089939\n",
       "5    6       Others         2024/01  ...     -0.761192    -0.606944    -0.123160\n",
       "6    7       Others         2024/07  ...     -0.700979    -0.849695     0.004260\n",
       "7    8       Others         2024/07  ...      0.000000     0.000000     0.000000\n",
       "8    9       Others         2024/07  ...     -0.569806    -0.584224     0.143645\n",
       "9   10       Others         2024/05  ...     -0.859955    -0.689171     0.005245\n",
       "\n",
       "[10 rows x 317 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ccb40ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "70f0edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=df.drop(columns=[\"salary_category\"])\n",
    "Y_train=df[\"salary_category\"]\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'bool':\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "        test[col] = test[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ecbf0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop(columns=[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ac3c65f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 311 numaric columns and 4 catagorical columns\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,OrdinalEncoder\n",
    "\n",
    "\n",
    "numaric_columns=X_train.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "catagory_columns=X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numaric_columns)} numaric columns and {len(catagory_columns)} catagorical columns\")\n",
    "num_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"mean\")),\n",
    "        (\"standard\",StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"odrinal\",OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transform=ColumnTransformer(\n",
    "    [\n",
    "        (\"numaric\",num_pipeline,numaric_columns),\n",
    "        (\"cat\",cat_pipeline,catagory_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929ecce",
   "metadata": {},
   "source": [
    "## LightGBM Model Train Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "\n",
    "# lgb_model = lgb.LGBMClassifier(\n",
    "#     objective='multiclass',\n",
    "#     num_class=len(np.unique(Y_train)),\n",
    "#     boosting_type='gbdt',\n",
    "#     learning_rate=0.05,\n",
    "#     n_estimators=2000,\n",
    "#     max_depth=12,\n",
    "#     num_leaves=30,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     min_child_samples=20,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# model=Pipeline(\n",
    "#     [\n",
    "#         (\"transform\",column_transform),\n",
    "#         (\"lgb\",lgb_model)\n",
    "#     ]\n",
    "# )\n",
    "# model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c02525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c89afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb53df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>job_desc_011</th>\n",
       "      <th>job_desc_012</th>\n",
       "      <th>job_desc_013</th>\n",
       "      <th>job_desc_014</th>\n",
       "      <th>job_desc_015</th>\n",
       "      <th>job_desc_016</th>\n",
       "      <th>job_desc_017</th>\n",
       "      <th>job_desc_018</th>\n",
       "      <th>job_desc_019</th>\n",
       "      <th>job_desc_020</th>\n",
       "      <th>job_desc_021</th>\n",
       "      <th>job_desc_022</th>\n",
       "      <th>job_desc_023</th>\n",
       "      <th>job_desc_024</th>\n",
       "      <th>job_desc_025</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_261</th>\n",
       "      <th>job_desc_262</th>\n",
       "      <th>job_desc_263</th>\n",
       "      <th>job_desc_264</th>\n",
       "      <th>job_desc_265</th>\n",
       "      <th>job_desc_266</th>\n",
       "      <th>job_desc_267</th>\n",
       "      <th>job_desc_268</th>\n",
       "      <th>job_desc_269</th>\n",
       "      <th>job_desc_270</th>\n",
       "      <th>job_desc_271</th>\n",
       "      <th>job_desc_272</th>\n",
       "      <th>job_desc_273</th>\n",
       "      <th>job_desc_274</th>\n",
       "      <th>job_desc_275</th>\n",
       "      <th>job_desc_276</th>\n",
       "      <th>job_desc_277</th>\n",
       "      <th>job_desc_278</th>\n",
       "      <th>job_desc_279</th>\n",
       "      <th>job_desc_280</th>\n",
       "      <th>job_desc_281</th>\n",
       "      <th>job_desc_282</th>\n",
       "      <th>job_desc_283</th>\n",
       "      <th>job_desc_284</th>\n",
       "      <th>job_desc_285</th>\n",
       "      <th>job_desc_286</th>\n",
       "      <th>job_desc_287</th>\n",
       "      <th>job_desc_288</th>\n",
       "      <th>job_desc_289</th>\n",
       "      <th>job_desc_290</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265933</td>\n",
       "      <td>2.070912</td>\n",
       "      <td>-0.646306</td>\n",
       "      <td>-0.318699</td>\n",
       "      <td>-0.181419</td>\n",
       "      <td>-0.406402</td>\n",
       "      <td>-0.555239</td>\n",
       "      <td>-0.625735</td>\n",
       "      <td>-0.488355</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>-0.602128</td>\n",
       "      <td>-0.581684</td>\n",
       "      <td>0.518630</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>-0.271015</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.236189</td>\n",
       "      <td>-0.152509</td>\n",
       "      <td>0.564038</td>\n",
       "      <td>-0.175079</td>\n",
       "      <td>3.536066</td>\n",
       "      <td>0.403862</td>\n",
       "      <td>0.416160</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.549940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583465</td>\n",
       "      <td>0.513510</td>\n",
       "      <td>-0.056576</td>\n",
       "      <td>-0.912723</td>\n",
       "      <td>0.052374</td>\n",
       "      <td>0.419195</td>\n",
       "      <td>-0.223077</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>-0.279963</td>\n",
       "      <td>-0.155815</td>\n",
       "      <td>-0.598799</td>\n",
       "      <td>-0.124844</td>\n",
       "      <td>0.193408</td>\n",
       "      <td>-0.324402</td>\n",
       "      <td>0.556537</td>\n",
       "      <td>-0.214403</td>\n",
       "      <td>-0.361113</td>\n",
       "      <td>0.395566</td>\n",
       "      <td>-0.100708</td>\n",
       "      <td>0.081053</td>\n",
       "      <td>-0.291276</td>\n",
       "      <td>-0.084332</td>\n",
       "      <td>0.614833</td>\n",
       "      <td>0.341948</td>\n",
       "      <td>-0.535607</td>\n",
       "      <td>-0.385840</td>\n",
       "      <td>-0.036796</td>\n",
       "      <td>-0.100846</td>\n",
       "      <td>-0.423849</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>-0.054078</td>\n",
       "      <td>-0.573635</td>\n",
       "      <td>-0.306883</td>\n",
       "      <td>-0.325092</td>\n",
       "      <td>0.089463</td>\n",
       "      <td>-0.353476</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.667958</td>\n",
       "      <td>-0.702116</td>\n",
       "      <td>-0.206267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/08</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134647</td>\n",
       "      <td>2.323485</td>\n",
       "      <td>-0.545002</td>\n",
       "      <td>-0.398136</td>\n",
       "      <td>-0.219534</td>\n",
       "      <td>-0.593899</td>\n",
       "      <td>-0.934010</td>\n",
       "      <td>-0.347509</td>\n",
       "      <td>-0.150662</td>\n",
       "      <td>-0.025937</td>\n",
       "      <td>-0.788193</td>\n",
       "      <td>-0.758666</td>\n",
       "      <td>0.395563</td>\n",
       "      <td>-0.069356</td>\n",
       "      <td>-0.188280</td>\n",
       "      <td>-0.038201</td>\n",
       "      <td>0.553241</td>\n",
       "      <td>-0.329062</td>\n",
       "      <td>0.405169</td>\n",
       "      <td>0.280156</td>\n",
       "      <td>3.380567</td>\n",
       "      <td>0.446040</td>\n",
       "      <td>0.106416</td>\n",
       "      <td>-0.187433</td>\n",
       "      <td>-0.295211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381289</td>\n",
       "      <td>0.264247</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>-1.098606</td>\n",
       "      <td>-0.149598</td>\n",
       "      <td>0.528904</td>\n",
       "      <td>0.063392</td>\n",
       "      <td>-0.291882</td>\n",
       "      <td>-0.414426</td>\n",
       "      <td>-0.078166</td>\n",
       "      <td>-0.276882</td>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.264712</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.525958</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.313780</td>\n",
       "      <td>0.454644</td>\n",
       "      <td>-0.147099</td>\n",
       "      <td>0.385120</td>\n",
       "      <td>-0.784835</td>\n",
       "      <td>-0.293819</td>\n",
       "      <td>0.245624</td>\n",
       "      <td>0.277302</td>\n",
       "      <td>-0.678327</td>\n",
       "      <td>-0.459686</td>\n",
       "      <td>-0.096691</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>-0.497901</td>\n",
       "      <td>-0.123180</td>\n",
       "      <td>-0.868718</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>-0.179036</td>\n",
       "      <td>-0.717763</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>-0.190448</td>\n",
       "      <td>-1.261702</td>\n",
       "      <td>-0.505897</td>\n",
       "      <td>0.082080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>2023/01</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.146202</td>\n",
       "      <td>2.149297</td>\n",
       "      <td>-0.473768</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.059767</td>\n",
       "      <td>-0.347324</td>\n",
       "      <td>-0.462137</td>\n",
       "      <td>-0.635334</td>\n",
       "      <td>-0.460823</td>\n",
       "      <td>0.114211</td>\n",
       "      <td>-0.196667</td>\n",
       "      <td>-0.666750</td>\n",
       "      <td>0.199481</td>\n",
       "      <td>-0.359138</td>\n",
       "      <td>-0.244184</td>\n",
       "      <td>-0.033460</td>\n",
       "      <td>0.378710</td>\n",
       "      <td>-0.369817</td>\n",
       "      <td>0.560494</td>\n",
       "      <td>0.048275</td>\n",
       "      <td>3.746783</td>\n",
       "      <td>0.286187</td>\n",
       "      <td>0.286217</td>\n",
       "      <td>0.398518</td>\n",
       "      <td>-0.366492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244125</td>\n",
       "      <td>0.176481</td>\n",
       "      <td>0.042843</td>\n",
       "      <td>-0.743970</td>\n",
       "      <td>-0.036478</td>\n",
       "      <td>0.152495</td>\n",
       "      <td>-0.037869</td>\n",
       "      <td>-0.367833</td>\n",
       "      <td>-0.422593</td>\n",
       "      <td>0.244620</td>\n",
       "      <td>-0.238463</td>\n",
       "      <td>-0.038709</td>\n",
       "      <td>0.382557</td>\n",
       "      <td>-0.725669</td>\n",
       "      <td>0.360670</td>\n",
       "      <td>-0.064571</td>\n",
       "      <td>-0.200544</td>\n",
       "      <td>0.297640</td>\n",
       "      <td>-0.207100</td>\n",
       "      <td>0.574868</td>\n",
       "      <td>-0.784014</td>\n",
       "      <td>-0.083340</td>\n",
       "      <td>0.543385</td>\n",
       "      <td>0.521625</td>\n",
       "      <td>-0.876975</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.028694</td>\n",
       "      <td>0.067733</td>\n",
       "      <td>-0.627439</td>\n",
       "      <td>-0.385466</td>\n",
       "      <td>-0.416109</td>\n",
       "      <td>-0.619822</td>\n",
       "      <td>-0.493653</td>\n",
       "      <td>-0.347556</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>-0.331212</td>\n",
       "      <td>-0.381348</td>\n",
       "      <td>-0.506540</td>\n",
       "      <td>-0.773561</td>\n",
       "      <td>-0.105221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278451</td>\n",
       "      <td>1.929512</td>\n",
       "      <td>-0.400708</td>\n",
       "      <td>-0.395092</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>-0.280901</td>\n",
       "      <td>-0.719048</td>\n",
       "      <td>-0.251112</td>\n",
       "      <td>-0.182981</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>-0.350532</td>\n",
       "      <td>-0.528154</td>\n",
       "      <td>0.531242</td>\n",
       "      <td>-0.328141</td>\n",
       "      <td>0.200239</td>\n",
       "      <td>0.136406</td>\n",
       "      <td>0.199327</td>\n",
       "      <td>-0.246777</td>\n",
       "      <td>0.501189</td>\n",
       "      <td>-0.470432</td>\n",
       "      <td>3.739622</td>\n",
       "      <td>-0.049199</td>\n",
       "      <td>0.276417</td>\n",
       "      <td>-0.023738</td>\n",
       "      <td>-0.441035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349094</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>-0.080796</td>\n",
       "      <td>-0.612868</td>\n",
       "      <td>-0.063049</td>\n",
       "      <td>0.588547</td>\n",
       "      <td>-0.198869</td>\n",
       "      <td>0.219284</td>\n",
       "      <td>-0.383540</td>\n",
       "      <td>-0.093432</td>\n",
       "      <td>-0.401378</td>\n",
       "      <td>-0.145946</td>\n",
       "      <td>0.446052</td>\n",
       "      <td>-0.765058</td>\n",
       "      <td>0.194471</td>\n",
       "      <td>-0.045353</td>\n",
       "      <td>-0.112149</td>\n",
       "      <td>0.289670</td>\n",
       "      <td>0.147988</td>\n",
       "      <td>0.422955</td>\n",
       "      <td>-0.687581</td>\n",
       "      <td>-0.015552</td>\n",
       "      <td>0.421841</td>\n",
       "      <td>0.290365</td>\n",
       "      <td>-0.965742</td>\n",
       "      <td>0.220519</td>\n",
       "      <td>-0.214108</td>\n",
       "      <td>0.211355</td>\n",
       "      <td>-0.609563</td>\n",
       "      <td>-0.238449</td>\n",
       "      <td>-0.297560</td>\n",
       "      <td>-0.481448</td>\n",
       "      <td>-0.497642</td>\n",
       "      <td>-0.254823</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>-0.362739</td>\n",
       "      <td>-0.102704</td>\n",
       "      <td>-0.491272</td>\n",
       "      <td>-0.808156</td>\n",
       "      <td>-0.048326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154716</td>\n",
       "      <td>2.221011</td>\n",
       "      <td>-0.493091</td>\n",
       "      <td>-0.319782</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>-0.265350</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.306911</td>\n",
       "      <td>-0.237550</td>\n",
       "      <td>0.454508</td>\n",
       "      <td>-0.776502</td>\n",
       "      <td>-0.492894</td>\n",
       "      <td>0.406902</td>\n",
       "      <td>-0.095398</td>\n",
       "      <td>-0.056578</td>\n",
       "      <td>-0.266051</td>\n",
       "      <td>0.043975</td>\n",
       "      <td>-0.335315</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>-0.352373</td>\n",
       "      <td>3.555421</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.208665</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>-0.584533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051410</td>\n",
       "      <td>0.517515</td>\n",
       "      <td>-0.173539</td>\n",
       "      <td>-0.700081</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>-0.037963</td>\n",
       "      <td>0.029592</td>\n",
       "      <td>-0.343201</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>-0.278971</td>\n",
       "      <td>0.151569</td>\n",
       "      <td>0.326661</td>\n",
       "      <td>-0.468884</td>\n",
       "      <td>0.576501</td>\n",
       "      <td>-0.213073</td>\n",
       "      <td>-0.532033</td>\n",
       "      <td>0.322577</td>\n",
       "      <td>-0.142936</td>\n",
       "      <td>0.231748</td>\n",
       "      <td>-0.622124</td>\n",
       "      <td>-0.180121</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.171529</td>\n",
       "      <td>-0.699755</td>\n",
       "      <td>-0.575953</td>\n",
       "      <td>0.068527</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-0.331506</td>\n",
       "      <td>-0.216373</td>\n",
       "      <td>-0.176458</td>\n",
       "      <td>-0.726473</td>\n",
       "      <td>-0.323976</td>\n",
       "      <td>-0.145825</td>\n",
       "      <td>-0.046866</td>\n",
       "      <td>-0.229873</td>\n",
       "      <td>-0.568318</td>\n",
       "      <td>-0.614605</td>\n",
       "      <td>-0.770506</td>\n",
       "      <td>0.142140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 315 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_posted_date  ... job_desc_299 job_desc_300\n",
       "0       Others         2024/06  ...    -0.702116    -0.206267\n",
       "1       Others         2024/08  ...    -0.505897     0.082080\n",
       "2       Others         2023/01  ...    -0.773561    -0.105221\n",
       "3  Job_Title_5         2024/06  ...    -0.808156    -0.048326\n",
       "4       Others         2024/05  ...    -0.770506     0.142140\n",
       "\n",
       "[5 rows x 315 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03042f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 3, 1, 2, 1, 3, 2, 2, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 3, 2, 1, 3, 3, 1, 1, 3, 3, 2, 1, 2, 3, 2, 3, 2, 2, 3, 1, 3, 1,\n",
       "       3, 2, 1, 3, 3, 3, 2, 1, 3, 2, 3, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 2, 1, 3, 3, 2, 2, 3, 3, 1, 1, 2, 1, 2, 1, 2, 2, 2, 3, 3,\n",
       "       1, 1, 3, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1, 3,\n",
       "       1, 3, 1, 1, 3, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1,\n",
       "       2, 2, 3, 1, 3, 1, 1, 1, 1, 3, 2, 2, 1, 1, 3, 1, 1, 2, 2, 1, 2, 3,\n",
       "       1, 3, 3, 3, 3, 1, 2, 1, 2, 1, 1, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2,\n",
       "       2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 2, 3, 2, 3, 2, 1, 1, 2, 1, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 2, 3, 2, 3, 1, 3, 1, 2, 2, 1, 3, 1, 2, 1, 3, 2, 2,\n",
       "       2, 3, 3, 2, 1, 3, 3, 1, 2, 2, 1, 1, 1, 2, 3, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 1, 2, 2, 1, 3, 1, 2, 1, 2, 1, 3, 3, 1, 3, 1, 1, 2, 1, 3, 1,\n",
       "       1, 1, 3, 1, 3, 3, 1, 1, 2, 2, 2, 2, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3,\n",
       "       3, 1, 1, 2, 2, 1, 1, 1, 3, 1, 3, 3, 2, 1, 3, 2, 3, 2, 1, 3, 1, 1,\n",
       "       3, 1, 1, 3, 2, 1, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1,\n",
       "       1, 2, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2, 3, 2, 1, 3, 1, 2, 1, 2, 3, 1,\n",
       "       2, 1, 3, 2, 2, 2, 3, 1, 1, 3, 2, 1, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1,\n",
       "       2, 1, 3, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 2, 1, 2, 1, 2, 3, 1, 3, 3,\n",
       "       2, 2, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 2, 2, 3, 1, 3, 3, 1, 1, 2, 2,\n",
       "       2, 3, 1, 3, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 3, 1, 3,\n",
       "       2, 1, 3, 1, 3, 3, 1, 2, 1, 1, 1, 1, 3, 2, 1, 3, 1, 3, 1, 1, 2, 1,\n",
       "       3, 1, 1, 1, 1, 2, 2, 3, 2, 1, 1, 1, 3, 2, 1, 3, 2, 1, 1, 1, 3, 3,\n",
       "       3, 3, 2, 2, 2, 2, 1, 3, 1, 1, 3, 2, 3, 2, 3, 1, 3, 1, 3, 2, 3, 3,\n",
       "       1, 3, 1, 3, 2, 1, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 2, 1,\n",
       "       3, 2, 3, 1, 2, 3, 2, 3, 2, 3, 1, 3, 3, 3, 1, 2, 3, 3, 2, 1, 1, 1,\n",
       "       3, 1, 1, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 1, 1, 2, 3, 2, 3, 3, 2, 3,\n",
       "       1, 1, 3, 1, 2, 1, 1, 3, 1, 3, 2, 2, 3, 1, 1, 3, 3, 1, 1, 2, 1, 1,\n",
       "       1, 3, 1, 2, 3, 2, 3, 1, 1, 3, 3, 3, 3, 1, 3, 2, 2, 1, 3, 1, 3, 2,\n",
       "       1, 3, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2,\n",
       "       2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 2, 2,\n",
       "       1, 3, 3, 3, 3, 2, 3, 2, 2, 3, 1, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1,\n",
       "       2, 2, 3, 3, 1, 1, 3, 1, 3, 1, 2, 2, 3, 1, 1, 2, 2, 1, 3, 2, 3, 2,\n",
       "       2, 2, 1, 3, 1, 3, 3, 3, 3, 2, 2, 2, 1, 2, 3, 3, 2, 1, 1, 3, 2, 1,\n",
       "       3, 3, 2, 1, 1, 1, 2, 3, 1, 1, 3, 1, 3, 3, 1, 2, 2, 2, 1, 1, 3, 3,\n",
       "       2, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 3, 2, 1, 3, 2, 3, 2, 1, 3, 2, 3,\n",
       "       3, 2, 3, 1, 1, 3, 1, 3, 1, 1, 2, 1, 2, 2, 3, 3, 2, 3, 1, 3, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 3, 2, 1, 3, 1, 3, 2, 3, 3, 3, 1,\n",
       "       1, 3, 2, 3, 3, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1,\n",
       "       1, 2, 2, 1, 3, 3, 3, 3, 1, 2, 1, 2, 2, 1, 2, 3, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y=model.predict(Test_X)\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5332794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Low', 'High', 'High', 'Medium', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Low', 'High', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'Low', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Mapping numeric classes to category labels\n",
    "label_map = {1: \"Low\", 2: \"Medium\", 3: \"High\"}\n",
    "mapped_preds = [label_map[val] for val in predicted_y]\n",
    "print(mapped_preds)\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": mapped_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"engineerr_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a27e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "sns.heatmap(X_train.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661420de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('float64'), dtype('int32')], dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(\n",
    "    [\n",
    "        (\"transformer\",column_transform)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1ddba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46477322, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.14717881, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.20977889, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.12256281, -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.0659181 , -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.0670786 , -0.3602    , -0.2236985 , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_trained_processed = pipeline.transform(X_train)\n",
    "X_trained_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 421)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trained_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbdca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254151e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ab36c",
   "metadata": {},
   "source": [
    "### he he he some help from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68203ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Subtract 1 from Y_train to make the labels zero-indexed\n",
    "Y_train_zero_indexed = Y_train - 1\n",
    "\n",
    "# Now apply to_categorical with the correct number of classes\n",
    "Y_train_one_hot = to_categorical(Y_train_zero_indexed, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5648",
   "metadata": {},
   "source": [
    "# Work with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9712e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,020</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_90 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │        \u001b[38;5;34m75,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │           \u001b[38;5;34m720\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m18,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_66 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m2,020\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_67 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,343</span> (380.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97,343\u001b[0m (380.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,743</span> (377.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m96,743\u001b[0m (377.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> (2.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m600\u001b[0m (2.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "\n",
    "model=Sequential(\n",
    "    [\n",
    "        Dense(180,activation=\"relu\",input_shape=[X_trained_processed.shape[1]]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(100,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "        \n",
    "        Dense(20,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(3,activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='accuracy', factor=0.3, patience=10, min_lr=1e-6\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy', patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='ai/ann_best_model.keras',      \n",
    "    monitor='accuracy',                 \n",
    "    mode='max',                        \n",
    "    save_best_only=True,                 \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfa89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(.01),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['accuracy', 'AUC', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca42be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9970 - Precision: 0.9694 - accuracy: 0.9684 - loss: 0.1027\n",
      "Epoch 1: accuracy improved from -inf to 0.95996, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - AUC: 0.9969 - Precision: 0.9692 - accuracy: 0.9681 - loss: 0.1037 - val_AUC: 0.8435 - val_Precision: 0.6988 - val_accuracy: 0.6914 - val_loss: 1.3987 - learning_rate: 0.0100\n",
      "Epoch 2/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9940 - Precision: 0.9476 - accuracy: 0.9447 - loss: 0.1462\n",
      "Epoch 2: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9938 - Precision: 0.9478 - accuracy: 0.9443 - loss: 0.1483 - val_AUC: 0.8564 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.1937 - learning_rate: 0.0100\n",
      "Epoch 3/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9948 - Precision: 0.9534 - accuracy: 0.9490 - loss: 0.1312\n",
      "Epoch 3: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9945 - Precision: 0.9539 - accuracy: 0.9487 - loss: 0.1320 - val_AUC: 0.8554 - val_Precision: 0.7211 - val_accuracy: 0.7188 - val_loss: 1.2396 - learning_rate: 0.0100\n",
      "Epoch 4/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9940 - Precision: 0.9612 - accuracy: 0.9500 - loss: 0.1425\n",
      "Epoch 4: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9934 - Precision: 0.9607 - accuracy: 0.9501 - loss: 0.1463 - val_AUC: 0.8437 - val_Precision: 0.7120 - val_accuracy: 0.7148 - val_loss: 1.3139 - learning_rate: 0.0100\n",
      "Epoch 5/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9929 - Precision: 0.9561 - accuracy: 0.9424 - loss: 0.1490\n",
      "Epoch 5: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9930 - Precision: 0.9561 - accuracy: 0.9426 - loss: 0.1485 - val_AUC: 0.8564 - val_Precision: 0.6996 - val_accuracy: 0.6953 - val_loss: 1.2571 - learning_rate: 0.0100\n",
      "Epoch 6/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9945 - Precision: 0.9411 - accuracy: 0.9381 - loss: 0.1415\n",
      "Epoch 6: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9940 - Precision: 0.9418 - accuracy: 0.9394 - loss: 0.1452 - val_AUC: 0.8345 - val_Precision: 0.6908 - val_accuracy: 0.6914 - val_loss: 1.3936 - learning_rate: 0.0100\n",
      "Epoch 7/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9947 - Precision: 0.9624 - accuracy: 0.9522 - loss: 0.1424\n",
      "Epoch 7: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9947 - Precision: 0.9623 - accuracy: 0.9521 - loss: 0.1422 - val_AUC: 0.8295 - val_Precision: 0.6996 - val_accuracy: 0.7031 - val_loss: 1.4193 - learning_rate: 0.0100\n",
      "Epoch 8/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9926 - Precision: 0.9567 - accuracy: 0.9523 - loss: 0.1439\n",
      "Epoch 8: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9927 - Precision: 0.9564 - accuracy: 0.9522 - loss: 0.1442 - val_AUC: 0.8517 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.3213 - learning_rate: 0.0100\n",
      "Epoch 9/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9924 - Precision: 0.9483 - accuracy: 0.9424 - loss: 0.1514\n",
      "Epoch 9: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9925 - Precision: 0.9477 - accuracy: 0.9424 - loss: 0.1529 - val_AUC: 0.8512 - val_Precision: 0.7222 - val_accuracy: 0.7188 - val_loss: 1.2713 - learning_rate: 0.0100\n",
      "Epoch 10/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9967 - Precision: 0.9691 - accuracy: 0.9659 - loss: 0.1023\n",
      "Epoch 10: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9961 - Precision: 0.9672 - accuracy: 0.9637 - loss: 0.1095 - val_AUC: 0.8488 - val_Precision: 0.7441 - val_accuracy: 0.7383 - val_loss: 1.2898 - learning_rate: 0.0100\n",
      "Epoch 11/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9941 - Precision: 0.9561 - accuracy: 0.9535 - loss: 0.1315\n",
      "Epoch 11: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9943 - Precision: 0.9566 - accuracy: 0.9541 - loss: 0.1292 - val_AUC: 0.8452 - val_Precision: 0.7244 - val_accuracy: 0.7188 - val_loss: 1.3606 - learning_rate: 0.0100\n",
      "Epoch 12/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9919 - Precision: 0.9585 - accuracy: 0.9489 - loss: 0.1567\n",
      "Epoch 12: accuracy did not improve from 0.95996\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9927 - Precision: 0.9591 - accuracy: 0.9505 - loss: 0.1488 - val_AUC: 0.8508 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.3921 - learning_rate: 0.0030\n",
      "Epoch 13/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9960 - Precision: 0.9709 - accuracy: 0.9645 - loss: 0.0897\n",
      "Epoch 13: accuracy improved from 0.95996 to 0.96191, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9960 - Precision: 0.9692 - accuracy: 0.9637 - loss: 0.0945 - val_AUC: 0.8462 - val_Precision: 0.7331 - val_accuracy: 0.7305 - val_loss: 1.3244 - learning_rate: 0.0030\n",
      "Epoch 14/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9960 - Precision: 0.9678 - accuracy: 0.9610 - loss: 0.1022\n",
      "Epoch 14: accuracy improved from 0.96191 to 0.96484, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9962 - Precision: 0.9686 - accuracy: 0.9617 - loss: 0.1011 - val_AUC: 0.8498 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.3150 - learning_rate: 0.0030\n",
      "Epoch 15/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9959 - Precision: 0.9511 - accuracy: 0.9501 - loss: 0.1264\n",
      "Epoch 15: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9959 - Precision: 0.9513 - accuracy: 0.9503 - loss: 0.1261 - val_AUC: 0.8493 - val_Precision: 0.7194 - val_accuracy: 0.7148 - val_loss: 1.3294 - learning_rate: 0.0030\n",
      "Epoch 16/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9973 - Precision: 0.9606 - accuracy: 0.9584 - loss: 0.1012\n",
      "Epoch 16: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9970 - Precision: 0.9604 - accuracy: 0.9585 - loss: 0.1041 - val_AUC: 0.8486 - val_Precision: 0.7331 - val_accuracy: 0.7305 - val_loss: 1.3227 - learning_rate: 0.0030\n",
      "Epoch 17/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9944 - Precision: 0.9556 - accuracy: 0.9519 - loss: 0.1289\n",
      "Epoch 17: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9945 - Precision: 0.9559 - accuracy: 0.9523 - loss: 0.1280 - val_AUC: 0.8499 - val_Precision: 0.7165 - val_accuracy: 0.7148 - val_loss: 1.4177 - learning_rate: 0.0030\n",
      "Epoch 18/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9972 - Precision: 0.9707 - accuracy: 0.9664 - loss: 0.0892\n",
      "Epoch 18: accuracy did not improve from 0.96484\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9972 - Precision: 0.9712 - accuracy: 0.9669 - loss: 0.0888 - val_AUC: 0.8485 - val_Precision: 0.7189 - val_accuracy: 0.7109 - val_loss: 1.3645 - learning_rate: 0.0030\n",
      "Epoch 19/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9763 - accuracy: 0.9745 - loss: 0.0702\n",
      "Epoch 19: accuracy improved from 0.96484 to 0.97559, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9980 - Precision: 0.9765 - accuracy: 0.9744 - loss: 0.0705 - val_AUC: 0.8408 - val_Precision: 0.7143 - val_accuracy: 0.7109 - val_loss: 1.3718 - learning_rate: 0.0030\n",
      "Epoch 20/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9722 - accuracy: 0.9663 - loss: 0.0880\n",
      "Epoch 20: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9981 - Precision: 0.9734 - accuracy: 0.9678 - loss: 0.0862 - val_AUC: 0.8436 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 1.3972 - learning_rate: 0.0030\n",
      "Epoch 21/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9886 - accuracy: 0.9846 - loss: 0.0480\n",
      "Epoch 21: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9873 - accuracy: 0.9833 - loss: 0.0512 - val_AUC: 0.8444 - val_Precision: 0.7137 - val_accuracy: 0.7109 - val_loss: 1.5647 - learning_rate: 0.0030\n",
      "Epoch 22/200\n",
      "\u001b[1m55/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9965 - Precision: 0.9688 - accuracy: 0.9677 - loss: 0.0976\n",
      "Epoch 22: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9963 - Precision: 0.9684 - accuracy: 0.9672 - loss: 0.0991 - val_AUC: 0.8403 - val_Precision: 0.7480 - val_accuracy: 0.7383 - val_loss: 1.5427 - learning_rate: 0.0030\n",
      "Epoch 23/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9982 - Precision: 0.9783 - accuracy: 0.9777 - loss: 0.0850\n",
      "Epoch 23: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9982 - Precision: 0.9781 - accuracy: 0.9774 - loss: 0.0852 - val_AUC: 0.8410 - val_Precision: 0.7323 - val_accuracy: 0.7266 - val_loss: 1.5238 - learning_rate: 0.0030\n",
      "Epoch 24/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9824 - accuracy: 0.9782 - loss: 0.0721\n",
      "Epoch 24: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9817 - accuracy: 0.9775 - loss: 0.0733 - val_AUC: 0.8445 - val_Precision: 0.7087 - val_accuracy: 0.7070 - val_loss: 1.4916 - learning_rate: 0.0030\n",
      "Epoch 25/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9973 - Precision: 0.9620 - accuracy: 0.9588 - loss: 0.1012\n",
      "Epoch 25: accuracy did not improve from 0.97559\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9974 - Precision: 0.9625 - accuracy: 0.9595 - loss: 0.1009 - val_AUC: 0.8454 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 1.4946 - learning_rate: 0.0030\n",
      "Epoch 26/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9983 - Precision: 0.9723 - accuracy: 0.9716 - loss: 0.0752\n",
      "Epoch 26: accuracy improved from 0.97559 to 0.97852, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9983 - Precision: 0.9730 - accuracy: 0.9722 - loss: 0.0752 - val_AUC: 0.8462 - val_Precision: 0.7273 - val_accuracy: 0.7188 - val_loss: 1.5027 - learning_rate: 0.0030\n",
      "Epoch 27/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9959 - Precision: 0.9685 - accuracy: 0.9656 - loss: 0.1117\n",
      "Epoch 27: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9960 - Precision: 0.9688 - accuracy: 0.9657 - loss: 0.1108 - val_AUC: 0.8471 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4547 - learning_rate: 0.0030\n",
      "Epoch 28/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9727 - accuracy: 0.9698 - loss: 0.0872\n",
      "Epoch 28: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9979 - Precision: 0.9727 - accuracy: 0.9698 - loss: 0.0871 - val_AUC: 0.8483 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4406 - learning_rate: 0.0030\n",
      "Epoch 29/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9986 - Precision: 0.9714 - accuracy: 0.9715 - loss: 0.0687\n",
      "Epoch 29: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9720 - accuracy: 0.9720 - loss: 0.0683 - val_AUC: 0.8458 - val_Precision: 0.7373 - val_accuracy: 0.7383 - val_loss: 1.4644 - learning_rate: 0.0030\n",
      "Epoch 30/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9979 - Precision: 0.9798 - accuracy: 0.9782 - loss: 0.0773\n",
      "Epoch 30: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9977 - Precision: 0.9790 - accuracy: 0.9771 - loss: 0.0792 - val_AUC: 0.8479 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 1.4558 - learning_rate: 0.0030\n",
      "Epoch 31/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9974 - Precision: 0.9690 - accuracy: 0.9633 - loss: 0.0975\n",
      "Epoch 31: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9975 - Precision: 0.9700 - accuracy: 0.9646 - loss: 0.0955 - val_AUC: 0.8438 - val_Precision: 0.7165 - val_accuracy: 0.7109 - val_loss: 1.4669 - learning_rate: 0.0030\n",
      "Epoch 32/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9778 - accuracy: 0.9726 - loss: 0.0856\n",
      "Epoch 32: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9978 - Precision: 0.9770 - accuracy: 0.9719 - loss: 0.0859 - val_AUC: 0.8495 - val_Precision: 0.7216 - val_accuracy: 0.7227 - val_loss: 1.4690 - learning_rate: 0.0030\n",
      "Epoch 33/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9957 - Precision: 0.9796 - accuracy: 0.9796 - loss: 0.0945\n",
      "Epoch 33: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9957 - Precision: 0.9794 - accuracy: 0.9795 - loss: 0.0946 - val_AUC: 0.8402 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.4435 - learning_rate: 0.0030\n",
      "Epoch 34/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9986 - Precision: 0.9778 - accuracy: 0.9719 - loss: 0.0739\n",
      "Epoch 34: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9789 - accuracy: 0.9726 - loss: 0.0745 - val_AUC: 0.8350 - val_Precision: 0.7302 - val_accuracy: 0.7305 - val_loss: 1.4336 - learning_rate: 0.0030\n",
      "Epoch 35/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9749 - accuracy: 0.9723 - loss: 0.0734\n",
      "Epoch 35: accuracy did not improve from 0.97852\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9744 - accuracy: 0.9717 - loss: 0.0745 - val_AUC: 0.8388 - val_Precision: 0.7302 - val_accuracy: 0.7188 - val_loss: 1.5087 - learning_rate: 0.0030\n",
      "Epoch 36/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9895 - Precision: 0.9866 - accuracy: 0.9822 - loss: 0.1084\n",
      "Epoch 36: accuracy improved from 0.97852 to 0.98145, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9905 - Precision: 0.9868 - accuracy: 0.9820 - loss: 0.1035 - val_AUC: 0.8375 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.5180 - learning_rate: 0.0030\n",
      "Epoch 37/200\n",
      "\u001b[1m48/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9993 - Precision: 0.9831 - accuracy: 0.9805 - loss: 0.0542\n",
      "Epoch 37: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9993 - Precision: 0.9824 - accuracy: 0.9798 - loss: 0.0563 - val_AUC: 0.8400 - val_Precision: 0.7160 - val_accuracy: 0.6992 - val_loss: 1.5219 - learning_rate: 0.0030\n",
      "Epoch 38/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9981 - Precision: 0.9757 - accuracy: 0.9666 - loss: 0.0832\n",
      "Epoch 38: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9981 - Precision: 0.9765 - accuracy: 0.9680 - loss: 0.0818 - val_AUC: 0.8392 - val_Precision: 0.7176 - val_accuracy: 0.7148 - val_loss: 1.4776 - learning_rate: 0.0030\n",
      "Epoch 39/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9691 - loss: 0.0659\n",
      "Epoch 39: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9691 - loss: 0.0663 - val_AUC: 0.8444 - val_Precision: 0.7255 - val_accuracy: 0.7227 - val_loss: 1.4939 - learning_rate: 0.0030\n",
      "Epoch 40/200\n",
      "\u001b[1m55/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9970 - Precision: 0.9745 - accuracy: 0.9705 - loss: 0.0958\n",
      "Epoch 40: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9971 - Precision: 0.9752 - accuracy: 0.9715 - loss: 0.0939 - val_AUC: 0.8460 - val_Precision: 0.7233 - val_accuracy: 0.7188 - val_loss: 1.4743 - learning_rate: 0.0030\n",
      "Epoch 41/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9924 - Precision: 0.9454 - accuracy: 0.9453 - loss: 0.1386\n",
      "Epoch 41: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9934 - Precision: 0.9497 - accuracy: 0.9494 - loss: 0.1283 - val_AUC: 0.8427 - val_Precision: 0.7262 - val_accuracy: 0.7227 - val_loss: 1.4669 - learning_rate: 0.0030\n",
      "Epoch 42/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9963 - Precision: 0.9736 - accuracy: 0.9719 - loss: 0.0952\n",
      "Epoch 42: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9965 - Precision: 0.9740 - accuracy: 0.9721 - loss: 0.0927 - val_AUC: 0.8473 - val_Precision: 0.7154 - val_accuracy: 0.7109 - val_loss: 1.5719 - learning_rate: 0.0030\n",
      "Epoch 43/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9989 - Precision: 0.9797 - accuracy: 0.9764 - loss: 0.0618\n",
      "Epoch 43: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9989 - Precision: 0.9786 - accuracy: 0.9753 - loss: 0.0638 - val_AUC: 0.8494 - val_Precision: 0.7171 - val_accuracy: 0.7109 - val_loss: 1.4894 - learning_rate: 0.0030\n",
      "Epoch 44/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9676 - accuracy: 0.9669 - loss: 0.0837\n",
      "Epoch 44: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9980 - Precision: 0.9696 - accuracy: 0.9686 - loss: 0.0796 - val_AUC: 0.8517 - val_Precision: 0.7312 - val_accuracy: 0.7266 - val_loss: 1.4830 - learning_rate: 0.0030\n",
      "Epoch 45/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9656 - accuracy: 0.9633 - loss: 0.0938\n",
      "Epoch 45: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9981 - Precision: 0.9672 - accuracy: 0.9645 - loss: 0.0903 - val_AUC: 0.8498 - val_Precision: 0.7371 - val_accuracy: 0.7227 - val_loss: 1.4388 - learning_rate: 0.0030\n",
      "Epoch 46/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9713 - accuracy: 0.9693 - loss: 0.0848\n",
      "Epoch 46: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9976 - Precision: 0.9727 - accuracy: 0.9702 - loss: 0.0867 - val_AUC: 0.8513 - val_Precision: 0.7312 - val_accuracy: 0.7305 - val_loss: 1.4386 - learning_rate: 0.0030\n",
      "Epoch 47/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9985 - Precision: 0.9796 - accuracy: 0.9755 - loss: 0.0706\n",
      "Epoch 47: accuracy did not improve from 0.98145\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9796 - accuracy: 0.9759 - loss: 0.0695 - val_AUC: 0.8489 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 1.4314 - learning_rate: 9.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9984 - Precision: 0.9867 - accuracy: 0.9853 - loss: 0.0539\n",
      "Epoch 48: accuracy improved from 0.98145 to 0.98438, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9867 - accuracy: 0.9851 - loss: 0.0549 - val_AUC: 0.8505 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.4342 - learning_rate: 9.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9815 - accuracy: 0.9777 - loss: 0.0848\n",
      "Epoch 49: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9981 - Precision: 0.9812 - accuracy: 0.9776 - loss: 0.0828 - val_AUC: 0.8473 - val_Precision: 0.7410 - val_accuracy: 0.7383 - val_loss: 1.4123 - learning_rate: 9.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9978 - Precision: 0.9780 - accuracy: 0.9706 - loss: 0.0776\n",
      "Epoch 50: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9978 - Precision: 0.9776 - accuracy: 0.9705 - loss: 0.0775 - val_AUC: 0.8494 - val_Precision: 0.7402 - val_accuracy: 0.7383 - val_loss: 1.4216 - learning_rate: 9.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9969 - Precision: 0.9648 - accuracy: 0.9636 - loss: 0.0973\n",
      "Epoch 51: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9971 - Precision: 0.9661 - accuracy: 0.9648 - loss: 0.0946 - val_AUC: 0.8514 - val_Precision: 0.7431 - val_accuracy: 0.7383 - val_loss: 1.4191 - learning_rate: 9.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9987 - Precision: 0.9743 - accuracy: 0.9718 - loss: 0.0657\n",
      "Epoch 52: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9744 - accuracy: 0.9715 - loss: 0.0671 - val_AUC: 0.8513 - val_Precision: 0.7421 - val_accuracy: 0.7383 - val_loss: 1.4148 - learning_rate: 9.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9923 - accuracy: 0.9915 - loss: 0.0320\n",
      "Epoch 53: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9895 - accuracy: 0.9885 - loss: 0.0405 - val_AUC: 0.8485 - val_Precision: 0.7323 - val_accuracy: 0.7344 - val_loss: 1.3992 - learning_rate: 9.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m59/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9835 - accuracy: 0.9814 - loss: 0.0418\n",
      "Epoch 54: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9833 - accuracy: 0.9810 - loss: 0.0426 - val_AUC: 0.8460 - val_Precision: 0.7381 - val_accuracy: 0.7344 - val_loss: 1.4061 - learning_rate: 9.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9983 - Precision: 0.9757 - accuracy: 0.9686 - loss: 0.0810\n",
      "Epoch 55: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9757 - accuracy: 0.9689 - loss: 0.0807 - val_AUC: 0.8475 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 1.4241 - learning_rate: 9.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9980 - Precision: 0.9736 - accuracy: 0.9703 - loss: 0.0774\n",
      "Epoch 56: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9978 - Precision: 0.9731 - accuracy: 0.9701 - loss: 0.0787 - val_AUC: 0.8468 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 1.4177 - learning_rate: 9.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m62/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9874 - accuracy: 0.9835 - loss: 0.0549\n",
      "Epoch 57: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9871 - accuracy: 0.9832 - loss: 0.0555 - val_AUC: 0.8468 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4182 - learning_rate: 9.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9958 - Precision: 0.9746 - accuracy: 0.9727 - loss: 0.1071\n",
      "Epoch 58: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9959 - Precision: 0.9746 - accuracy: 0.9727 - loss: 0.1064 - val_AUC: 0.8489 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.4065 - learning_rate: 9.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9988 - Precision: 0.9742 - accuracy: 0.9720 - loss: 0.0641\n",
      "Epoch 59: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9745 - accuracy: 0.9720 - loss: 0.0675 - val_AUC: 0.8476 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4066 - learning_rate: 2.7000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9829 - accuracy: 0.9816 - loss: 0.0707\n",
      "Epoch 60: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9825 - accuracy: 0.9813 - loss: 0.0706 - val_AUC: 0.8478 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4065 - learning_rate: 2.7000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9844 - accuracy: 0.9799 - loss: 0.0433\n",
      "Epoch 61: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9842 - accuracy: 0.9797 - loss: 0.0441 - val_AUC: 0.8482 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4169 - learning_rate: 2.7000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9864 - accuracy: 0.9803 - loss: 0.0568\n",
      "Epoch 62: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9855 - accuracy: 0.9793 - loss: 0.0569 - val_AUC: 0.8455 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4086 - learning_rate: 2.7000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9986 - Precision: 0.9758 - accuracy: 0.9741 - loss: 0.0670\n",
      "Epoch 63: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9760 - accuracy: 0.9742 - loss: 0.0664 - val_AUC: 0.8474 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4164 - learning_rate: 2.7000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9979 - Precision: 0.9780 - accuracy: 0.9768 - loss: 0.0742\n",
      "Epoch 64: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9977 - Precision: 0.9779 - accuracy: 0.9767 - loss: 0.0752 - val_AUC: 0.8462 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4182 - learning_rate: 2.7000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9985 - Precision: 0.9742 - accuracy: 0.9719 - loss: 0.0780\n",
      "Epoch 65: accuracy did not improve from 0.98438\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9744 - accuracy: 0.9720 - loss: 0.0777 - val_AUC: 0.8457 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4152 - learning_rate: 2.7000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9997 - Precision: 0.9926 - accuracy: 0.9878 - loss: 0.0369\n",
      "Epoch 66: accuracy improved from 0.98438 to 0.98535, saving model to ai/ann_best_model.keras\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9996 - Precision: 0.9917 - accuracy: 0.9873 - loss: 0.0383 - val_AUC: 0.8436 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4137 - learning_rate: 2.7000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9806 - accuracy: 0.9776 - loss: 0.0564\n",
      "Epoch 67: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9824 - accuracy: 0.9794 - loss: 0.0526 - val_AUC: 0.8439 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4055 - learning_rate: 2.7000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9775 - accuracy: 0.9766 - loss: 0.0407\n",
      "Epoch 68: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9994 - Precision: 0.9787 - accuracy: 0.9775 - loss: 0.0424 - val_AUC: 0.8440 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4194 - learning_rate: 2.7000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9861 - accuracy: 0.9819 - loss: 0.0477\n",
      "Epoch 69: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9856 - accuracy: 0.9820 - loss: 0.0490 - val_AUC: 0.8438 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4394 - learning_rate: 2.7000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9927 - Precision: 0.9722 - accuracy: 0.9703 - loss: 0.1223\n",
      "Epoch 70: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9934 - Precision: 0.9729 - accuracy: 0.9707 - loss: 0.1161 - val_AUC: 0.8412 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4261 - learning_rate: 2.7000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9805 - accuracy: 0.9789 - loss: 0.0549\n",
      "Epoch 71: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9803 - accuracy: 0.9786 - loss: 0.0544 - val_AUC: 0.8428 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4400 - learning_rate: 2.7000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m57/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9901 - accuracy: 0.9891 - loss: 0.0374\n",
      "Epoch 72: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9893 - accuracy: 0.9882 - loss: 0.0393 - val_AUC: 0.8440 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4494 - learning_rate: 2.7000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m46/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9913 - Precision: 0.9559 - accuracy: 0.9543 - loss: 0.1164\n",
      "Epoch 73: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9931 - Precision: 0.9622 - accuracy: 0.9608 - loss: 0.1027 - val_AUC: 0.8444 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4408 - learning_rate: 2.7000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m62/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9887 - accuracy: 0.9884 - loss: 0.0404\n",
      "Epoch 74: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9996 - Precision: 0.9885 - accuracy: 0.9881 - loss: 0.0409 - val_AUC: 0.8444 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4499 - learning_rate: 2.7000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9991 - Precision: 0.9853 - accuracy: 0.9853 - loss: 0.0499\n",
      "Epoch 75: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9990 - Precision: 0.9842 - accuracy: 0.9841 - loss: 0.0524 - val_AUC: 0.8442 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.4357 - learning_rate: 2.7000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m52/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9866 - accuracy: 0.9842 - loss: 0.0483\n",
      "Epoch 76: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9857 - accuracy: 0.9830 - loss: 0.0505 - val_AUC: 0.8442 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4299 - learning_rate: 2.7000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9984 - Precision: 0.9832 - accuracy: 0.9785 - loss: 0.0746\n",
      "Epoch 77: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9983 - Precision: 0.9827 - accuracy: 0.9781 - loss: 0.0765 - val_AUC: 0.8443 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4397 - learning_rate: 8.1000e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9903 - accuracy: 0.9843 - loss: 0.0507\n",
      "Epoch 78: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9901 - accuracy: 0.9842 - loss: 0.0510 - val_AUC: 0.8443 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4398 - learning_rate: 8.1000e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9851 - accuracy: 0.9787 - loss: 0.0441\n",
      "Epoch 79: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9849 - accuracy: 0.9787 - loss: 0.0446 - val_AUC: 0.8445 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4293 - learning_rate: 8.1000e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m58/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9988 - Precision: 0.9728 - accuracy: 0.9730 - loss: 0.0650\n",
      "Epoch 80: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9739 - accuracy: 0.9740 - loss: 0.0634 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4364 - learning_rate: 8.1000e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m60/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9879 - accuracy: 0.9875 - loss: 0.0393\n",
      "Epoch 81: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9877 - accuracy: 0.9873 - loss: 0.0398 - val_AUC: 0.8452 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4351 - learning_rate: 8.1000e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m53/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9977 - Precision: 0.9856 - accuracy: 0.9844 - loss: 0.0594 \n",
      "Epoch 82: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9972 - Precision: 0.9838 - accuracy: 0.9825 - loss: 0.0657 - val_AUC: 0.8450 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4406 - learning_rate: 8.1000e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m47/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9793 - accuracy: 0.9791 - loss: 0.0558\n",
      "Epoch 83: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9784 - accuracy: 0.9779 - loss: 0.0574 - val_AUC: 0.8451 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4400 - learning_rate: 8.1000e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m49/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9996 - Precision: 0.9896 - accuracy: 0.9897 - loss: 0.0403\n",
      "Epoch 84: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9887 - accuracy: 0.9888 - loss: 0.0430 - val_AUC: 0.8449 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4378 - learning_rate: 8.1000e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9997 - Precision: 0.9909 - accuracy: 0.9883 - loss: 0.0347\n",
      "Epoch 85: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9995 - Precision: 0.9884 - accuracy: 0.9859 - loss: 0.0405 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4307 - learning_rate: 8.1000e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9971 - Precision: 0.9752 - accuracy: 0.9747 - loss: 0.0808\n",
      "Epoch 86: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9972 - Precision: 0.9756 - accuracy: 0.9750 - loss: 0.0786 - val_AUC: 0.8452 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4417 - learning_rate: 8.1000e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m54/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9994 - Precision: 0.9797 - accuracy: 0.9781 - loss: 0.0484\n",
      "Epoch 87: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9993 - Precision: 0.9794 - accuracy: 0.9776 - loss: 0.0510 - val_AUC: 0.8451 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.4415 - learning_rate: 2.4300e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m61/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9987 - Precision: 0.9835 - accuracy: 0.9785 - loss: 0.0533\n",
      "Epoch 88: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9835 - accuracy: 0.9785 - loss: 0.0535 - val_AUC: 0.8448 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4361 - learning_rate: 2.4300e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m50/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9995 - Precision: 0.9856 - accuracy: 0.9831 - loss: 0.0461\n",
      "Epoch 89: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9857 - accuracy: 0.9831 - loss: 0.0462 - val_AUC: 0.8448 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4369 - learning_rate: 2.4300e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m51/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9955 - Precision: 0.9772 - accuracy: 0.9716 - loss: 0.1073\n",
      "Epoch 90: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9958 - Precision: 0.9775 - accuracy: 0.9726 - loss: 0.1008 - val_AUC: 0.8446 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4474 - learning_rate: 2.4300e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9992 - Precision: 0.9813 - accuracy: 0.9813 - loss: 0.0509\n",
      "Epoch 91: accuracy did not improve from 0.98535\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9813 - accuracy: 0.9813 - loss: 0.0508 - val_AUC: 0.8445 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.4453 - learning_rate: 2.4300e-05\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_trained_processed,Y_train_one_hot,epochs=200,validation_split=0.2,batch_size=16,callbacks=[early_stopping,lr_scheduler,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7357e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X_encoded=pipeline.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0e4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(854, 421)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\".//ai/ann_best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(Test_X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcf1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9929667e-01, 7.0321001e-04, 1.6503252e-07],\n",
       "       [2.8722810e-02, 9.6408790e-01, 7.1892454e-03],\n",
       "       [9.9999905e-01, 9.2857380e-07, 6.4296866e-09],\n",
       "       ...,\n",
       "       [9.4172263e-01, 3.0688921e-02, 2.7588399e-02],\n",
       "       [8.6403525e-01, 8.4469788e-02, 5.1494971e-02],\n",
       "       [9.9113524e-01, 8.8646896e-03, 2.3500960e-13]], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the mapping based on index\n",
    "index_to_label = {0: 'High', 1: 'Medium', 2: 'Low'}\n",
    "\n",
    "# Get the index of the max probability for each row\n",
    "predicted_indices = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# Map the indices to labels\n",
    "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cafc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Low', 'High', 'High', 'Medium', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'Medium', 'High', 'High', 'Low', 'High', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'Medium', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'High', 'Low', 'High', 'High', 'High', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'Medium', 'Low', 'Low', 'High', 'Low', 'High', 'Medium', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'High', 'Low', 'Medium', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'High', 'High', 'High', 'Low', 'High', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'Low', 'High', 'Low', 'Medium', 'Medium', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'High', 'Low', 'High', 'High', 'Medium', 'High', 'Medium', 'High', 'High', 'High', 'Medium', 'High', 'Low', 'Medium', 'Medium', 'High', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'High', 'Low', 'High', 'High', 'High', 'High', 'Medium', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'High', 'Low', 'Medium', 'Medium', 'Medium', 'Low', 'Low', 'High', 'High', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Medium', 'Low', 'High', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'High', 'Medium', 'High', 'Low', 'High', 'Medium', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Medium', 'Low', 'High', 'Low', 'High', 'Medium', 'High', 'High', 'High', 'Low', 'Low', 'High', 'Medium', 'High', 'High', 'Low', 'High', 'Low', 'Medium', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Medium', 'Low', 'High', 'High', 'High', 'High', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Low', 'Medium', 'High', 'High', 'Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "\n",
    "print(mapped_preds)\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"new_engineerr_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb03c59",
   "metadata": {},
   "source": [
    "### Let's try PCA with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "708c8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.columns.tolist()\n",
    "job_features=[f for f in features if f.startswith(\"job_desc\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "41408656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_desc_001</th>\n",
       "      <th>job_desc_002</th>\n",
       "      <th>job_desc_003</th>\n",
       "      <th>job_desc_004</th>\n",
       "      <th>job_desc_005</th>\n",
       "      <th>job_desc_006</th>\n",
       "      <th>job_desc_007</th>\n",
       "      <th>job_desc_008</th>\n",
       "      <th>job_desc_009</th>\n",
       "      <th>job_desc_010</th>\n",
       "      <th>job_desc_011</th>\n",
       "      <th>job_desc_012</th>\n",
       "      <th>job_desc_013</th>\n",
       "      <th>job_desc_014</th>\n",
       "      <th>job_desc_015</th>\n",
       "      <th>job_desc_016</th>\n",
       "      <th>job_desc_017</th>\n",
       "      <th>job_desc_018</th>\n",
       "      <th>job_desc_019</th>\n",
       "      <th>job_desc_020</th>\n",
       "      <th>job_desc_021</th>\n",
       "      <th>job_desc_022</th>\n",
       "      <th>job_desc_023</th>\n",
       "      <th>job_desc_024</th>\n",
       "      <th>job_desc_025</th>\n",
       "      <th>job_desc_026</th>\n",
       "      <th>job_desc_027</th>\n",
       "      <th>job_desc_028</th>\n",
       "      <th>job_desc_029</th>\n",
       "      <th>job_desc_030</th>\n",
       "      <th>job_desc_031</th>\n",
       "      <th>job_desc_032</th>\n",
       "      <th>job_desc_033</th>\n",
       "      <th>job_desc_034</th>\n",
       "      <th>job_desc_035</th>\n",
       "      <th>job_desc_036</th>\n",
       "      <th>job_desc_037</th>\n",
       "      <th>job_desc_038</th>\n",
       "      <th>job_desc_039</th>\n",
       "      <th>job_desc_040</th>\n",
       "      <th>...</th>\n",
       "      <th>job_desc_261</th>\n",
       "      <th>job_desc_262</th>\n",
       "      <th>job_desc_263</th>\n",
       "      <th>job_desc_264</th>\n",
       "      <th>job_desc_265</th>\n",
       "      <th>job_desc_266</th>\n",
       "      <th>job_desc_267</th>\n",
       "      <th>job_desc_268</th>\n",
       "      <th>job_desc_269</th>\n",
       "      <th>job_desc_270</th>\n",
       "      <th>job_desc_271</th>\n",
       "      <th>job_desc_272</th>\n",
       "      <th>job_desc_273</th>\n",
       "      <th>job_desc_274</th>\n",
       "      <th>job_desc_275</th>\n",
       "      <th>job_desc_276</th>\n",
       "      <th>job_desc_277</th>\n",
       "      <th>job_desc_278</th>\n",
       "      <th>job_desc_279</th>\n",
       "      <th>job_desc_280</th>\n",
       "      <th>job_desc_281</th>\n",
       "      <th>job_desc_282</th>\n",
       "      <th>job_desc_283</th>\n",
       "      <th>job_desc_284</th>\n",
       "      <th>job_desc_285</th>\n",
       "      <th>job_desc_286</th>\n",
       "      <th>job_desc_287</th>\n",
       "      <th>job_desc_288</th>\n",
       "      <th>job_desc_289</th>\n",
       "      <th>job_desc_290</th>\n",
       "      <th>job_desc_291</th>\n",
       "      <th>job_desc_292</th>\n",
       "      <th>job_desc_293</th>\n",
       "      <th>job_desc_294</th>\n",
       "      <th>job_desc_295</th>\n",
       "      <th>job_desc_296</th>\n",
       "      <th>job_desc_297</th>\n",
       "      <th>job_desc_298</th>\n",
       "      <th>job_desc_299</th>\n",
       "      <th>job_desc_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.193511</td>\n",
       "      <td>2.275482</td>\n",
       "      <td>-0.440363</td>\n",
       "      <td>-0.327473</td>\n",
       "      <td>0.058464</td>\n",
       "      <td>-0.154043</td>\n",
       "      <td>-0.393158</td>\n",
       "      <td>-0.367905</td>\n",
       "      <td>-0.703665</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>0.058359</td>\n",
       "      <td>-0.632267</td>\n",
       "      <td>0.326872</td>\n",
       "      <td>-0.276131</td>\n",
       "      <td>0.098252</td>\n",
       "      <td>0.019475</td>\n",
       "      <td>0.105348</td>\n",
       "      <td>-0.411405</td>\n",
       "      <td>0.635027</td>\n",
       "      <td>-0.192049</td>\n",
       "      <td>3.849681</td>\n",
       "      <td>0.529550</td>\n",
       "      <td>0.269379</td>\n",
       "      <td>0.091730</td>\n",
       "      <td>-0.639549</td>\n",
       "      <td>0.047043</td>\n",
       "      <td>-0.159783</td>\n",
       "      <td>-0.322949</td>\n",
       "      <td>-0.492766</td>\n",
       "      <td>0.161221</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>-0.194594</td>\n",
       "      <td>0.395389</td>\n",
       "      <td>-0.287041</td>\n",
       "      <td>0.400163</td>\n",
       "      <td>-0.324728</td>\n",
       "      <td>-0.675304</td>\n",
       "      <td>0.013366</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.076885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111471</td>\n",
       "      <td>0.266141</td>\n",
       "      <td>-0.142156</td>\n",
       "      <td>-0.684483</td>\n",
       "      <td>-0.174529</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>-0.024953</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>-0.342473</td>\n",
       "      <td>0.185418</td>\n",
       "      <td>-0.233041</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>0.602155</td>\n",
       "      <td>-0.511326</td>\n",
       "      <td>0.602380</td>\n",
       "      <td>-0.254410</td>\n",
       "      <td>-0.114025</td>\n",
       "      <td>0.086060</td>\n",
       "      <td>-0.012974</td>\n",
       "      <td>0.523685</td>\n",
       "      <td>-0.864781</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>0.283154</td>\n",
       "      <td>-0.848735</td>\n",
       "      <td>-0.108635</td>\n",
       "      <td>0.049179</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>-0.535840</td>\n",
       "      <td>0.113221</td>\n",
       "      <td>-0.362079</td>\n",
       "      <td>-0.499308</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>-0.214881</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.271177</td>\n",
       "      <td>-0.113347</td>\n",
       "      <td>-0.587955</td>\n",
       "      <td>-0.919095</td>\n",
       "      <td>-0.207340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100152</td>\n",
       "      <td>2.291134</td>\n",
       "      <td>-0.356041</td>\n",
       "      <td>-0.494735</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>-0.356995</td>\n",
       "      <td>-0.633020</td>\n",
       "      <td>-0.444805</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>0.187210</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>-0.734535</td>\n",
       "      <td>0.264041</td>\n",
       "      <td>-0.209023</td>\n",
       "      <td>0.144627</td>\n",
       "      <td>-0.182167</td>\n",
       "      <td>0.144194</td>\n",
       "      <td>-0.436690</td>\n",
       "      <td>0.405126</td>\n",
       "      <td>-0.240945</td>\n",
       "      <td>3.781854</td>\n",
       "      <td>0.436835</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.118699</td>\n",
       "      <td>-0.336167</td>\n",
       "      <td>-0.148608</td>\n",
       "      <td>0.150601</td>\n",
       "      <td>-0.284540</td>\n",
       "      <td>-0.915190</td>\n",
       "      <td>-0.018915</td>\n",
       "      <td>0.126589</td>\n",
       "      <td>-0.231199</td>\n",
       "      <td>0.604595</td>\n",
       "      <td>-0.284158</td>\n",
       "      <td>0.078514</td>\n",
       "      <td>-0.208533</td>\n",
       "      <td>-0.529389</td>\n",
       "      <td>0.088278</td>\n",
       "      <td>0.041133</td>\n",
       "      <td>-0.037323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033901</td>\n",
       "      <td>0.475052</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>-0.412693</td>\n",
       "      <td>-0.424291</td>\n",
       "      <td>0.518947</td>\n",
       "      <td>-0.151527</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>-0.395344</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>-0.198469</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.390649</td>\n",
       "      <td>-0.484126</td>\n",
       "      <td>0.533130</td>\n",
       "      <td>-0.196634</td>\n",
       "      <td>-0.330162</td>\n",
       "      <td>0.200502</td>\n",
       "      <td>0.047225</td>\n",
       "      <td>0.263908</td>\n",
       "      <td>-0.817924</td>\n",
       "      <td>-0.069964</td>\n",
       "      <td>0.357457</td>\n",
       "      <td>0.323456</td>\n",
       "      <td>-0.977607</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-0.053379</td>\n",
       "      <td>-0.027340</td>\n",
       "      <td>-0.511633</td>\n",
       "      <td>-0.105435</td>\n",
       "      <td>-0.300989</td>\n",
       "      <td>-0.415411</td>\n",
       "      <td>-0.341824</td>\n",
       "      <td>-0.319064</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.124755</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>-0.893224</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.112364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406864</td>\n",
       "      <td>1.986625</td>\n",
       "      <td>-0.726046</td>\n",
       "      <td>-0.316294</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>-0.451118</td>\n",
       "      <td>-0.659871</td>\n",
       "      <td>-0.451544</td>\n",
       "      <td>-0.505597</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>-0.542267</td>\n",
       "      <td>-0.623306</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>-0.103152</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>0.236105</td>\n",
       "      <td>0.204340</td>\n",
       "      <td>-0.280865</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>-0.435184</td>\n",
       "      <td>3.603102</td>\n",
       "      <td>0.318330</td>\n",
       "      <td>0.206301</td>\n",
       "      <td>0.134952</td>\n",
       "      <td>-0.430970</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>0.406767</td>\n",
       "      <td>-0.445361</td>\n",
       "      <td>-0.713593</td>\n",
       "      <td>0.208482</td>\n",
       "      <td>-0.259031</td>\n",
       "      <td>-0.361557</td>\n",
       "      <td>0.511871</td>\n",
       "      <td>-0.378734</td>\n",
       "      <td>0.270862</td>\n",
       "      <td>0.134759</td>\n",
       "      <td>-0.581157</td>\n",
       "      <td>0.432804</td>\n",
       "      <td>-0.036824</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474084</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>-0.111558</td>\n",
       "      <td>-0.916092</td>\n",
       "      <td>-0.327722</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>0.074941</td>\n",
       "      <td>0.175365</td>\n",
       "      <td>-0.272249</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>-0.153464</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.295818</td>\n",
       "      <td>-0.174998</td>\n",
       "      <td>0.781811</td>\n",
       "      <td>-0.155236</td>\n",
       "      <td>-0.612957</td>\n",
       "      <td>0.254792</td>\n",
       "      <td>-0.063058</td>\n",
       "      <td>0.259242</td>\n",
       "      <td>-0.669112</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>0.200105</td>\n",
       "      <td>0.227985</td>\n",
       "      <td>-0.829408</td>\n",
       "      <td>-0.361299</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>-0.118158</td>\n",
       "      <td>-0.654845</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>-0.406159</td>\n",
       "      <td>-0.654657</td>\n",
       "      <td>-0.074398</td>\n",
       "      <td>-0.464479</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>-0.276270</td>\n",
       "      <td>-0.696853</td>\n",
       "      <td>-0.601466</td>\n",
       "      <td>0.089939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>0.247692</td>\n",
       "      <td>2.241664</td>\n",
       "      <td>-0.773778</td>\n",
       "      <td>-0.349046</td>\n",
       "      <td>-0.112922</td>\n",
       "      <td>-0.443660</td>\n",
       "      <td>-0.623765</td>\n",
       "      <td>-0.574480</td>\n",
       "      <td>-0.646541</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>-0.608944</td>\n",
       "      <td>-0.674689</td>\n",
       "      <td>0.141252</td>\n",
       "      <td>-0.477024</td>\n",
       "      <td>-0.348456</td>\n",
       "      <td>-0.120051</td>\n",
       "      <td>0.207638</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.117197</td>\n",
       "      <td>-0.292721</td>\n",
       "      <td>3.531815</td>\n",
       "      <td>0.345161</td>\n",
       "      <td>0.311558</td>\n",
       "      <td>-0.073906</td>\n",
       "      <td>-0.513094</td>\n",
       "      <td>0.109718</td>\n",
       "      <td>0.420074</td>\n",
       "      <td>-0.910997</td>\n",
       "      <td>-1.076696</td>\n",
       "      <td>0.101371</td>\n",
       "      <td>-0.250386</td>\n",
       "      <td>-0.451869</td>\n",
       "      <td>0.081838</td>\n",
       "      <td>-0.543334</td>\n",
       "      <td>0.150362</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>-0.842606</td>\n",
       "      <td>-0.102979</td>\n",
       "      <td>-0.207155</td>\n",
       "      <td>0.040521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258869</td>\n",
       "      <td>0.360034</td>\n",
       "      <td>-0.200701</td>\n",
       "      <td>-0.799798</td>\n",
       "      <td>-0.398149</td>\n",
       "      <td>0.306747</td>\n",
       "      <td>0.155311</td>\n",
       "      <td>0.164311</td>\n",
       "      <td>-0.198417</td>\n",
       "      <td>-0.426756</td>\n",
       "      <td>-0.255219</td>\n",
       "      <td>-0.174296</td>\n",
       "      <td>0.400743</td>\n",
       "      <td>-0.480432</td>\n",
       "      <td>0.679290</td>\n",
       "      <td>0.175697</td>\n",
       "      <td>-0.340478</td>\n",
       "      <td>0.430571</td>\n",
       "      <td>0.329472</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>-0.448444</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.316680</td>\n",
       "      <td>0.310137</td>\n",
       "      <td>-0.763874</td>\n",
       "      <td>-0.120096</td>\n",
       "      <td>0.167375</td>\n",
       "      <td>-0.401628</td>\n",
       "      <td>-0.434618</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>-0.089303</td>\n",
       "      <td>-0.471856</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.245694</td>\n",
       "      <td>0.251105</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>-0.184764</td>\n",
       "      <td>-0.482634</td>\n",
       "      <td>-0.819574</td>\n",
       "      <td>-0.241306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>-0.000119</td>\n",
       "      <td>2.386898</td>\n",
       "      <td>-0.568260</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>-0.176302</td>\n",
       "      <td>0.119118</td>\n",
       "      <td>-0.414387</td>\n",
       "      <td>-0.924835</td>\n",
       "      <td>-0.025357</td>\n",
       "      <td>0.178054</td>\n",
       "      <td>-0.441924</td>\n",
       "      <td>-0.684222</td>\n",
       "      <td>0.417051</td>\n",
       "      <td>-0.119762</td>\n",
       "      <td>0.342301</td>\n",
       "      <td>-0.517594</td>\n",
       "      <td>0.345503</td>\n",
       "      <td>-0.205205</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>0.139779</td>\n",
       "      <td>3.666357</td>\n",
       "      <td>-0.097009</td>\n",
       "      <td>0.599485</td>\n",
       "      <td>-0.192683</td>\n",
       "      <td>-0.561154</td>\n",
       "      <td>0.198849</td>\n",
       "      <td>-0.175415</td>\n",
       "      <td>-0.598443</td>\n",
       "      <td>-0.107163</td>\n",
       "      <td>-0.025763</td>\n",
       "      <td>-0.059220</td>\n",
       "      <td>-0.222539</td>\n",
       "      <td>0.152810</td>\n",
       "      <td>0.083113</td>\n",
       "      <td>-0.107035</td>\n",
       "      <td>-0.291498</td>\n",
       "      <td>-0.055485</td>\n",
       "      <td>0.343354</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>-0.125927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133942</td>\n",
       "      <td>0.192049</td>\n",
       "      <td>-0.241705</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.240993</td>\n",
       "      <td>-0.549458</td>\n",
       "      <td>-0.014106</td>\n",
       "      <td>-0.390905</td>\n",
       "      <td>-0.307105</td>\n",
       "      <td>0.192189</td>\n",
       "      <td>0.139593</td>\n",
       "      <td>-0.071225</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>-0.657543</td>\n",
       "      <td>0.623251</td>\n",
       "      <td>-0.420262</td>\n",
       "      <td>-0.578801</td>\n",
       "      <td>0.212982</td>\n",
       "      <td>-0.393071</td>\n",
       "      <td>0.313594</td>\n",
       "      <td>-0.320288</td>\n",
       "      <td>-0.315026</td>\n",
       "      <td>0.758135</td>\n",
       "      <td>0.155649</td>\n",
       "      <td>-0.873734</td>\n",
       "      <td>-0.305164</td>\n",
       "      <td>-0.365587</td>\n",
       "      <td>-0.214003</td>\n",
       "      <td>-0.433800</td>\n",
       "      <td>-0.040142</td>\n",
       "      <td>-0.079315</td>\n",
       "      <td>-0.796785</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>-0.462344</td>\n",
       "      <td>-0.053803</td>\n",
       "      <td>-0.297083</td>\n",
       "      <td>-0.277624</td>\n",
       "      <td>-0.924568</td>\n",
       "      <td>-0.897608</td>\n",
       "      <td>-0.236864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.020596</td>\n",
       "      <td>1.710889</td>\n",
       "      <td>-0.127225</td>\n",
       "      <td>0.094155</td>\n",
       "      <td>-0.230931</td>\n",
       "      <td>-0.094211</td>\n",
       "      <td>-0.464730</td>\n",
       "      <td>-0.415120</td>\n",
       "      <td>-0.250254</td>\n",
       "      <td>0.244051</td>\n",
       "      <td>-0.254329</td>\n",
       "      <td>-0.507631</td>\n",
       "      <td>0.550465</td>\n",
       "      <td>-0.464457</td>\n",
       "      <td>-0.121907</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.291175</td>\n",
       "      <td>-0.519736</td>\n",
       "      <td>0.394070</td>\n",
       "      <td>-0.202267</td>\n",
       "      <td>3.905797</td>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.391131</td>\n",
       "      <td>0.257897</td>\n",
       "      <td>-0.364532</td>\n",
       "      <td>0.067574</td>\n",
       "      <td>-0.331174</td>\n",
       "      <td>-0.054258</td>\n",
       "      <td>-0.239125</td>\n",
       "      <td>0.150287</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>-0.371555</td>\n",
       "      <td>-0.016493</td>\n",
       "      <td>0.105978</td>\n",
       "      <td>-0.098783</td>\n",
       "      <td>-0.242300</td>\n",
       "      <td>-0.532427</td>\n",
       "      <td>0.195698</td>\n",
       "      <td>0.365892</td>\n",
       "      <td>0.244592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120673</td>\n",
       "      <td>0.434760</td>\n",
       "      <td>-0.235444</td>\n",
       "      <td>-0.506747</td>\n",
       "      <td>0.056726</td>\n",
       "      <td>0.559161</td>\n",
       "      <td>-0.140767</td>\n",
       "      <td>-0.284269</td>\n",
       "      <td>-0.522720</td>\n",
       "      <td>-0.102024</td>\n",
       "      <td>-0.273132</td>\n",
       "      <td>0.162582</td>\n",
       "      <td>0.256406</td>\n",
       "      <td>-0.629145</td>\n",
       "      <td>0.296464</td>\n",
       "      <td>-0.180029</td>\n",
       "      <td>-0.076404</td>\n",
       "      <td>0.291573</td>\n",
       "      <td>-0.155750</td>\n",
       "      <td>0.201018</td>\n",
       "      <td>-0.546103</td>\n",
       "      <td>-0.157728</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>0.254010</td>\n",
       "      <td>-0.608748</td>\n",
       "      <td>0.074008</td>\n",
       "      <td>0.082367</td>\n",
       "      <td>-0.024769</td>\n",
       "      <td>-0.440734</td>\n",
       "      <td>0.158114</td>\n",
       "      <td>-0.030367</td>\n",
       "      <td>-0.329539</td>\n",
       "      <td>-0.429462</td>\n",
       "      <td>-0.141221</td>\n",
       "      <td>0.119336</td>\n",
       "      <td>-0.483549</td>\n",
       "      <td>-0.146601</td>\n",
       "      <td>-0.715821</td>\n",
       "      <td>-0.739832</td>\n",
       "      <td>-0.128655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0.385978</td>\n",
       "      <td>2.305014</td>\n",
       "      <td>-0.250134</td>\n",
       "      <td>0.010997</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>-0.393612</td>\n",
       "      <td>-0.502189</td>\n",
       "      <td>-0.598519</td>\n",
       "      <td>-0.664700</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>-0.892154</td>\n",
       "      <td>-1.014702</td>\n",
       "      <td>0.575211</td>\n",
       "      <td>-0.433060</td>\n",
       "      <td>0.173855</td>\n",
       "      <td>-0.284825</td>\n",
       "      <td>0.121565</td>\n",
       "      <td>-0.224954</td>\n",
       "      <td>0.409088</td>\n",
       "      <td>-0.195918</td>\n",
       "      <td>3.553530</td>\n",
       "      <td>0.066307</td>\n",
       "      <td>-0.236097</td>\n",
       "      <td>-0.203720</td>\n",
       "      <td>-0.382032</td>\n",
       "      <td>0.317824</td>\n",
       "      <td>0.128182</td>\n",
       "      <td>-0.378691</td>\n",
       "      <td>-0.157871</td>\n",
       "      <td>0.285039</td>\n",
       "      <td>0.244884</td>\n",
       "      <td>-0.106542</td>\n",
       "      <td>0.318417</td>\n",
       "      <td>-0.291888</td>\n",
       "      <td>0.168756</td>\n",
       "      <td>-0.286959</td>\n",
       "      <td>-0.729526</td>\n",
       "      <td>0.260447</td>\n",
       "      <td>0.210248</td>\n",
       "      <td>0.783564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334416</td>\n",
       "      <td>0.786392</td>\n",
       "      <td>-0.526524</td>\n",
       "      <td>-0.303474</td>\n",
       "      <td>-0.345900</td>\n",
       "      <td>0.604478</td>\n",
       "      <td>-0.057588</td>\n",
       "      <td>0.174257</td>\n",
       "      <td>-0.579230</td>\n",
       "      <td>0.045546</td>\n",
       "      <td>-0.366639</td>\n",
       "      <td>-0.222223</td>\n",
       "      <td>0.081380</td>\n",
       "      <td>-0.169502</td>\n",
       "      <td>0.584033</td>\n",
       "      <td>-0.224276</td>\n",
       "      <td>-0.285235</td>\n",
       "      <td>0.397047</td>\n",
       "      <td>-0.141591</td>\n",
       "      <td>0.413654</td>\n",
       "      <td>-0.673819</td>\n",
       "      <td>-0.387608</td>\n",
       "      <td>0.256822</td>\n",
       "      <td>0.142165</td>\n",
       "      <td>-0.689537</td>\n",
       "      <td>-0.110737</td>\n",
       "      <td>-0.260208</td>\n",
       "      <td>-0.196423</td>\n",
       "      <td>-0.511908</td>\n",
       "      <td>-0.446026</td>\n",
       "      <td>-0.213866</td>\n",
       "      <td>-0.242665</td>\n",
       "      <td>-0.311564</td>\n",
       "      <td>-0.032751</td>\n",
       "      <td>-0.301447</td>\n",
       "      <td>-0.480434</td>\n",
       "      <td>-0.112401</td>\n",
       "      <td>-0.828844</td>\n",
       "      <td>-1.066424</td>\n",
       "      <td>-0.228583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0.276502</td>\n",
       "      <td>2.362514</td>\n",
       "      <td>-0.311881</td>\n",
       "      <td>-0.314943</td>\n",
       "      <td>-0.120564</td>\n",
       "      <td>-0.346545</td>\n",
       "      <td>-0.538098</td>\n",
       "      <td>-0.561801</td>\n",
       "      <td>-0.507609</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>-0.416789</td>\n",
       "      <td>-0.537681</td>\n",
       "      <td>0.400847</td>\n",
       "      <td>-0.245076</td>\n",
       "      <td>-0.062089</td>\n",
       "      <td>0.411445</td>\n",
       "      <td>0.409933</td>\n",
       "      <td>-0.440644</td>\n",
       "      <td>0.442484</td>\n",
       "      <td>-0.232069</td>\n",
       "      <td>3.377306</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>-0.051288</td>\n",
       "      <td>-0.566371</td>\n",
       "      <td>0.022975</td>\n",
       "      <td>-0.043450</td>\n",
       "      <td>-0.220781</td>\n",
       "      <td>-0.555346</td>\n",
       "      <td>0.041727</td>\n",
       "      <td>0.188101</td>\n",
       "      <td>-0.164355</td>\n",
       "      <td>0.374461</td>\n",
       "      <td>-0.274176</td>\n",
       "      <td>0.128526</td>\n",
       "      <td>-0.358542</td>\n",
       "      <td>-0.614278</td>\n",
       "      <td>0.447641</td>\n",
       "      <td>0.063649</td>\n",
       "      <td>0.173885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.148066</td>\n",
       "      <td>0.063563</td>\n",
       "      <td>-0.494108</td>\n",
       "      <td>-0.137698</td>\n",
       "      <td>0.412805</td>\n",
       "      <td>0.147055</td>\n",
       "      <td>0.204958</td>\n",
       "      <td>-0.256772</td>\n",
       "      <td>-0.087136</td>\n",
       "      <td>-0.550882</td>\n",
       "      <td>-0.192647</td>\n",
       "      <td>0.271607</td>\n",
       "      <td>-0.186132</td>\n",
       "      <td>0.498985</td>\n",
       "      <td>-0.058023</td>\n",
       "      <td>-0.433207</td>\n",
       "      <td>0.377525</td>\n",
       "      <td>-0.113898</td>\n",
       "      <td>0.600979</td>\n",
       "      <td>-0.475422</td>\n",
       "      <td>-0.382745</td>\n",
       "      <td>0.561544</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>-0.631298</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>0.039341</td>\n",
       "      <td>0.096617</td>\n",
       "      <td>-0.224361</td>\n",
       "      <td>-0.064944</td>\n",
       "      <td>-0.181357</td>\n",
       "      <td>-0.721596</td>\n",
       "      <td>-0.324486</td>\n",
       "      <td>-0.084486</td>\n",
       "      <td>-0.100463</td>\n",
       "      <td>-0.329224</td>\n",
       "      <td>-0.369456</td>\n",
       "      <td>-0.872554</td>\n",
       "      <td>-0.829080</td>\n",
       "      <td>-0.081490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_desc_001  job_desc_002  ...  job_desc_299  job_desc_300\n",
       "0         0.193511      2.275482  ...     -0.919095     -0.207340\n",
       "1         0.100152      2.291134  ...     -0.823024      0.112364\n",
       "2         0.000000      0.000000  ...      0.000000      0.000000\n",
       "3         0.000000      0.000000  ...      0.000000      0.000000\n",
       "4         0.406864      1.986625  ...     -0.601466      0.089939\n",
       "...            ...           ...  ...           ...           ...\n",
       "1275      0.247692      2.241664  ...     -0.819574     -0.241306\n",
       "1276     -0.000119      2.386898  ...     -0.897608     -0.236864\n",
       "1277      0.020596      1.710889  ...     -0.739832     -0.128655\n",
       "1278      0.385978      2.305014  ...     -1.066424     -0.228583\n",
       "1279      0.276502      2.362514  ...     -0.829080     -0.081490\n",
       "\n",
       "[1280 rows x 300 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[job_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "da266200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.380112</td>\n",
       "      <td>0.469987</td>\n",
       "      <td>-0.075655</td>\n",
       "      <td>-1.038070</td>\n",
       "      <td>0.417364</td>\n",
       "      <td>-0.591890</td>\n",
       "      <td>-0.349069</td>\n",
       "      <td>0.128737</td>\n",
       "      <td>0.344107</td>\n",
       "      <td>0.286108</td>\n",
       "      <td>-0.300371</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>-0.380345</td>\n",
       "      <td>-0.385681</td>\n",
       "      <td>-0.437636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.210201</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>-0.126489</td>\n",
       "      <td>-1.323580</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>-0.470055</td>\n",
       "      <td>-0.958410</td>\n",
       "      <td>-0.599482</td>\n",
       "      <td>0.581677</td>\n",
       "      <td>-0.649324</td>\n",
       "      <td>0.075285</td>\n",
       "      <td>-0.215050</td>\n",
       "      <td>0.548066</td>\n",
       "      <td>-0.107075</td>\n",
       "      <td>-0.100766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.049791</td>\n",
       "      <td>-0.032132</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.049791</td>\n",
       "      <td>-0.032132</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.261816</td>\n",
       "      <td>-1.331539</td>\n",
       "      <td>-0.602937</td>\n",
       "      <td>-0.419042</td>\n",
       "      <td>0.312947</td>\n",
       "      <td>-0.456932</td>\n",
       "      <td>-0.397784</td>\n",
       "      <td>0.386785</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>0.020821</td>\n",
       "      <td>0.414080</td>\n",
       "      <td>0.514070</td>\n",
       "      <td>0.568281</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>0.082863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PCA_1     PCA_2     PCA_3  ...    PCA_13    PCA_14    PCA_15\n",
       "0  1.380112  0.469987 -0.075655  ... -0.380345 -0.385681 -0.437636\n",
       "1  1.210201  0.024486 -0.126489  ...  0.548066 -0.107075 -0.100766\n",
       "2 -7.344770 -0.170561  0.067727  ... -0.002547 -0.030970  0.002572\n",
       "3 -7.344770 -0.170561  0.067727  ... -0.002547 -0.030970  0.002572\n",
       "4  1.261816 -1.331539 -0.602937  ...  0.568281  0.036273  0.082863\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(15)\n",
    "new_job_features=pca.fit_transform(X_train[job_features])\n",
    "\n",
    "pca_df=pd.DataFrame(new_job_features,columns=[\"PCA_1\",\"PCA_2\",\"PCA_3\",\"PCA_4\",\"PCA_5\",\"PCA_6\",\"PCA_7\",\"PCA_8\",\"PCA_9\",\"PCA_10\",\"PCA_11\",\"PCA_12\",\"PCA_13\",\"PCA_14\",\"PCA_15\"])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "588dbbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.380112</td>\n",
       "      <td>0.469987</td>\n",
       "      <td>-0.075655</td>\n",
       "      <td>-1.038070</td>\n",
       "      <td>0.417364</td>\n",
       "      <td>-0.591890</td>\n",
       "      <td>-0.349069</td>\n",
       "      <td>0.128737</td>\n",
       "      <td>0.344107</td>\n",
       "      <td>0.286108</td>\n",
       "      <td>-0.300371</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>-0.380345</td>\n",
       "      <td>-0.385681</td>\n",
       "      <td>-0.437636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job_Title_1</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.210201</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>-0.126489</td>\n",
       "      <td>-1.323580</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>-0.470055</td>\n",
       "      <td>-0.958410</td>\n",
       "      <td>-0.599482</td>\n",
       "      <td>0.581677</td>\n",
       "      <td>-0.649324</td>\n",
       "      <td>0.075285</td>\n",
       "      <td>-0.215050</td>\n",
       "      <td>0.548066</td>\n",
       "      <td>-0.107075</td>\n",
       "      <td>-0.100766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.049791</td>\n",
       "      <td>-0.032132</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.344770</td>\n",
       "      <td>-0.170561</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>-0.030255</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.049791</td>\n",
       "      <td>-0.032132</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/07</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261816</td>\n",
       "      <td>-1.331539</td>\n",
       "      <td>-0.602937</td>\n",
       "      <td>-0.419042</td>\n",
       "      <td>0.312947</td>\n",
       "      <td>-0.456932</td>\n",
       "      <td>-0.397784</td>\n",
       "      <td>0.386785</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>0.020821</td>\n",
       "      <td>0.414080</td>\n",
       "      <td>0.514070</td>\n",
       "      <td>0.568281</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>0.082863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_posted_date job_state  ...    PCA_13    PCA_14    PCA_15\n",
       "0       Others         2024/07        NY  ... -0.380345 -0.385681 -0.437636\n",
       "1  Job_Title_1         2024/07        CA  ...  0.548066 -0.107075 -0.100766\n",
       "2       Others         2024/07        CA  ... -0.002547 -0.030970  0.002572\n",
       "3       Others         2024/07        CA  ... -0.002547 -0.030970  0.002572\n",
       "4       Others         2024/07        CA  ...  0.568281  0.036273  0.082863\n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca=X_train.drop(columns=job_features)\n",
    "X_train_pca=pd.concat([X_train_pca,pca_df],axis=1)\n",
    "X_train_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "efd60fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 numaric columns and 4 catagorical columns\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "numaric_columns=X_train_pca.select_dtypes(include=[\"int\",\"float64\"]).columns.tolist()\n",
    "catagory_columns=X_train_pca.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numaric_columns)} numaric columns and {len(catagory_columns)} catagorical columns\")\n",
    "num_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"mean\")),\n",
    "        (\"standard\",StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline=Pipeline(\n",
    "    [\n",
    "        (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"odrinal\",OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_transform=ColumnTransformer(\n",
    "    [\n",
    "        (\"numaric\",num_pipeline,numaric_columns),\n",
    "        (\"cat\",cat_pipeline,catagory_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3ea2ddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 136)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline=Pipeline(\n",
    "    [\n",
    "        (\"transformer\",column_transform)\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train_pca)\n",
    "\n",
    "X_trained_processed = pipeline.transform(X_train_pca)\n",
    "X_trained_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "55cbbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Subtract 1 from Y_train to make the labels zero-indexed\n",
    "Y_train_zero_indexed = Y_train - 1\n",
    "\n",
    "# Now apply to_categorical with the correct number of classes\n",
    "Y_train_one_hot = to_categorical(Y_train_zero_indexed, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21522b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_70          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_71          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_72          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_94 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │        \u001b[38;5;34m18,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_70          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │           \u001b[38;5;34m544\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_70 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m13,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_71          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_71 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_72          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_72 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_97 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,359</span> (134.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,359\u001b[0m (134.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,867</span> (132.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,867\u001b[0m (132.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">492</span> (1.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m492\u001b[0m (1.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "\n",
    "model=Sequential(\n",
    "    [\n",
    "        Dense(136,activation=\"relu\",input_shape=[X_trained_processed.shape[1]]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(100,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(10,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.2),\n",
    "\n",
    "        Dense(3,activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "322e6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.3, patience=10, min_lr=1e-6\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=35, restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='pca/ann_best_model.keras',      \n",
    "    monitor='val_accuracy',                 \n",
    "    mode='max',                        \n",
    "    save_best_only=True,                 \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "41acb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(.01),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['accuracy', 'AUC', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "766e029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history=model.fit(X_trained_processed,Y_train_one_hot,epochs=200,validation_split=0.2,batch_size=20,callbacks=[early_stopping,lr_scheduler,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67a495",
   "metadata": {},
   "source": [
    "# Trying Advanced Modified model strucure from ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "fecb64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_44\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_44\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_174 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_130         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_175 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_131         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_176 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_132         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_177 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_174 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m70,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_130         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_57 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_130 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_175 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_131         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_58 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_131 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_176 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_132         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_59 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_132 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_177 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">238,339</span> (931.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m238,339\u001b[0m (931.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">236,547</span> (924.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m236,547\u001b[0m (924.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.6655 - Precision: 0.5050 - accuracy: 0.4845 - loss: 1.2074\n",
      "Epoch 1: accuracy improved from -inf to 0.56543, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - AUC: 0.6796 - Precision: 0.5204 - accuracy: 0.4991 - loss: 1.1801 - val_AUC: 0.7079 - val_Precision: 0.5582 - val_accuracy: 0.5508 - val_loss: 2.1998 - learning_rate: 0.0100\n",
      "Epoch 2/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8218 - Precision: 0.6939 - accuracy: 0.6383 - loss: 0.8146\n",
      "Epoch 2: accuracy improved from 0.56543 to 0.67480, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.8232 - Precision: 0.6961 - accuracy: 0.6405 - loss: 0.8110 - val_AUC: 0.8228 - val_Precision: 0.6878 - val_accuracy: 0.6836 - val_loss: 1.0987 - learning_rate: 0.0100\n",
      "Epoch 3/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.8883 - Precision: 0.7886 - accuracy: 0.7461 - loss: 0.6461\n",
      "Epoch 3: accuracy improved from 0.67480 to 0.73828, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - AUC: 0.8886 - Precision: 0.7856 - accuracy: 0.7443 - loss: 0.6451 - val_AUC: 0.8240 - val_Precision: 0.6831 - val_accuracy: 0.6602 - val_loss: 1.1896 - learning_rate: 0.0100\n",
      "Epoch 4/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9167 - Precision: 0.8018 - accuracy: 0.7501 - loss: 0.5547\n",
      "Epoch 4: accuracy improved from 0.73828 to 0.75586, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9163 - Precision: 0.8013 - accuracy: 0.7507 - loss: 0.5567 - val_AUC: 0.8690 - val_Precision: 0.7089 - val_accuracy: 0.6992 - val_loss: 0.7673 - learning_rate: 0.0100\n",
      "Epoch 5/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9423 - Precision: 0.8443 - accuracy: 0.8328 - loss: 0.4627\n",
      "Epoch 5: accuracy improved from 0.75586 to 0.79688, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9414 - Precision: 0.8427 - accuracy: 0.8307 - loss: 0.4663 - val_AUC: 0.8592 - val_Precision: 0.7083 - val_accuracy: 0.7109 - val_loss: 0.8571 - learning_rate: 0.0100\n",
      "Epoch 6/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9384 - Precision: 0.8260 - accuracy: 0.8150 - loss: 0.4764\n",
      "Epoch 6: accuracy did not improve from 0.79688\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9364 - Precision: 0.8240 - accuracy: 0.8119 - loss: 0.4840 - val_AUC: 0.8590 - val_Precision: 0.7231 - val_accuracy: 0.7109 - val_loss: 0.8440 - learning_rate: 0.0100\n",
      "Epoch 7/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9443 - Precision: 0.8608 - accuracy: 0.8443 - loss: 0.4584\n",
      "Epoch 7: accuracy improved from 0.79688 to 0.82324, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9428 - Precision: 0.8548 - accuracy: 0.8348 - loss: 0.4632 - val_AUC: 0.8566 - val_Precision: 0.7438 - val_accuracy: 0.7148 - val_loss: 0.9702 - learning_rate: 0.0100\n",
      "Epoch 8/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9624 - Precision: 0.8754 - accuracy: 0.8628 - loss: 0.3716\n",
      "Epoch 8: accuracy improved from 0.82324 to 0.83984, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9618 - Precision: 0.8734 - accuracy: 0.8609 - loss: 0.3749 - val_AUC: 0.8587 - val_Precision: 0.7388 - val_accuracy: 0.7266 - val_loss: 0.9180 - learning_rate: 0.0100\n",
      "Epoch 9/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9560 - Precision: 0.8750 - accuracy: 0.8482 - loss: 0.3944\n",
      "Epoch 9: accuracy improved from 0.83984 to 0.85547, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9580 - Precision: 0.8749 - accuracy: 0.8507 - loss: 0.3863 - val_AUC: 0.8578 - val_Precision: 0.7063 - val_accuracy: 0.7031 - val_loss: 1.1063 - learning_rate: 0.0100\n",
      "Epoch 10/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9678 - Precision: 0.8919 - accuracy: 0.8653 - loss: 0.3355\n",
      "Epoch 10: accuracy did not improve from 0.85547\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9677 - Precision: 0.8914 - accuracy: 0.8650 - loss: 0.3362 - val_AUC: 0.8478 - val_Precision: 0.7165 - val_accuracy: 0.7148 - val_loss: 1.1155 - learning_rate: 0.0100\n",
      "Epoch 11/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9598 - Precision: 0.8855 - accuracy: 0.8765 - loss: 0.3882\n",
      "Epoch 11: accuracy improved from 0.85547 to 0.86523, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9599 - Precision: 0.8851 - accuracy: 0.8761 - loss: 0.3876 - val_AUC: 0.8681 - val_Precision: 0.7430 - val_accuracy: 0.7383 - val_loss: 0.9765 - learning_rate: 0.0100\n",
      "Epoch 12/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9828 - Precision: 0.9169 - accuracy: 0.9020 - loss: 0.2565\n",
      "Epoch 12: accuracy improved from 0.86523 to 0.88574, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9811 - Precision: 0.9129 - accuracy: 0.8974 - loss: 0.2660 - val_AUC: 0.8387 - val_Precision: 0.7040 - val_accuracy: 0.7031 - val_loss: 1.3229 - learning_rate: 0.0100\n",
      "Epoch 13/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9870 - Precision: 0.9368 - accuracy: 0.9251 - loss: 0.2146\n",
      "Epoch 13: accuracy improved from 0.88574 to 0.89160, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9844 - Precision: 0.9303 - accuracy: 0.9184 - loss: 0.2331 - val_AUC: 0.8742 - val_Precision: 0.7360 - val_accuracy: 0.7344 - val_loss: 0.9560 - learning_rate: 0.0100\n",
      "Epoch 14/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9758 - Precision: 0.9004 - accuracy: 0.8927 - loss: 0.2963\n",
      "Epoch 14: accuracy did not improve from 0.89160\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9755 - Precision: 0.8992 - accuracy: 0.8915 - loss: 0.2983 - val_AUC: 0.8429 - val_Precision: 0.7063 - val_accuracy: 0.7109 - val_loss: 1.2181 - learning_rate: 0.0100\n",
      "Epoch 15/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9791 - Precision: 0.9109 - accuracy: 0.8980 - loss: 0.2709\n",
      "Epoch 15: accuracy did not improve from 0.89160\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9791 - Precision: 0.9107 - accuracy: 0.8978 - loss: 0.2712 - val_AUC: 0.8494 - val_Precision: 0.7200 - val_accuracy: 0.7109 - val_loss: 1.2053 - learning_rate: 0.0100\n",
      "Epoch 16/250\n",
      "\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9899 - Precision: 0.9354 - accuracy: 0.9313 - loss: 0.1930\n",
      "Epoch 16: accuracy improved from 0.89160 to 0.90820, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9885 - Precision: 0.9284 - accuracy: 0.9243 - loss: 0.2034 - val_AUC: 0.8537 - val_Precision: 0.7200 - val_accuracy: 0.7070 - val_loss: 1.2330 - learning_rate: 0.0100\n",
      "Epoch 17/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9870 - Precision: 0.9305 - accuracy: 0.9225 - loss: 0.2141\n",
      "Epoch 17: accuracy did not improve from 0.90820\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9851 - Precision: 0.9254 - accuracy: 0.9180 - loss: 0.2276 - val_AUC: 0.8448 - val_Precision: 0.7211 - val_accuracy: 0.7148 - val_loss: 1.1647 - learning_rate: 0.0100\n",
      "Epoch 18/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9860 - Precision: 0.9230 - accuracy: 0.9192 - loss: 0.2230\n",
      "Epoch 18: accuracy improved from 0.90820 to 0.91992, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9859 - Precision: 0.9231 - accuracy: 0.9192 - loss: 0.2232 - val_AUC: 0.8351 - val_Precision: 0.7040 - val_accuracy: 0.6953 - val_loss: 1.2847 - learning_rate: 0.0100\n",
      "Epoch 19/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9869 - Precision: 0.9221 - accuracy: 0.9154 - loss: 0.2112\n",
      "Epoch 19: accuracy improved from 0.91992 to 0.92090, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - AUC: 0.9869 - Precision: 0.9224 - accuracy: 0.9159 - loss: 0.2116 - val_AUC: 0.8482 - val_Precision: 0.7222 - val_accuracy: 0.7109 - val_loss: 1.2866 - learning_rate: 0.0100\n",
      "Epoch 20/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9917 - Precision: 0.9426 - accuracy: 0.9395 - loss: 0.1699\n",
      "Epoch 20: accuracy improved from 0.92090 to 0.93066, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9913 - Precision: 0.9420 - accuracy: 0.9387 - loss: 0.1724 - val_AUC: 0.8502 - val_Precision: 0.7165 - val_accuracy: 0.7109 - val_loss: 1.3860 - learning_rate: 0.0100\n",
      "Epoch 21/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9861 - Precision: 0.9306 - accuracy: 0.9270 - loss: 0.2137\n",
      "Epoch 21: accuracy did not improve from 0.93066\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9857 - Precision: 0.9297 - accuracy: 0.9259 - loss: 0.2167 - val_AUC: 0.8566 - val_Precision: 0.7320 - val_accuracy: 0.7266 - val_loss: 1.1815 - learning_rate: 0.0100\n",
      "Epoch 22/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9895 - Precision: 0.9270 - accuracy: 0.9217 - loss: 0.1896\n",
      "Epoch 22: accuracy did not improve from 0.93066\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9895 - Precision: 0.9269 - accuracy: 0.9215 - loss: 0.1900 - val_AUC: 0.8628 - val_Precision: 0.7391 - val_accuracy: 0.7305 - val_loss: 1.2227 - learning_rate: 0.0100\n",
      "Epoch 23/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9918 - Precision: 0.9416 - accuracy: 0.9400 - loss: 0.1627\n",
      "Epoch 23: accuracy did not improve from 0.93066\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9905 - Precision: 0.9380 - accuracy: 0.9353 - loss: 0.1766 - val_AUC: 0.8520 - val_Precision: 0.7251 - val_accuracy: 0.7109 - val_loss: 1.2900 - learning_rate: 0.0100\n",
      "Epoch 24/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9887 - Precision: 0.9432 - accuracy: 0.9313 - loss: 0.1935\n",
      "Epoch 24: accuracy did not improve from 0.93066\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9888 - Precision: 0.9429 - accuracy: 0.9312 - loss: 0.1934 - val_AUC: 0.8624 - val_Precision: 0.7154 - val_accuracy: 0.7148 - val_loss: 1.2312 - learning_rate: 0.0100\n",
      "Epoch 25/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9895 - Precision: 0.9440 - accuracy: 0.9421 - loss: 0.1833\n",
      "Epoch 25: accuracy improved from 0.93066 to 0.93750, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9898 - Precision: 0.9433 - accuracy: 0.9414 - loss: 0.1808 - val_AUC: 0.8493 - val_Precision: 0.7216 - val_accuracy: 0.7188 - val_loss: 1.4168 - learning_rate: 0.0100\n",
      "Epoch 26/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9882 - Precision: 0.9451 - accuracy: 0.9438 - loss: 0.1821\n",
      "Epoch 26: accuracy did not improve from 0.93750\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9882 - Precision: 0.9449 - accuracy: 0.9435 - loss: 0.1824 - val_AUC: 0.8711 - val_Precision: 0.7323 - val_accuracy: 0.7266 - val_loss: 1.1983 - learning_rate: 0.0100\n",
      "Epoch 27/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9894 - Precision: 0.9445 - accuracy: 0.9430 - loss: 0.1788\n",
      "Epoch 27: accuracy did not improve from 0.93750\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9893 - Precision: 0.9425 - accuracy: 0.9401 - loss: 0.1810 - val_AUC: 0.8790 - val_Precision: 0.7461 - val_accuracy: 0.7461 - val_loss: 1.1997 - learning_rate: 0.0100\n",
      "Epoch 28/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9893 - Precision: 0.9397 - accuracy: 0.9397 - loss: 0.1733\n",
      "Epoch 28: accuracy improved from 0.93750 to 0.94629, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9894 - Precision: 0.9401 - accuracy: 0.9401 - loss: 0.1725 - val_AUC: 0.8551 - val_Precision: 0.7362 - val_accuracy: 0.7305 - val_loss: 1.3637 - learning_rate: 0.0100\n",
      "Epoch 29/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9950 - Precision: 0.9541 - accuracy: 0.9496 - loss: 0.1246\n",
      "Epoch 29: accuracy did not improve from 0.94629\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9945 - Precision: 0.9523 - accuracy: 0.9477 - loss: 0.1315 - val_AUC: 0.8388 - val_Precision: 0.7115 - val_accuracy: 0.7070 - val_loss: 1.5250 - learning_rate: 0.0100\n",
      "Epoch 30/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9926 - Precision: 0.9590 - accuracy: 0.9534 - loss: 0.1501\n",
      "Epoch 30: accuracy improved from 0.94629 to 0.95020, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9926 - Precision: 0.9590 - accuracy: 0.9533 - loss: 0.1502 - val_AUC: 0.8484 - val_Precision: 0.7302 - val_accuracy: 0.7266 - val_loss: 1.4122 - learning_rate: 0.0100\n",
      "Epoch 31/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9957 - Precision: 0.9588 - accuracy: 0.9529 - loss: 0.1235\n",
      "Epoch 31: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9956 - Precision: 0.9577 - accuracy: 0.9521 - loss: 0.1254 - val_AUC: 0.8484 - val_Precision: 0.7410 - val_accuracy: 0.7305 - val_loss: 1.4053 - learning_rate: 0.0100\n",
      "Epoch 32/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9952 - Precision: 0.9583 - accuracy: 0.9570 - loss: 0.1163 \n",
      "Epoch 32: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - AUC: 0.9939 - Precision: 0.9525 - accuracy: 0.9500 - loss: 0.1309 - val_AUC: 0.8533 - val_Precision: 0.7312 - val_accuracy: 0.7266 - val_loss: 1.4614 - learning_rate: 0.0100\n",
      "Epoch 33/250\n",
      "\u001b[1m28/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9913 - Precision: 0.9469 - accuracy: 0.9429 - loss: 0.1713\n",
      "Epoch 33: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9915 - Precision: 0.9473 - accuracy: 0.9428 - loss: 0.1687 - val_AUC: 0.8593 - val_Precision: 0.7441 - val_accuracy: 0.7422 - val_loss: 1.4214 - learning_rate: 0.0100\n",
      "Epoch 34/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9927 - Precision: 0.9578 - accuracy: 0.9499 - loss: 0.1335\n",
      "Epoch 34: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9927 - Precision: 0.9573 - accuracy: 0.9494 - loss: 0.1345 - val_AUC: 0.8510 - val_Precision: 0.7240 - val_accuracy: 0.7227 - val_loss: 1.5525 - learning_rate: 0.0100\n",
      "Epoch 35/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9964 - Precision: 0.9622 - accuracy: 0.9576 - loss: 0.1125\n",
      "Epoch 35: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9964 - Precision: 0.9619 - accuracy: 0.9573 - loss: 0.1132 - val_AUC: 0.8528 - val_Precision: 0.7075 - val_accuracy: 0.6992 - val_loss: 1.5989 - learning_rate: 0.0100\n",
      "Epoch 36/250\n",
      "\u001b[1m28/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9944 - Precision: 0.9525 - accuracy: 0.9499 - loss: 0.1294\n",
      "Epoch 36: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 0.9940 - Precision: 0.9513 - accuracy: 0.9486 - loss: 0.1338 - val_AUC: 0.8332 - val_Precision: 0.6602 - val_accuracy: 0.6602 - val_loss: 1.8053 - learning_rate: 0.0100\n",
      "Epoch 37/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9940 - Precision: 0.9662 - accuracy: 0.9511 - loss: 0.1255\n",
      "Epoch 37: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9935 - Precision: 0.9628 - accuracy: 0.9494 - loss: 0.1303 - val_AUC: 0.8470 - val_Precision: 0.7036 - val_accuracy: 0.7031 - val_loss: 1.5461 - learning_rate: 0.0100\n",
      "Epoch 38/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9943 - Precision: 0.9546 - accuracy: 0.9528 - loss: 0.1352\n",
      "Epoch 38: accuracy did not improve from 0.95020\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9943 - Precision: 0.9542 - accuracy: 0.9524 - loss: 0.1350 - val_AUC: 0.8351 - val_Precision: 0.6920 - val_accuracy: 0.6836 - val_loss: 1.6598 - learning_rate: 0.0100\n",
      "Epoch 39/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9963 - Precision: 0.9606 - accuracy: 0.9601 - loss: 0.1097\n",
      "Epoch 39: accuracy improved from 0.95020 to 0.95703, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9961 - Precision: 0.9605 - accuracy: 0.9598 - loss: 0.1113 - val_AUC: 0.8569 - val_Precision: 0.7233 - val_accuracy: 0.7227 - val_loss: 1.5815 - learning_rate: 0.0100\n",
      "Epoch 40/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9921 - Precision: 0.9387 - accuracy: 0.9358 - loss: 0.1534\n",
      "Epoch 40: accuracy did not improve from 0.95703\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9920 - Precision: 0.9389 - accuracy: 0.9359 - loss: 0.1552 - val_AUC: 0.8451 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 1.6346 - learning_rate: 0.0100\n",
      "Epoch 41/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9940 - Precision: 0.9563 - accuracy: 0.9555 - loss: 0.1325\n",
      "Epoch 41: accuracy did not improve from 0.95703\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 0.9936 - Precision: 0.9553 - accuracy: 0.9542 - loss: 0.1361 - val_AUC: 0.8367 - val_Precision: 0.6996 - val_accuracy: 0.6992 - val_loss: 1.5555 - learning_rate: 0.0100\n",
      "Epoch 42/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9929 - Precision: 0.9533 - accuracy: 0.9534 - loss: 0.1451\n",
      "Epoch 42: accuracy did not improve from 0.95703\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9934 - Precision: 0.9537 - accuracy: 0.9538 - loss: 0.1405 - val_AUC: 0.8513 - val_Precision: 0.7059 - val_accuracy: 0.7031 - val_loss: 1.5137 - learning_rate: 0.0100\n",
      "Epoch 43/250\n",
      "\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9948 - Precision: 0.9633 - accuracy: 0.9615 - loss: 0.1154\n",
      "Epoch 43: accuracy did not improve from 0.95703\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9941 - Precision: 0.9581 - accuracy: 0.9561 - loss: 0.1272 - val_AUC: 0.8398 - val_Precision: 0.7103 - val_accuracy: 0.7070 - val_loss: 1.6625 - learning_rate: 0.0100\n",
      "Epoch 44/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9986 - Precision: 0.9694 - accuracy: 0.9684 - loss: 0.0721\n",
      "Epoch 44: accuracy improved from 0.95703 to 0.96387, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9986 - Precision: 0.9693 - accuracy: 0.9683 - loss: 0.0726 - val_AUC: 0.8351 - val_Precision: 0.7070 - val_accuracy: 0.7070 - val_loss: 1.8816 - learning_rate: 0.0100\n",
      "Epoch 45/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9976 - Precision: 0.9753 - accuracy: 0.9744 - loss: 0.0824\n",
      "Epoch 45: accuracy improved from 0.96387 to 0.96777, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9975 - Precision: 0.9747 - accuracy: 0.9738 - loss: 0.0842 - val_AUC: 0.8580 - val_Precision: 0.7500 - val_accuracy: 0.7422 - val_loss: 1.4949 - learning_rate: 0.0100\n",
      "Epoch 46/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9952 - Precision: 0.9526 - accuracy: 0.9527 - loss: 0.1141\n",
      "Epoch 46: accuracy did not improve from 0.96777\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9952 - Precision: 0.9528 - accuracy: 0.9529 - loss: 0.1138 - val_AUC: 0.8390 - val_Precision: 0.7233 - val_accuracy: 0.7227 - val_loss: 1.7581 - learning_rate: 0.0100\n",
      "Epoch 47/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9952 - Precision: 0.9640 - accuracy: 0.9641 - loss: 0.1210\n",
      "Epoch 47: accuracy did not improve from 0.96777\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9944 - Precision: 0.9588 - accuracy: 0.9589 - loss: 0.1298 - val_AUC: 0.8417 - val_Precision: 0.7283 - val_accuracy: 0.7266 - val_loss: 1.5725 - learning_rate: 0.0100\n",
      "Epoch 48/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9934 - Precision: 0.9513 - accuracy: 0.9513 - loss: 0.1477\n",
      "Epoch 48: accuracy did not improve from 0.96777\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9935 - Precision: 0.9519 - accuracy: 0.9513 - loss: 0.1451 - val_AUC: 0.8549 - val_Precision: 0.7391 - val_accuracy: 0.7383 - val_loss: 1.4573 - learning_rate: 0.0100\n",
      "Epoch 49/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9956 - Precision: 0.9652 - accuracy: 0.9636 - loss: 0.1095\n",
      "Epoch 49: accuracy did not improve from 0.96777\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9955 - Precision: 0.9644 - accuracy: 0.9617 - loss: 0.1118 - val_AUC: 0.8442 - val_Precision: 0.7262 - val_accuracy: 0.7188 - val_loss: 1.5622 - learning_rate: 0.0100\n",
      "Epoch 50/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9970 - Precision: 0.9640 - accuracy: 0.9640 - loss: 0.0948\n",
      "Epoch 50: accuracy did not improve from 0.96777\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 0.9969 - Precision: 0.9636 - accuracy: 0.9635 - loss: 0.0955 - val_AUC: 0.8513 - val_Precision: 0.7383 - val_accuracy: 0.7383 - val_loss: 1.4902 - learning_rate: 0.0100\n",
      "Epoch 51/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9970 - Precision: 0.9718 - accuracy: 0.9718 - loss: 0.0869\n",
      "Epoch 51: accuracy improved from 0.96777 to 0.97266, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9971 - Precision: 0.9723 - accuracy: 0.9721 - loss: 0.0856 - val_AUC: 0.8510 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.5525 - learning_rate: 0.0100\n",
      "Epoch 52/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9969 - Precision: 0.9688 - accuracy: 0.9665 - loss: 0.1009\n",
      "Epoch 52: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9969 - Precision: 0.9685 - accuracy: 0.9662 - loss: 0.1012 - val_AUC: 0.8461 - val_Precision: 0.7047 - val_accuracy: 0.7031 - val_loss: 1.6412 - learning_rate: 0.0100\n",
      "Epoch 53/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9720 - accuracy: 0.9721 - loss: 0.0763\n",
      "Epoch 53: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9978 - Precision: 0.9712 - accuracy: 0.9713 - loss: 0.0812 - val_AUC: 0.8431 - val_Precision: 0.7148 - val_accuracy: 0.7148 - val_loss: 1.5992 - learning_rate: 0.0100\n",
      "Epoch 54/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9932 - Precision: 0.9568 - accuracy: 0.9568 - loss: 0.1315\n",
      "Epoch 54: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9932 - Precision: 0.9564 - accuracy: 0.9564 - loss: 0.1323 - val_AUC: 0.8512 - val_Precision: 0.7244 - val_accuracy: 0.7188 - val_loss: 1.6425 - learning_rate: 0.0100\n",
      "Epoch 55/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9954 - Precision: 0.9627 - accuracy: 0.9611 - loss: 0.1166\n",
      "Epoch 55: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9954 - Precision: 0.9625 - accuracy: 0.9609 - loss: 0.1166 - val_AUC: 0.8570 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 1.6347 - learning_rate: 0.0100\n",
      "Epoch 56/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9985 - Precision: 0.9669 - accuracy: 0.9650 - loss: 0.0750\n",
      "Epoch 56: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9984 - Precision: 0.9667 - accuracy: 0.9647 - loss: 0.0765 - val_AUC: 0.8493 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 1.7970 - learning_rate: 0.0100\n",
      "Epoch 57/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9979 - Precision: 0.9687 - accuracy: 0.9641 - loss: 0.0816\n",
      "Epoch 57: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9975 - Precision: 0.9680 - accuracy: 0.9636 - loss: 0.0851 - val_AUC: 0.8419 - val_Precision: 0.7115 - val_accuracy: 0.7109 - val_loss: 1.9000 - learning_rate: 0.0100\n",
      "Epoch 58/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9935 - Precision: 0.9531 - accuracy: 0.9530 - loss: 0.1304\n",
      "Epoch 58: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9933 - Precision: 0.9527 - accuracy: 0.9526 - loss: 0.1331 - val_AUC: 0.8394 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 1.7719 - learning_rate: 0.0100\n",
      "Epoch 59/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9970 - Precision: 0.9664 - accuracy: 0.9640 - loss: 0.0985\n",
      "Epoch 59: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 0.9967 - Precision: 0.9661 - accuracy: 0.9638 - loss: 0.1006 - val_AUC: 0.8510 - val_Precision: 0.7098 - val_accuracy: 0.7070 - val_loss: 1.6954 - learning_rate: 0.0100\n",
      "Epoch 60/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9954 - Precision: 0.9718 - accuracy: 0.9718 - loss: 0.1062\n",
      "Epoch 60: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9955 - Precision: 0.9703 - accuracy: 0.9700 - loss: 0.1075 - val_AUC: 0.8558 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 1.6402 - learning_rate: 0.0100\n",
      "Epoch 61/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9970 - Precision: 0.9800 - accuracy: 0.9790 - loss: 0.0832\n",
      "Epoch 61: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9970 - Precision: 0.9797 - accuracy: 0.9787 - loss: 0.0838 - val_AUC: 0.8479 - val_Precision: 0.7047 - val_accuracy: 0.7031 - val_loss: 1.6272 - learning_rate: 0.0100\n",
      "Epoch 62/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9980 - Precision: 0.9678 - accuracy: 0.9668 - loss: 0.0786\n",
      "Epoch 62: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9979 - Precision: 0.9674 - accuracy: 0.9663 - loss: 0.0797 - val_AUC: 0.8500 - val_Precision: 0.7176 - val_accuracy: 0.7148 - val_loss: 1.6641 - learning_rate: 0.0100\n",
      "Epoch 63/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9960 - Precision: 0.9759 - accuracy: 0.9724 - loss: 0.0915\n",
      "Epoch 63: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9959 - Precision: 0.9741 - accuracy: 0.9710 - loss: 0.0936 - val_AUC: 0.8514 - val_Precision: 0.7323 - val_accuracy: 0.7266 - val_loss: 1.6411 - learning_rate: 0.0100\n",
      "Epoch 64/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9987 - Precision: 0.9790 - accuracy: 0.9790 - loss: 0.0694\n",
      "Epoch 64: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9984 - Precision: 0.9762 - accuracy: 0.9761 - loss: 0.0742 - val_AUC: 0.8392 - val_Precision: 0.7031 - val_accuracy: 0.7031 - val_loss: 1.6412 - learning_rate: 0.0100\n",
      "Epoch 65/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9775 - accuracy: 0.9775 - loss: 0.0646\n",
      "Epoch 65: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9978 - Precision: 0.9742 - accuracy: 0.9738 - loss: 0.0729 - val_AUC: 0.8412 - val_Precision: 0.6797 - val_accuracy: 0.6797 - val_loss: 1.6661 - learning_rate: 0.0100\n",
      "Epoch 66/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9974 - Precision: 0.9774 - accuracy: 0.9769 - loss: 0.0806\n",
      "Epoch 66: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9972 - Precision: 0.9764 - accuracy: 0.9758 - loss: 0.0834 - val_AUC: 0.8502 - val_Precision: 0.7412 - val_accuracy: 0.7422 - val_loss: 1.6342 - learning_rate: 0.0100\n",
      "Epoch 67/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9955 - Precision: 0.9655 - accuracy: 0.9635 - loss: 0.0959\n",
      "Epoch 67: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9955 - Precision: 0.9644 - accuracy: 0.9624 - loss: 0.0977 - val_AUC: 0.8466 - val_Precision: 0.7126 - val_accuracy: 0.7109 - val_loss: 1.6959 - learning_rate: 0.0100\n",
      "Epoch 68/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9976 - Precision: 0.9593 - accuracy: 0.9594 - loss: 0.0911\n",
      "Epoch 68: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9975 - Precision: 0.9581 - accuracy: 0.9581 - loss: 0.0945 - val_AUC: 0.8561 - val_Precision: 0.7451 - val_accuracy: 0.7461 - val_loss: 1.6009 - learning_rate: 0.0100\n",
      "Epoch 69/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9968 - Precision: 0.9671 - accuracy: 0.9671 - loss: 0.0942\n",
      "Epoch 69: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9968 - Precision: 0.9670 - accuracy: 0.9670 - loss: 0.0941 - val_AUC: 0.8465 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.7578 - learning_rate: 0.0100\n",
      "Epoch 70/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9945 - Precision: 0.9457 - accuracy: 0.9445 - loss: 0.1432\n",
      "Epoch 70: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9946 - Precision: 0.9486 - accuracy: 0.9473 - loss: 0.1373 - val_AUC: 0.8511 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.7203 - learning_rate: 0.0100\n",
      "Epoch 71/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9752 - accuracy: 0.9733 - loss: 0.0660\n",
      "Epoch 71: accuracy did not improve from 0.97266\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9982 - Precision: 0.9716 - accuracy: 0.9697 - loss: 0.0758 - val_AUC: 0.8551 - val_Precision: 0.7008 - val_accuracy: 0.6953 - val_loss: 1.5501 - learning_rate: 0.0100\n",
      "Epoch 72/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9966 - Precision: 0.9753 - accuracy: 0.9727 - loss: 0.0940\n",
      "Epoch 72: accuracy did not improve from 0.97266\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9965 - Precision: 0.9749 - accuracy: 0.9724 - loss: 0.0943 - val_AUC: 0.8592 - val_Precision: 0.7165 - val_accuracy: 0.7148 - val_loss: 1.4581 - learning_rate: 0.0030\n",
      "Epoch 73/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9984 - Precision: 0.9749 - accuracy: 0.9744 - loss: 0.0698\n",
      "Epoch 73: accuracy improved from 0.97266 to 0.97754, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9983 - Precision: 0.9762 - accuracy: 0.9754 - loss: 0.0687 - val_AUC: 0.8596 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.4716 - learning_rate: 0.0030\n",
      "Epoch 74/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9895 - accuracy: 0.9896 - loss: 0.0489\n",
      "Epoch 74: accuracy improved from 0.97754 to 0.98828, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9995 - Precision: 0.9895 - accuracy: 0.9895 - loss: 0.0487 - val_AUC: 0.8593 - val_Precision: 0.7412 - val_accuracy: 0.7422 - val_loss: 1.4824 - learning_rate: 0.0030\n",
      "Epoch 75/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9997 - Precision: 0.9872 - accuracy: 0.9872 - loss: 0.0365\n",
      "Epoch 75: accuracy improved from 0.98828 to 0.99023, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9997 - Precision: 0.9874 - accuracy: 0.9874 - loss: 0.0362 - val_AUC: 0.8615 - val_Precision: 0.7490 - val_accuracy: 0.7461 - val_loss: 1.4882 - learning_rate: 0.0030\n",
      "Epoch 76/250\n",
      "\u001b[1m28/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9984 - Precision: 0.9769 - accuracy: 0.9769 - loss: 0.0648\n",
      "Epoch 76: accuracy did not improve from 0.99023\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 0.9984 - Precision: 0.9772 - accuracy: 0.9772 - loss: 0.0641 - val_AUC: 0.8581 - val_Precision: 0.7461 - val_accuracy: 0.7461 - val_loss: 1.5721 - learning_rate: 0.0030\n",
      "Epoch 77/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9985 - Precision: 0.9897 - accuracy: 0.9897 - loss: 0.0417\n",
      "Epoch 77: accuracy improved from 0.99023 to 0.99121, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 0.9985 - Precision: 0.9898 - accuracy: 0.9898 - loss: 0.0415 - val_AUC: 0.8564 - val_Precision: 0.7500 - val_accuracy: 0.7500 - val_loss: 1.5881 - learning_rate: 0.0030\n",
      "Epoch 78/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9996 - Precision: 0.9781 - accuracy: 0.9781 - loss: 0.0408\n",
      "Epoch 78: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9994 - Precision: 0.9791 - accuracy: 0.9791 - loss: 0.0418 - val_AUC: 0.8573 - val_Precision: 0.7422 - val_accuracy: 0.7422 - val_loss: 1.6293 - learning_rate: 0.0030\n",
      "Epoch 79/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9993 - Precision: 0.9818 - accuracy: 0.9818 - loss: 0.0530\n",
      "Epoch 79: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9993 - Precision: 0.9815 - accuracy: 0.9815 - loss: 0.0531 - val_AUC: 0.8554 - val_Precision: 0.7194 - val_accuracy: 0.7148 - val_loss: 1.6920 - learning_rate: 0.0030\n",
      "Epoch 80/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9955 - Precision: 0.9819 - accuracy: 0.9819 - loss: 0.0897\n",
      "Epoch 80: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9956 - Precision: 0.9821 - accuracy: 0.9821 - loss: 0.0900 - val_AUC: 0.8576 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.6054 - learning_rate: 0.0030\n",
      "Epoch 81/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9974 - Precision: 0.9849 - accuracy: 0.9849 - loss: 0.0510\n",
      "Epoch 81: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9974 - Precision: 0.9847 - accuracy: 0.9847 - loss: 0.0514 - val_AUC: 0.8567 - val_Precision: 0.7383 - val_accuracy: 0.7383 - val_loss: 1.6062 - learning_rate: 0.0030\n",
      "Epoch 82/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9981 - Precision: 0.9907 - accuracy: 0.9907 - loss: 0.0445\n",
      "Epoch 82: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9982 - Precision: 0.9904 - accuracy: 0.9904 - loss: 0.0443 - val_AUC: 0.8565 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.5711 - learning_rate: 0.0030\n",
      "Epoch 83/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9997 - Precision: 0.9842 - accuracy: 0.9842 - loss: 0.0310\n",
      "Epoch 83: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9996 - Precision: 0.9841 - accuracy: 0.9841 - loss: 0.0316 - val_AUC: 0.8583 - val_Precision: 0.7344 - val_accuracy: 0.7344 - val_loss: 1.5731 - learning_rate: 0.0030\n",
      "Epoch 84/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9996 - Precision: 0.9932 - accuracy: 0.9932 - loss: 0.0289\n",
      "Epoch 84: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9995 - Precision: 0.9925 - accuracy: 0.9925 - loss: 0.0313 - val_AUC: 0.8562 - val_Precision: 0.7333 - val_accuracy: 0.7305 - val_loss: 1.6142 - learning_rate: 0.0030\n",
      "Epoch 85/250\n",
      "\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9998 - Precision: 0.9950 - accuracy: 0.9950 - loss: 0.0220\n",
      "Epoch 85: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9997 - Precision: 0.9923 - accuracy: 0.9923 - loss: 0.0278 - val_AUC: 0.8537 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 1.6406 - learning_rate: 0.0030\n",
      "Epoch 86/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9993 - Precision: 0.9767 - accuracy: 0.9767 - loss: 0.0476\n",
      "Epoch 86: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9993 - Precision: 0.9771 - accuracy: 0.9771 - loss: 0.0468 - val_AUC: 0.8493 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 1.7214 - learning_rate: 0.0030\n",
      "Epoch 87/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9967 - accuracy: 0.9967 - loss: 0.0212\n",
      "Epoch 87: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9943 - accuracy: 0.9941 - loss: 0.0244 - val_AUC: 0.8461 - val_Precision: 0.7216 - val_accuracy: 0.7227 - val_loss: 1.7527 - learning_rate: 0.0030\n",
      "Epoch 88/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9915 - accuracy: 0.9915 - loss: 0.0245\n",
      "Epoch 88: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9997 - Precision: 0.9914 - accuracy: 0.9914 - loss: 0.0279 - val_AUC: 0.8504 - val_Precision: 0.7216 - val_accuracy: 0.7227 - val_loss: 1.7590 - learning_rate: 0.0030\n",
      "Epoch 89/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9919 - accuracy: 0.9919 - loss: 0.0242\n",
      "Epoch 89: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9914 - accuracy: 0.9914 - loss: 0.0252 - val_AUC: 0.8531 - val_Precision: 0.7344 - val_accuracy: 0.7344 - val_loss: 1.7467 - learning_rate: 0.0030\n",
      "Epoch 90/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9994 - Precision: 0.9888 - accuracy: 0.9888 - loss: 0.0357\n",
      "Epoch 90: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9994 - Precision: 0.9887 - accuracy: 0.9887 - loss: 0.0357 - val_AUC: 0.8518 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.7735 - learning_rate: 0.0030\n",
      "Epoch 91/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9905 - accuracy: 0.9903 - loss: 0.0251\n",
      "Epoch 91: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9905 - accuracy: 0.9902 - loss: 0.0252 - val_AUC: 0.8526 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.7814 - learning_rate: 0.0030\n",
      "Epoch 92/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9999 - Precision: 0.9936 - accuracy: 0.9936 - loss: 0.0194\n",
      "Epoch 92: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9924 - accuracy: 0.9924 - loss: 0.0220 - val_AUC: 0.8554 - val_Precision: 0.7344 - val_accuracy: 0.7344 - val_loss: 1.7987 - learning_rate: 0.0030\n",
      "Epoch 93/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9997 - Precision: 0.9856 - accuracy: 0.9856 - loss: 0.0319\n",
      "Epoch 93: accuracy did not improve from 0.99121\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9997 - Precision: 0.9858 - accuracy: 0.9858 - loss: 0.0316 - val_AUC: 0.8548 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.8045 - learning_rate: 0.0030\n",
      "Epoch 94/250\n",
      "\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9996 - Precision: 0.9908 - accuracy: 0.9908 - loss: 0.0334\n",
      "Epoch 94: accuracy improved from 0.99121 to 0.99512, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - AUC: 0.9997 - Precision: 0.9925 - accuracy: 0.9925 - loss: 0.0284 - val_AUC: 0.8536 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.8273 - learning_rate: 0.0030\n",
      "Epoch 95/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 1.0000 - Precision: 0.9963 - accuracy: 0.9962 - loss: 0.0120\n",
      "Epoch 95: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9962 - accuracy: 0.9959 - loss: 0.0127 - val_AUC: 0.8550 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.8327 - learning_rate: 0.0030\n",
      "Epoch 96/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9940 - accuracy: 0.9940 - loss: 0.0141\n",
      "Epoch 96: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9938 - accuracy: 0.9938 - loss: 0.0148 - val_AUC: 0.8550 - val_Precision: 0.7412 - val_accuracy: 0.7383 - val_loss: 1.8123 - learning_rate: 0.0030\n",
      "Epoch 97/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9952 - accuracy: 0.9901 - loss: 0.0205\n",
      "Epoch 97: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9943 - accuracy: 0.9905 - loss: 0.0209 - val_AUC: 0.8594 - val_Precision: 0.7283 - val_accuracy: 0.7266 - val_loss: 1.8569 - learning_rate: 0.0030\n",
      "Epoch 98/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9998 - Precision: 0.9930 - accuracy: 0.9930 - loss: 0.0161\n",
      "Epoch 98: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.9997 - Precision: 0.9929 - accuracy: 0.9927 - loss: 0.0184 - val_AUC: 0.8560 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 1.8589 - learning_rate: 0.0030\n",
      "Epoch 99/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9966 - Precision: 0.9767 - accuracy: 0.9767 - loss: 0.0743\n",
      "Epoch 99: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9967 - Precision: 0.9768 - accuracy: 0.9768 - loss: 0.0738 - val_AUC: 0.8577 - val_Precision: 0.7333 - val_accuracy: 0.7305 - val_loss: 1.9101 - learning_rate: 0.0030\n",
      "Epoch 100/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9990 - Precision: 0.9895 - accuracy: 0.9895 - loss: 0.0329\n",
      "Epoch 100: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9990 - Precision: 0.9894 - accuracy: 0.9891 - loss: 0.0332 - val_AUC: 0.8554 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 1.9197 - learning_rate: 0.0030\n",
      "Epoch 101/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9996 - Precision: 0.9881 - accuracy: 0.9881 - loss: 0.0349\n",
      "Epoch 101: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9996 - Precision: 0.9876 - accuracy: 0.9876 - loss: 0.0364 - val_AUC: 0.8515 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 1.8821 - learning_rate: 0.0030\n",
      "Epoch 102/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9990 - Precision: 0.9746 - accuracy: 0.9729 - loss: 0.0542\n",
      "Epoch 102: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9992 - Precision: 0.9786 - accuracy: 0.9771 - loss: 0.0467 - val_AUC: 0.8527 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.8921 - learning_rate: 0.0030\n",
      "Epoch 103/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9997 - Precision: 0.9862 - accuracy: 0.9862 - loss: 0.0356\n",
      "Epoch 103: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9997 - Precision: 0.9862 - accuracy: 0.9862 - loss: 0.0351 - val_AUC: 0.8560 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 1.8526 - learning_rate: 0.0030\n",
      "Epoch 104/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9997 - Precision: 0.9855 - accuracy: 0.9855 - loss: 0.0325\n",
      "Epoch 104: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9997 - Precision: 0.9869 - accuracy: 0.9869 - loss: 0.0326 - val_AUC: 0.8563 - val_Precision: 0.7383 - val_accuracy: 0.7383 - val_loss: 1.9242 - learning_rate: 0.0030\n",
      "Epoch 105/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9885 - accuracy: 0.9885 - loss: 0.0401\n",
      "Epoch 105: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9996 - Precision: 0.9890 - accuracy: 0.9890 - loss: 0.0365 - val_AUC: 0.8549 - val_Precision: 0.7383 - val_accuracy: 0.7383 - val_loss: 1.9474 - learning_rate: 0.0030\n",
      "Epoch 106/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9998 - Precision: 0.9926 - accuracy: 0.9926 - loss: 0.0239\n",
      "Epoch 106: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9998 - Precision: 0.9919 - accuracy: 0.9919 - loss: 0.0257 - val_AUC: 0.8582 - val_Precision: 0.7500 - val_accuracy: 0.7500 - val_loss: 1.9229 - learning_rate: 0.0030\n",
      "Epoch 107/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9983 - Precision: 0.9913 - accuracy: 0.9913 - loss: 0.0334\n",
      "Epoch 107: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9983 - Precision: 0.9913 - accuracy: 0.9913 - loss: 0.0334 - val_AUC: 0.8561 - val_Precision: 0.7608 - val_accuracy: 0.7578 - val_loss: 1.8864 - learning_rate: 0.0030\n",
      "Epoch 108/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9900 - accuracy: 0.9900 - loss: 0.0291\n",
      "Epoch 108: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9998 - Precision: 0.9896 - accuracy: 0.9896 - loss: 0.0294 - val_AUC: 0.8472 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 1.9785 - learning_rate: 0.0030\n",
      "Epoch 109/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9950 - accuracy: 0.9945 - loss: 0.0175   \n",
      "Epoch 109: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9935 - accuracy: 0.9928 - loss: 0.0207 - val_AUC: 0.8479 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 1.9645 - learning_rate: 0.0030\n",
      "Epoch 110/250\n",
      "\u001b[1m25/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9996 - Precision: 0.9878 - accuracy: 0.9878 - loss: 0.0259\n",
      "Epoch 110: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 0.9994 - Precision: 0.9873 - accuracy: 0.9873 - loss: 0.0280 - val_AUC: 0.8471 - val_Precision: 0.7283 - val_accuracy: 0.7305 - val_loss: 1.9600 - learning_rate: 0.0030\n",
      "Epoch 111/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9990 - Precision: 0.9842 - accuracy: 0.9842 - loss: 0.0418\n",
      "Epoch 111: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - AUC: 0.9991 - Precision: 0.9849 - accuracy: 0.9849 - loss: 0.0404 - val_AUC: 0.8496 - val_Precision: 0.7373 - val_accuracy: 0.7344 - val_loss: 1.9951 - learning_rate: 0.0030\n",
      "Epoch 112/250\n",
      "\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9998 - Precision: 0.9870 - accuracy: 0.9870 - loss: 0.0244\n",
      "Epoch 112: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9871 - accuracy: 0.9871 - loss: 0.0261 - val_AUC: 0.8428 - val_Precision: 0.7148 - val_accuracy: 0.7148 - val_loss: 2.0284 - learning_rate: 0.0030\n",
      "Epoch 113/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9988 - Precision: 0.9826 - accuracy: 0.9826 - loss: 0.0438\n",
      "Epoch 113: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9989 - Precision: 0.9828 - accuracy: 0.9828 - loss: 0.0435 - val_AUC: 0.8432 - val_Precision: 0.6992 - val_accuracy: 0.6992 - val_loss: 2.0869 - learning_rate: 0.0030\n",
      "Epoch 114/250\n",
      "\u001b[1m28/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9998 - Precision: 0.9914 - accuracy: 0.9914 - loss: 0.0280\n",
      "Epoch 114: accuracy did not improve from 0.99512\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9914 - accuracy: 0.9914 - loss: 0.0277 - val_AUC: 0.8467 - val_Precision: 0.7070 - val_accuracy: 0.7070 - val_loss: 2.0674 - learning_rate: 0.0030\n",
      "Epoch 115/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9973 - accuracy: 0.9954 - loss: 0.0191\n",
      "Epoch 115: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9956 - accuracy: 0.9939 - loss: 0.0208 - val_AUC: 0.8504 - val_Precision: 0.7070 - val_accuracy: 0.7070 - val_loss: 2.0549 - learning_rate: 9.0000e-04\n",
      "Epoch 116/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9999 - Precision: 0.9925 - accuracy: 0.9925 - loss: 0.0195\n",
      "Epoch 116: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9928 - accuracy: 0.9928 - loss: 0.0191 - val_AUC: 0.8520 - val_Precision: 0.7148 - val_accuracy: 0.7148 - val_loss: 2.0298 - learning_rate: 9.0000e-04\n",
      "Epoch 117/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9920 - accuracy: 0.9920 - loss: 0.0282\n",
      "Epoch 117: accuracy did not improve from 0.99512\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9920 - accuracy: 0.9920 - loss: 0.0280 - val_AUC: 0.8518 - val_Precision: 0.7148 - val_accuracy: 0.7148 - val_loss: 2.0416 - learning_rate: 9.0000e-04\n",
      "Epoch 118/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 1.0000 - Precision: 0.9968 - accuracy: 0.9968 - loss: 0.0154\n",
      "Epoch 118: accuracy improved from 0.99512 to 0.99707, saving model to gpt2/best_model.keras\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - AUC: 1.0000 - Precision: 0.9968 - accuracy: 0.9968 - loss: 0.0153 - val_AUC: 0.8485 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 2.0639 - learning_rate: 9.0000e-04\n",
      "Epoch 119/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9963 - accuracy: 0.9959 - loss: 0.0183\n",
      "Epoch 119: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9962 - accuracy: 0.9957 - loss: 0.0186 - val_AUC: 0.8475 - val_Precision: 0.7216 - val_accuracy: 0.7188 - val_loss: 2.0825 - learning_rate: 9.0000e-04\n",
      "Epoch 120/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 1.0000 - Precision: 0.9979 - accuracy: 0.9979 - loss: 0.0099\n",
      "Epoch 120: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9965 - accuracy: 0.9965 - loss: 0.0125 - val_AUC: 0.8473 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 2.0847 - learning_rate: 9.0000e-04\n",
      "Epoch 121/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 1.0000 - Precision: 0.9973 - accuracy: 0.9973 - loss: 0.0100   \n",
      "Epoch 121: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9963 - accuracy: 0.9963 - loss: 0.0123 - val_AUC: 0.8472 - val_Precision: 0.7176 - val_accuracy: 0.7148 - val_loss: 2.0843 - learning_rate: 9.0000e-04\n",
      "Epoch 122/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9994 - Precision: 0.9905 - accuracy: 0.9905 - loss: 0.0229\n",
      "Epoch 122: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9994 - Precision: 0.9907 - accuracy: 0.9907 - loss: 0.0233 - val_AUC: 0.8501 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 2.0880 - learning_rate: 9.0000e-04\n",
      "Epoch 123/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 1.0000 - Precision: 0.9969 - accuracy: 0.9969 - loss: 0.0098   \n",
      "Epoch 123: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9968 - accuracy: 0.9968 - loss: 0.0107 - val_AUC: 0.8481 - val_Precision: 0.7148 - val_accuracy: 0.7148 - val_loss: 2.0960 - learning_rate: 9.0000e-04\n",
      "Epoch 124/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9997 - Precision: 0.9877 - accuracy: 0.9877 - loss: 0.0300\n",
      "Epoch 124: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9878 - accuracy: 0.9878 - loss: 0.0297 - val_AUC: 0.8524 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 2.0903 - learning_rate: 9.0000e-04\n",
      "Epoch 125/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9995 - Precision: 0.9907 - accuracy: 0.9907 - loss: 0.0235   \n",
      "Epoch 125: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9990 - Precision: 0.9894 - accuracy: 0.9895 - loss: 0.0294 - val_AUC: 0.8519 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 2.0985 - learning_rate: 9.0000e-04\n",
      "Epoch 126/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9978 - Precision: 0.9853 - accuracy: 0.9853 - loss: 0.0516\n",
      "Epoch 126: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9982 - Precision: 0.9872 - accuracy: 0.9872 - loss: 0.0443 - val_AUC: 0.8509 - val_Precision: 0.7188 - val_accuracy: 0.7188 - val_loss: 2.0845 - learning_rate: 9.0000e-04\n",
      "Epoch 127/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9840 - accuracy: 0.9840 - loss: 0.0436\n",
      "Epoch 127: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9991 - Precision: 0.9849 - accuracy: 0.9849 - loss: 0.0399 - val_AUC: 0.8503 - val_Precision: 0.7227 - val_accuracy: 0.7227 - val_loss: 2.0650 - learning_rate: 9.0000e-04\n",
      "Epoch 128/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9894 - accuracy: 0.9894 - loss: 0.0163\n",
      "Epoch 128: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9999 - Precision: 0.9905 - accuracy: 0.9905 - loss: 0.0168 - val_AUC: 0.8500 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 2.0720 - learning_rate: 9.0000e-04\n",
      "Epoch 129/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9904 - accuracy: 0.9904 - loss: 0.0233\n",
      "Epoch 129: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9904 - accuracy: 0.9904 - loss: 0.0232 - val_AUC: 0.8496 - val_Precision: 0.7373 - val_accuracy: 0.7383 - val_loss: 2.0666 - learning_rate: 9.0000e-04\n",
      "Epoch 130/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9999 - Precision: 0.9917 - accuracy: 0.9917 - loss: 0.0191   \n",
      "Epoch 130: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 0.9999 - Precision: 0.9919 - accuracy: 0.9919 - loss: 0.0187 - val_AUC: 0.8500 - val_Precision: 0.7383 - val_accuracy: 0.7383 - val_loss: 2.0439 - learning_rate: 9.0000e-04\n",
      "Epoch 131/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9888 - accuracy: 0.9888 - loss: 0.0223\n",
      "Epoch 131: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9885 - accuracy: 0.9885 - loss: 0.0229 - val_AUC: 0.8473 - val_Precision: 0.7362 - val_accuracy: 0.7344 - val_loss: 2.0840 - learning_rate: 9.0000e-04\n",
      "Epoch 132/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9946 - accuracy: 0.9946 - loss: 0.0211\n",
      "Epoch 132: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9946 - accuracy: 0.9946 - loss: 0.0211 - val_AUC: 0.8475 - val_Precision: 0.7283 - val_accuracy: 0.7227 - val_loss: 2.0788 - learning_rate: 9.0000e-04\n",
      "Epoch 133/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9992 - Precision: 0.9909 - accuracy: 0.9909 - loss: 0.0193\n",
      "Epoch 133: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9992 - Precision: 0.9909 - accuracy: 0.9909 - loss: 0.0195 - val_AUC: 0.8462 - val_Precision: 0.7283 - val_accuracy: 0.7266 - val_loss: 2.0788 - learning_rate: 9.0000e-04\n",
      "Epoch 134/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9874 - accuracy: 0.9874 - loss: 0.0262\n",
      "Epoch 134: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - AUC: 0.9998 - Precision: 0.9878 - accuracy: 0.9878 - loss: 0.0257 - val_AUC: 0.8490 - val_Precision: 0.7305 - val_accuracy: 0.7305 - val_loss: 2.0916 - learning_rate: 9.0000e-04\n",
      "Epoch 135/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 1.0000 - Precision: 0.9965 - accuracy: 0.9965 - loss: 0.0088\n",
      "Epoch 135: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9965 - accuracy: 0.9965 - loss: 0.0088 - val_AUC: 0.8503 - val_Precision: 0.7294 - val_accuracy: 0.7305 - val_loss: 2.0930 - learning_rate: 9.0000e-04\n",
      "Epoch 136/250\n",
      "\u001b[1m27/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9998 - Precision: 0.9873 - accuracy: 0.9873 - loss: 0.0255\n",
      "Epoch 136: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9998 - Precision: 0.9881 - accuracy: 0.9881 - loss: 0.0249 - val_AUC: 0.8464 - val_Precision: 0.7323 - val_accuracy: 0.7305 - val_loss: 2.0792 - learning_rate: 9.0000e-04\n",
      "Epoch 137/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9912 - accuracy: 0.9912 - loss: 0.0187\n",
      "Epoch 137: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9920 - accuracy: 0.9920 - loss: 0.0177 - val_AUC: 0.8479 - val_Precision: 0.7266 - val_accuracy: 0.7266 - val_loss: 2.0998 - learning_rate: 9.0000e-04\n",
      "Epoch 138/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9998 - Precision: 0.9880 - accuracy: 0.9880 - loss: 0.0262\n",
      "Epoch 138: accuracy did not improve from 0.99707\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9998 - Precision: 0.9883 - accuracy: 0.9883 - loss: 0.0253 - val_AUC: 0.8452 - val_Precision: 0.7244 - val_accuracy: 0.7266 - val_loss: 2.1100 - learning_rate: 9.0000e-04\n",
      "Epoch 139/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 1.0000 - Precision: 0.9986 - accuracy: 0.9986 - loss: 0.0082\n",
      "Epoch 139: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9985 - accuracy: 0.9985 - loss: 0.0083 - val_AUC: 0.8454 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 2.1163 - learning_rate: 2.7000e-04\n",
      "Epoch 140/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9959 - accuracy: 0.9959 - loss: 0.0142\n",
      "Epoch 140: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9999 - Precision: 0.9957 - accuracy: 0.9957 - loss: 0.0139 - val_AUC: 0.8457 - val_Precision: 0.7244 - val_accuracy: 0.7266 - val_loss: 2.1188 - learning_rate: 2.7000e-04\n",
      "Epoch 141/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9930 - accuracy: 0.9930 - loss: 0.0150\n",
      "Epoch 141: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9928 - accuracy: 0.9928 - loss: 0.0152 - val_AUC: 0.8456 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 2.1133 - learning_rate: 2.7000e-04\n",
      "Epoch 142/250\n",
      "\u001b[1m23/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9928 - accuracy: 0.9928 - loss: 0.0123\n",
      "Epoch 142: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9920 - accuracy: 0.9920 - loss: 0.0148 - val_AUC: 0.8460 - val_Precision: 0.7244 - val_accuracy: 0.7266 - val_loss: 2.1057 - learning_rate: 2.7000e-04\n",
      "Epoch 143/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 1.0000 - Precision: 0.9959 - accuracy: 0.9959 - loss: 0.0087   \n",
      "Epoch 143: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 1.0000 - Precision: 0.9954 - accuracy: 0.9954 - loss: 0.0097 - val_AUC: 0.8458 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 2.0986 - learning_rate: 2.7000e-04\n",
      "Epoch 144/250\n",
      "\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9949 - accuracy: 0.9949 - loss: 0.0161\n",
      "Epoch 144: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9948 - accuracy: 0.9948 - loss: 0.0161 - val_AUC: 0.8452 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 2.1025 - learning_rate: 2.7000e-04\n",
      "Epoch 145/250\n",
      "\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 1.0000 - Precision: 0.9948 - accuracy: 0.9948 - loss: 0.0092\n",
      "Epoch 145: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 1.0000 - Precision: 0.9943 - accuracy: 0.9943 - loss: 0.0098 - val_AUC: 0.8468 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 2.1036 - learning_rate: 2.7000e-04\n",
      "Epoch 146/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9850 - accuracy: 0.9850 - loss: 0.0230\n",
      "Epoch 146: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9862 - accuracy: 0.9862 - loss: 0.0225 - val_AUC: 0.8487 - val_Precision: 0.7255 - val_accuracy: 0.7227 - val_loss: 2.1012 - learning_rate: 2.7000e-04\n",
      "Epoch 147/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9925 - accuracy: 0.9925 - loss: 0.0200\n",
      "Epoch 147: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9927 - accuracy: 0.9927 - loss: 0.0196 - val_AUC: 0.8490 - val_Precision: 0.7244 - val_accuracy: 0.7227 - val_loss: 2.1059 - learning_rate: 2.7000e-04\n",
      "Epoch 148/250\n",
      "\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9991 - Precision: 0.9931 - accuracy: 0.9931 - loss: 0.0236 \n",
      "Epoch 148: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9991 - Precision: 0.9930 - accuracy: 0.9930 - loss: 0.0236 - val_AUC: 0.8473 - val_Precision: 0.7176 - val_accuracy: 0.7188 - val_loss: 2.1163 - learning_rate: 2.7000e-04\n",
      "Epoch 149/250\n",
      "\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9999 - Precision: 0.9938 - accuracy: 0.9920 - loss: 0.0174\n",
      "Epoch 149: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9999 - Precision: 0.9929 - accuracy: 0.9914 - loss: 0.0184 - val_AUC: 0.8473 - val_Precision: 0.7216 - val_accuracy: 0.7188 - val_loss: 2.1238 - learning_rate: 2.7000e-04\n",
      "Epoch 150/250\n",
      "\u001b[1m24/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 1.0000 - Precision: 0.9984 - accuracy: 0.9984 - loss: 0.0063\n",
      "Epoch 150: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 1.0000 - Precision: 0.9976 - accuracy: 0.9976 - loss: 0.0078 - val_AUC: 0.8488 - val_Precision: 0.7216 - val_accuracy: 0.7188 - val_loss: 2.1236 - learning_rate: 2.7000e-04\n",
      "Epoch 151/250\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9998 - Precision: 0.9898 - accuracy: 0.9898 - loss: 0.0230\n",
      "Epoch 151: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9998 - Precision: 0.9898 - accuracy: 0.9898 - loss: 0.0229 - val_AUC: 0.8476 - val_Precision: 0.7176 - val_accuracy: 0.7148 - val_loss: 2.1223 - learning_rate: 2.7000e-04\n",
      "Epoch 152/250\n",
      "\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - AUC: 0.9999 - Precision: 0.9931 - accuracy: 0.9913 - loss: 0.0183\n",
      "Epoch 152: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - AUC: 0.9999 - Precision: 0.9932 - accuracy: 0.9915 - loss: 0.0182 - val_AUC: 0.8480 - val_Precision: 0.7255 - val_accuracy: 0.7266 - val_loss: 2.1185 - learning_rate: 2.7000e-04\n",
      "Epoch 153/250\n",
      "\u001b[1m26/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9999 - Precision: 0.9965 - accuracy: 0.9965 - loss: 0.0126\n",
      "Epoch 153: accuracy did not improve from 0.99707\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.9999 - Precision: 0.9963 - accuracy: 0.9963 - loss: 0.0129 - val_AUC: 0.8478 - val_Precision: 0.7294 - val_accuracy: 0.7266 - val_loss: 2.1202 - learning_rate: 2.7000e-04\n",
      "Epoch 153: early stopping\n",
      "Restoring model weights from the end of the best epoch: 118.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
    "from tensorflow.keras.initializers import HeNormal, Constant\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    # Input layer + first hidden block\n",
    "    Dense(512,\n",
    "          kernel_initializer=HeNormal(),\n",
    "          bias_initializer=Constant(0.1),\n",
    "          input_shape=(X_trained_processed.shape[1],)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    # Or use LeakyReLU(alpha=0.01):\n",
    "    # LeakyReLU(alpha=0.01),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second hidden block\n",
    "    Dense(256, kernel_initializer=HeNormal(), bias_initializer=Constant(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third hidden block\n",
    "    Dense(128, kernel_initializer=HeNormal(), bias_initializer=Constant(0.1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer for 3-class classification\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam + LR schedule\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Save only the best model based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        filepath='gpt2/best_model.keras',\n",
    "        monitor='accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=2\n",
    "    ),\n",
    "    \n",
    "    # Stop training early if no improvement (prevents overfitting)\n",
    "    EarlyStopping(\n",
    "        monitor='accuracy',\n",
    "        mode='max',\n",
    "        patience=35,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate if stuck\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='accuracy',\n",
    "        mode='max',\n",
    "        factor=0.3,\n",
    "        patience=20,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Example fit call:\n",
    "history = model.fit(\n",
    "    X_trained_processed, Y_train_one_hot,\n",
    "    epochs=250,\n",
    "    batch_size=32,\n",
    "    validation_split=.2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3abdb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model(\"gpt2/best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "af698d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_state</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.071270</td>\n",
       "      <td>-0.544388</td>\n",
       "      <td>-0.441977</td>\n",
       "      <td>0.474749</td>\n",
       "      <td>0.261783</td>\n",
       "      <td>0.168065</td>\n",
       "      <td>0.155831</td>\n",
       "      <td>-0.105221</td>\n",
       "      <td>-0.855129</td>\n",
       "      <td>0.077505</td>\n",
       "      <td>0.340463</td>\n",
       "      <td>0.791441</td>\n",
       "      <td>-0.375751</td>\n",
       "      <td>-0.196253</td>\n",
       "      <td>-0.378314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/08</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.035270</td>\n",
       "      <td>-0.466104</td>\n",
       "      <td>-0.665476</td>\n",
       "      <td>0.675356</td>\n",
       "      <td>0.667040</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.522456</td>\n",
       "      <td>-1.674080</td>\n",
       "      <td>-0.543447</td>\n",
       "      <td>-0.239287</td>\n",
       "      <td>0.530151</td>\n",
       "      <td>0.242503</td>\n",
       "      <td>-0.836072</td>\n",
       "      <td>-0.325414</td>\n",
       "      <td>1.046846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others</td>\n",
       "      <td>2023/01</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614346</td>\n",
       "      <td>0.096122</td>\n",
       "      <td>2.027837</td>\n",
       "      <td>-0.988678</td>\n",
       "      <td>-0.351694</td>\n",
       "      <td>-0.114182</td>\n",
       "      <td>0.128963</td>\n",
       "      <td>-0.496279</td>\n",
       "      <td>-0.034282</td>\n",
       "      <td>-0.258599</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>-0.201151</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>-0.417021</td>\n",
       "      <td>0.093116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job_Title_5</td>\n",
       "      <td>2024/06</td>\n",
       "      <td>NY</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.398380</td>\n",
       "      <td>0.421391</td>\n",
       "      <td>0.254911</td>\n",
       "      <td>0.300071</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.164509</td>\n",
       "      <td>-0.934464</td>\n",
       "      <td>-0.854196</td>\n",
       "      <td>1.057971</td>\n",
       "      <td>0.206792</td>\n",
       "      <td>-0.485801</td>\n",
       "      <td>-0.027462</td>\n",
       "      <td>0.201683</td>\n",
       "      <td>-0.141402</td>\n",
       "      <td>0.047435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Others</td>\n",
       "      <td>2024/05</td>\n",
       "      <td>CA</td>\n",
       "      <td>A</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.174363</td>\n",
       "      <td>-0.476998</td>\n",
       "      <td>-0.661285</td>\n",
       "      <td>-0.686967</td>\n",
       "      <td>-0.349405</td>\n",
       "      <td>-0.790193</td>\n",
       "      <td>0.474133</td>\n",
       "      <td>0.709674</td>\n",
       "      <td>-0.420866</td>\n",
       "      <td>-0.132481</td>\n",
       "      <td>0.558299</td>\n",
       "      <td>-0.908974</td>\n",
       "      <td>-0.282736</td>\n",
       "      <td>-0.180746</td>\n",
       "      <td>-0.619603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title job_posted_date job_state  ...    PCA_13    PCA_14    PCA_15\n",
       "0       Others         2024/06        CA  ... -0.375751 -0.196253 -0.378314\n",
       "1       Others         2024/08        NY  ... -0.836072 -0.325414  1.046846\n",
       "2       Others         2023/01        CA  ...  0.423814 -0.417021  0.093116\n",
       "3  Job_Title_5         2024/06        NY  ...  0.201683 -0.141402  0.047435\n",
       "4       Others         2024/05        CA  ... -0.282736 -0.180746 -0.619603\n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pca=pca.transform(Test_X[job_features])\n",
    "\n",
    "test_pca_df=pd.DataFrame(test_pca,columns=[\"PCA_1\",\"PCA_2\",\"PCA_3\",\"PCA_4\",\"PCA_5\",\"PCA_6\",\"PCA_7\",\"PCA_8\",\"PCA_9\",\"PCA_10\",\"PCA_11\",\"PCA_12\",\"PCA_13\",\"PCA_14\",\"PCA_15\"])\n",
    "\n",
    "new_Test_X=Test_X.drop(columns=job_features)\n",
    "\n",
    "Test_X_pca=pd.concat([new_Test_X,test_pca_df],axis=1)\n",
    "\n",
    "Test_X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f69d20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X_encoded=pipeline.transform(Test_X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ae57e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(Test_X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c232b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Medium',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the mapping based on index\n",
    "index_to_label = {0: 'High', 1: 'Medium', 2: 'Low'}\n",
    "\n",
    "# Get the index of the max probability for each row\n",
    "predicted_indices = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# Map the indices to labels\n",
    "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "ea9e1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"../Dataset/engineers_salary_prediction_test.csv\")\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"obs\": test[\"obs\"],  # assuming obs starts at 1281\n",
    "    \"salary_category\": predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"pca_new_engineer_salary_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a6daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
